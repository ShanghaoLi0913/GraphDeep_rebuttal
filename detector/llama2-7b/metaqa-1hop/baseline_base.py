#!/usr/bin/env python3
"""
Baselineæ–¹æ³•åŸºç¡€ç±» - ç”¨äºå•ç‹¬è°ƒè¯•æ¯ä¸ªbaselineæ–¹æ³•
"""

import json
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class BaselineBase:
    """åŸºç¡€baselineæ£€æµ‹å™¨"""
    
    def __init__(self, model_name="meta-llama/Llama-2-7b-chat-hf"):
        """åˆå§‹åŒ–åŸºç¡€æ£€æµ‹å™¨ - Colab L4 GPUä¼˜åŒ–ç‰ˆ"""
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ğŸš€ Colab L4 GPUä¼˜åŒ–é…ç½® (22.5GBæ˜¾å­˜)
        if torch.cuda.is_available():
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸ’¾ GPU Memory: {gpu_memory_gb:.1f} GB")
            
            if gpu_memory_gb > 20:  # L4 GPU
                self.batch_size = 32  # è¿›ä¸€æ­¥å¢å¤§batch size
                self.max_length = 256  # å‡å°‘åºåˆ—é•¿åº¦æé€Ÿ
                print("ğŸš€ L4 GPUä¼˜åŒ–: batch_size=32, max_length=256")
            else:
                self.batch_size = 8
                self.max_length = 512
                print("âš¡ æ ‡å‡†GPUé…ç½®: batch_size=8, max_length=512")
        else:
            self.batch_size = 4
            self.max_length = 256
            print("ğŸ–¥ï¸ CPUæ¨¡å¼: batch_size=4, max_length=256")
        
        print(f"ğŸ“± Device: {self.device}")
    
    def load_data(self, max_train_samples=100, max_test_samples=50):
        """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        print("ğŸ“¥ Loading training and test data...")
        
        # åŠ è½½è®­ç»ƒæ•°æ® - è‡ªåŠ¨æ£€æµ‹è·¯å¾„
        train_samples = []
        
        # ğŸš€ æ™ºèƒ½è·¯å¾„æ£€æµ‹ï¼šæ”¯æŒæœ¬åœ°å’ŒColabç¯å¢ƒ
        base_paths = [
            '/content/GraphDeep/experiment_records/inference_results/llama2-7b/',  # Colabè·¯å¾„
            '/mnt/d/experiments/GraphDeEP/experiment_records/inference_results/llama2-7b/',  # æœ¬åœ°è·¯å¾„
        ]
        
        train_files = []
        for base_path in base_paths:
            if os.path.exists(base_path):
                train_files = [
                    os.path.join(base_path, 'colab_train_simple_part1&2.jsonl'),
                    os.path.join(base_path, 'colab_train_simple_part1.jsonl'),
                    os.path.join(base_path, 'colab_train_simple_part2.jsonl'),
                ]
                break
        
        train_file = None
        for tf in train_files:
            if os.path.exists(tf):
                train_file = tf
                break
        
        if train_file is None:
            print("âŒ No training data file found")
            return None, None
        
        try:
            with open(train_file, 'r', encoding='utf-8') as f:
                next(f)  # è·³è¿‡ç¬¬ä¸€è¡Œé…ç½®ä¿¡æ¯
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        train_samples.append(data)
                        if len(train_samples) >= max_train_samples:
                            break
        except Exception as e:
            print(f"âŒ Error loading training data from {train_file}: {e}")
            return None, None
        
        # åŠ è½½æµ‹è¯•æ•°æ® - ä½¿ç”¨ç›¸åŒçš„base_path
        test_samples = []
        test_files = []
        for base_path in base_paths:
            if os.path.exists(base_path):
                test_files = [
                    os.path.join(base_path, 'colab_test_simple.jsonl')
                ]
                break
        
        test_file = None
        for tf in test_files:
            if os.path.exists(tf):
                test_file = tf
                break
        
        if test_file is None:
            print("âŒ No test data file found")
            return None, None
        
        try:
            with open(test_file, 'r', encoding='utf-8') as f:
                next(f)  # è·³è¿‡ç¬¬ä¸€è¡Œé…ç½®ä¿¡æ¯
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        test_samples.append(data)
                        if len(test_samples) >= max_test_samples:
                            break
        except Exception as e:
            print(f"âŒ Error loading test data from {test_file}: {e}")
            return None, None
        
        print(f"âœ… Loaded {len(train_samples)} training samples from {train_file}")
        print(f"âœ… Loaded {len(test_samples)} test samples from {test_file}")
        return train_samples, test_samples
    
    def extract_labels(self, samples):
        """ä»æ ·æœ¬ä¸­æå–æ ‡ç­¾ï¼ˆ0=æ­£ç¡®ï¼Œ1=å¹»è§‰ï¼‰ï¼Œæ”¹ä¸ºç”¨SQuADè¯„ä¼°"""
        labels = []
        for sample in samples:
            # ç”¨SQuADè¯„ä¼°çš„æ ‡ç­¾
            squad_eval = sample.get('squad_evaluation', {})
            is_hallucination = squad_eval.get('squad_is_hallucination', False)
            labels.append(1 if is_hallucination else 0)  # 1=å¹»è§‰ï¼Œ0=æ­£ç¡®
        return np.array(labels)
    
    def optimize_threshold(self, y_true, y_proba, method_name=None):
        """ä½¿ç”¨PRD+SASçš„ç®€å•F1ä¼˜åŒ–ç­–ç•¥"""
        from sklearn.metrics import precision_recall_curve
        
        # ğŸ¯ ä½¿ç”¨PRD+SASçš„ç®€å•ç­–ç•¥ï¼šç›´æ¥æœ€å¤§åŒ–F1
        precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)
        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
        
        best_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5
        best_f1 = f1_scores[best_idx]
        
        # è·å–æœ€ä¼˜é˜ˆå€¼ä¸‹çš„æ‰€æœ‰æŒ‡æ ‡
        y_pred = (y_proba >= best_threshold).astype(int)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        
        # è¾“å‡ºé€‰æ‹©çš„æŒ‡æ ‡ä¿¡æ¯
        print(f"    ğŸ“Š F1-optimized - P:{precision:.3f}, R:{recall:.3f}, F1:{best_f1:.3f}")
        print(f"    ğŸ“Š Threshold: {best_threshold:.3f}")
        
        return best_threshold, best_f1
    
    def evaluate_method(self, train_samples, test_samples, method_name, feature_extractor):
        """è¯„ä¼°å•ä¸ªæ–¹æ³•"""
        print(f"\nğŸ” Evaluating {method_name}...")
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        print("ğŸ“Š Extracting features...")
        X_train = feature_extractor(train_samples)
        y_train = self.extract_labels(train_samples)
        
        X_test = feature_extractor(test_samples)
        y_test = self.extract_labels(test_samples)
        
        # æ•°æ®é¢„å¤„ç†
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 1))
        X_test_scaled = scaler.transform(X_test.reshape(-1, 1))
        
        # è®­ç»ƒéªŒè¯åˆ†å‰²
        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
            X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        print(f"ğŸ“Š Data splits: Train={len(X_train_split)}, Val={len(X_val_split)}, Test={len(X_test_scaled)}")
        print(f"ğŸ“Š Label distribution - Train: {np.bincount(y_train_split)}, Val: {np.bincount(y_val_split)}, Test: {np.bincount(y_test)}")
        
        # è®­ç»ƒæ¨¡å‹
        model = LogisticRegression(random_state=42, class_weight='balanced')
        model.fit(X_train_split, y_train_split)
        
        # åœ¨éªŒè¯é›†ä¸Šä¼˜åŒ–é˜ˆå€¼
        y_proba_val = model.predict_proba(X_val_split)[:, 1]
        threshold, f1_opt = self.optimize_threshold(y_val_split, y_proba_val, method_name)
        
        print(f"ğŸ“Š Optimized threshold: {threshold:.3f} (Val F1: {f1_opt:.3f})")
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        y_proba_test = model.predict_proba(X_test_scaled)[:, 1]
        y_pred_test = (y_proba_test >= threshold).astype(int)
        
        # ğŸ” è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        print(f"\nğŸ” DEBUG: Test predictions analysis")
        print(f"   Test probabilities range: [{y_proba_test.min():.3f}, {y_proba_test.max():.3f}]")
        print(f"   Threshold: {threshold:.3f}")
        print(f"   Predicted as hallucination: {np.sum(y_pred_test)}/{len(y_pred_test)}")
        print(f"   Actually hallucination: {np.sum(y_test)}/{len(y_test)}")
        
        # æ˜¾ç¤ºæ··æ·†çŸ©é˜µä¿¡æ¯
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_test, y_pred_test)
        print(f"   Confusion Matrix:")
        print(f"   TN={cm[0,0]}, FP={cm[0,1]}")
        print(f"   FN={cm[1,0]}, TP={cm[1,1]}")
        
        # è®¡ç®—æŒ‡æ ‡
        auc = roc_auc_score(y_test, y_proba_test)
        precision = precision_score(y_test, y_pred_test, zero_division=0)
        recall = recall_score(y_test, y_pred_test, zero_division=0)
        f1 = f1_score(y_test, y_pred_test, zero_division=0)
        accuracy = accuracy_score(y_test, y_pred_test)
        
        print(f"   Precision = TP/(TP+FP) = {cm[1,1]}/({cm[1,1]}+{cm[0,1]}) = {precision:.3f}")
        print(f"   Recall = TP/(TP+FN) = {cm[1,1]}/({cm[1,1]}+{cm[1,0]}) = {recall:.3f}")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        report = classification_report(y_test, y_pred_test, output_dict=True, zero_division=0)
        
        results = {
            'auc': float(auc),
            'threshold': float(threshold),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'accuracy': float(accuracy),
            'classification_report': report
        }
        
        # æ‰“å°ç»“æœ
        print(f"âœ… Results for {method_name}:")
        print(f"   AUC: {auc:.4f}")
        print(f"   Precision: {precision:.4f}")
        print(f"   Recall: {recall:.4f}")
        print(f"   F1-Score: {f1:.4f}")
        print(f"   Accuracy: {accuracy:.4f}")
        
        return results
    
    def save_results(self, method_name, results):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(os.path.dirname(__file__), "results", "individual_baselines")
        os.makedirs(output_dir, exist_ok=True)
        
        output_file = os.path.join(output_dir, f"{method_name.lower().replace(' ', '_')}_results_{timestamp}.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({method_name: results}, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Results saved to: {output_file}")
        return output_file