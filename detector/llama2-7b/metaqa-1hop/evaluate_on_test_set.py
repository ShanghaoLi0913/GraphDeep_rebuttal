"""
åœ¨çœŸæ­£çš„test setä¸Šè¯„ä¼°è®­ç»ƒå¥½çš„detectoræ¨¡å‹
è·å¾—çœŸå®ã€æ— åçš„æ€§èƒ½è¯„ä¼°
"""

import json
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import joblib
import glob

class TestSetEvaluator:
    def __init__(self):
        self.results = {}
        
    def load_data(self, train_file, test_file):
        """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        print("Loading data...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_data = []
        with open(train_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() and not line.startswith('{"config"'):
                    train_data.append(json.loads(line))
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = []
        with open(test_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() and not line.startswith('{"config"'):
                    test_data.append(json.loads(line))
        
        print(f"Loaded {len(train_data)} training samples, {len(test_data)} test samples")
        return train_data, test_data
    
    def extract_enhanced_features(self, data):
        """æå–å¢å¼ºç‰¹å¾ - ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´"""
        features = []
        labels = []
        
        for item in data:
            # åŸºç¡€ç‰¹å¾
            feature_dict = {
                'gass_score': item.get('gass_score', 0),
                'prd_score': item.get('prd_score', 0),
                'balanced_calibrated_gass': item.get('balanced_calibrated_gass', 0),
                'tus_score': item.get('tus_score', 0),
                'gass_jsd_score': item.get('gass_jsd_score', 0),
            }
            
            # ç‰¹å¾å·¥ç¨‹ï¼šç»„åˆç‰¹å¾
            feature_dict['gass_prd_ratio'] = feature_dict['gass_score'] / (feature_dict['prd_score'] + 1e-8)
            feature_dict['gass_tus_diff'] = feature_dict['gass_score'] - feature_dict['tus_score']
            feature_dict['balanced_original_gass_diff'] = feature_dict['balanced_calibrated_gass'] - feature_dict['gass_score']
            
            # è¡¨é¢ç‰¹å¾
            if 'model_output' in item:
                output = item['model_output']
                words = output.split()
                feature_dict.update({
                    'output_length': len(words),
                    'repetition_score': self.calculate_repetition(output),
                    'avg_word_length': np.mean([len(w) for w in words]) if words else 0,
                    'unique_word_ratio': len(set(words)) / len(words) if words else 0,
                    'has_ans_prefix': 1 if 'ans:' in output.lower() else 0,
                    'comma_count': output.count(','),
                    'question_mark_count': output.count('?'),
                })
            
            # é—®é¢˜ç‰¹å¾
            if 'question' in item:
                question = item['question']
                q_words = question.split()
                feature_dict.update({
                    'question_length': len(q_words),
                    'question_has_what': 1 if 'what' in question.lower() else 0,
                    'question_has_which': 1 if 'which' in question.lower() else 0,
                    'question_has_who': 1 if 'who' in question.lower() else 0,
                })
            
            features.append(feature_dict)
            
            # æ ‡ç­¾
            if 'squad_evaluation' in item:
                is_hallucination = item['squad_evaluation'].get('squad_is_hallucination', False)
            else:
                is_hallucination = not item.get('metrics', {}).get('hit@1', False)
            
            labels.append(int(is_hallucination))
        
        df = pd.DataFrame(features)
        return df, np.array(labels)
    
    def calculate_repetition(self, text):
        """è®¡ç®—æ–‡æœ¬é‡å¤åº¦"""
        words = text.lower().split()
        if len(words) <= 1:
            return 0
        unique_words = len(set(words))
        return 1 - (unique_words / len(words))
    
    def load_trained_models(self, model_dir):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("ğŸ”„ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹å…ƒæ•°æ®æ–‡ä»¶
        metadata_files = glob.glob(f"{model_dir}/models_metadata_*.json")
        if not metadata_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹å…ƒæ•°æ®æ–‡ä»¶")
            print("è¯·å…ˆè¿è¡Œ train_detector_optimized.py è®­ç»ƒæ¨¡å‹")
            return None
        
        # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
        latest_metadata_file = max(metadata_files)
        print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹å…ƒæ•°æ®æ–‡ä»¶: {latest_metadata_file}")
        
        # åŠ è½½å…ƒæ•°æ®
        with open(latest_metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # åŠ è½½scaler
        scaler_path = metadata['scaler_path']
        if not os.path.exists(scaler_path):
            print(f"âŒ Scaleræ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}")
            return None
        
        scaler = joblib.load(scaler_path)
        print(f"âœ… åŠ è½½scaler: {scaler_path}")
        
        # åŠ è½½ç‰¹å¾åç§°
        feature_names_path = metadata['feature_names_path']
        if not os.path.exists(feature_names_path):
            print(f"âŒ ç‰¹å¾åç§°æ–‡ä»¶ä¸å­˜åœ¨: {feature_names_path}")
            return None
        
        with open(feature_names_path, 'r', encoding='utf-8') as f:
            feature_names = json.load(f)
        print(f"âœ… åŠ è½½ç‰¹å¾åç§°: {len(feature_names)} ä¸ªç‰¹å¾")
        
        # åŠ è½½ä¼˜åŒ–é˜ˆå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        thresholds = {}
        if 'threshold_path' in metadata and os.path.exists(metadata['threshold_path']):
            with open(metadata['threshold_path'], 'r', encoding='utf-8') as f:
                thresholds = json.load(f)
            print(f"âœ… åŠ è½½ä¼˜åŒ–é˜ˆå€¼: {len(thresholds)} ä¸ªæ¨¡å‹")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ä¼˜åŒ–é˜ˆå€¼ï¼Œå°†ä½¿ç”¨é»˜è®¤é˜ˆå€¼0.5")
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹
        models = {}
        model_paths = metadata['model_paths']
        
        for model_name, model_path in model_paths.items():
            if not os.path.exists(model_path):
                print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                continue
            
            try:
                model = joblib.load(model_path)
                models[model_name] = model
                print(f"âœ… åŠ è½½æ¨¡å‹: {model_name}")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                continue
        
        if not models:
            print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ¨¡å‹")
            return None
        
        print(f"ğŸ‰ æˆåŠŸåŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
        print(f"ğŸ“ˆ è®­ç»ƒæ—¶æœ€ä½³æ¨¡å‹: {metadata['best_model']} (AUC: {metadata['best_auc']:.4f})")
        
        return {
            'models': models,
            'scaler': scaler,
            'feature_names': feature_names,
            'thresholds': thresholds,
            'metadata': metadata
        }
    
    def evaluate_on_test_set(self, test_data, model_dir):
        """åœ¨çœŸæ­£çš„test setä¸Šè¯„ä¼°"""
        print("\nğŸ§ª åœ¨çœŸæ­£çš„test setä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        model_data = self.load_trained_models(model_dir)
        if model_data is None:
            return None
        
        trained_models = model_data['models']
        scaler = model_data['scaler']
        feature_names = model_data['feature_names']
        thresholds = model_data['thresholds']
        metadata = model_data['metadata']
        
        # æå–æµ‹è¯•é›†ç‰¹å¾
        X_test, y_test = self.extract_enhanced_features(test_data)
        
        print(f"Test set: {len(X_test)} samples")
        print(f"Test set hallucination rate: {np.mean(y_test):.3f}")
        
        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
        if list(X_test.columns) != feature_names:
            print("âš ï¸ ç‰¹å¾é¡ºåºä¸ä¸€è‡´ï¼Œé‡æ–°æ’åº...")
            X_test = X_test[feature_names]
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„scalerè¿›è¡Œæ ‡å‡†åŒ–
        X_test_scaled = scaler.transform(X_test)
        
        # åœ¨test setä¸Šè¯„ä¼°
        print("\nğŸ“Š åœ¨Test Setä¸Šçš„çœŸå®æ€§èƒ½:")
        print("="*60)
        
        test_results = {}
        for name, model in trained_models.items():
            try:
                y_proba = model.predict_proba(X_test_scaled)[:, 1]
                auc = roc_auc_score(y_test, y_proba)
                
                # ä½¿ç”¨ä¼˜åŒ–é˜ˆå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é˜ˆå€¼
                optimal_threshold = thresholds.get(name, 0.5)
                
                # é»˜è®¤é˜ˆå€¼é¢„æµ‹
                y_pred_default = model.predict(X_test_scaled)
                report_default = classification_report(y_test, y_pred_default, output_dict=True)
                
                # ä¼˜åŒ–é˜ˆå€¼é¢„æµ‹
                y_pred_optimal = (y_proba >= optimal_threshold).astype(int)
                report_optimal = classification_report(y_test, y_pred_optimal, output_dict=True)
                
                test_results[name] = {
                    'auc': auc,
                    'optimal_threshold': optimal_threshold,
                    'default_results': {
                        'precision': report_default['1']['precision'] if '1' in report_default else 0,
                        'recall': report_default['1']['recall'] if '1' in report_default else 0,
                        'f1_score': report_default['1']['f1-score'] if '1' in report_default else 0,
                        'accuracy': report_default['accuracy']
                    },
                    'optimal_results': {
                        'precision': report_optimal['1']['precision'] if '1' in report_optimal else 0,
                        'recall': report_optimal['1']['recall'] if '1' in report_optimal else 0,
                        'f1_score': report_optimal['1']['f1-score'] if '1' in report_optimal else 0,
                        'accuracy': report_optimal['accuracy']
                    }
                }
                
                print(f"\n{name}:")
                print(f"  AUC: {auc:.4f}")
                print(f"  ä¼˜åŒ–é˜ˆå€¼: {optimal_threshold:.3f}")
                print(f"  é»˜è®¤é˜ˆå€¼(0.5) - P: {report_default['1']['precision']:.3f}, R: {report_default['1']['recall']:.3f}, F1: {report_default['1']['f1-score']:.3f}")
                print(f"  ä¼˜åŒ–é˜ˆå€¼({optimal_threshold:.3f}) - P: {report_optimal['1']['precision']:.3f}, R: {report_optimal['1']['recall']:.3f}, F1: {report_optimal['1']['f1-score']:.3f}")
                
            except Exception as e:
                print(f"âŒ è¯„ä¼°æ¨¡å‹ {name} æ—¶å‡ºé”™: {e}")
                continue
        
        return test_results
    
    def plot_test_results(self, test_results, output_dir):
        """ç»˜åˆ¶test setç»“æœ"""
        plt.figure(figsize=(12, 8))
        
        # AUCå¯¹æ¯”
        plt.subplot(2, 2, 1)
        names = list(test_results.keys())
        aucs = [test_results[name]['auc'] for name in names]
        colors = ['lightcoral' if name == 'VotingEnsemble' else 'skyblue' for name in names]
        bars = plt.bar(names, aucs, color=colors)
        plt.ylabel('AUC Score')
        plt.title('Test Set AUC Performance')
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
        
        for bar, auc in zip(bars, aucs):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{auc:.3f}', ha='center', va='bottom')
        
        # F1å¯¹æ¯” (ä½¿ç”¨ä¼˜åŒ–é˜ˆå€¼ç»“æœ)
        plt.subplot(2, 2, 2)
        f1s = [test_results[name]['optimal_results']['f1_score'] for name in names]
        bars = plt.bar(names, f1s, color=colors)
        plt.ylabel('F1 Score')
        plt.title('Test Set F1 Performance (ä¼˜åŒ–é˜ˆå€¼)')
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
        
        for bar, f1 in zip(bars, f1s):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{f1:.3f}', ha='center', va='bottom')
        
        # Precision vs Recall (ä½¿ç”¨ä¼˜åŒ–é˜ˆå€¼ç»“æœ)
        plt.subplot(2, 2, 3)
        precisions = [test_results[name]['optimal_results']['precision'] for name in names]
        recalls = [test_results[name]['optimal_results']['recall'] for name in names]
        
        plt.scatter(recalls, precisions, s=100, alpha=0.7)
        for i, name in enumerate(names):
            plt.annotate(name, (recalls[i], precisions[i]), 
                        xytext=(5, 5), textcoords='offset points')
        
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision vs Recall (Test Set)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(f"{output_dir}/test_set_evaluation_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.show()

def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„ - ä½¿ç”¨llama2-7bæµ‹è¯•æ•°æ®ï¼Œä»llama2-7bæ¨¡å‹ç›®å½•åŠ è½½æ¨¡å‹
    test_file = "/mnt/d/experiments/GraphDeEP/experiment_records/inference_results/llama2-7b/colab_test_simple.jsonl"
    output_dir = "/mnt/d/experiments/GraphDeEP/detector/llama2-7b/metaqa-1hop/results"
    model_dir = "/mnt/d/experiments/GraphDeEP/detector/llama2-7b/metaqa-1hop/models"
    
    print("ğŸ¯ Test Set çœŸå®æ€§èƒ½è¯„ä¼°")
    print("="*50)
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    if not os.path.exists(model_dir):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        print("è¯·å…ˆè¿è¡Œ train_detector_optimized.py è®­ç»ƒæ¨¡å‹")
        return
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = TestSetEvaluator()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = []
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip() and not line.startswith('{"config"'):
                test_data.append(json.loads(line))
    
    print(f"Loaded {len(test_data)} test samples")
    
    # åœ¨test setä¸Šè¯„ä¼°
    test_results = evaluator.evaluate_on_test_set(test_data, model_dir)
    
    if test_results is None:
        print("âŒ è¯„ä¼°å¤±è´¥")
        return
    
    # ç»˜åˆ¶ç»“æœ
    evaluator.plot_test_results(test_results, output_dir)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f"{output_dir}/test_set_results_{timestamp}.json", 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ† TEST SET æœ€ç»ˆç»“æœæ€»ç»“")
    print("="*60)
    
    best_model = max(test_results.keys(), key=lambda x: test_results[x]['auc'])
    best_auc = test_results[best_model]['auc']
    best_results = test_results[best_model]['optimal_results']
    
    print(f"æœ€ä½³æ¨¡å‹: {best_model}")
    print(f"Test Set AUC: {best_auc:.4f}")
    print(f"ä¼˜åŒ–é˜ˆå€¼ç»“æœ - P: {best_results['precision']:.4f}, R: {best_results['recall']:.4f}, F1: {best_results['f1_score']:.4f}")
    print("\nâœ… è¿™æ˜¯åŸºäºè®­ç»ƒå¥½çš„æ¨¡å‹å’Œä¼˜åŒ–é˜ˆå€¼çš„çœŸå®ã€æ— åçš„æ€§èƒ½è¯„ä¼°ï¼")

if __name__ == "__main__":
    main()