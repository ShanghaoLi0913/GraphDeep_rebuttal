#!/usr/bin/env python3
"""
GGAç‰¹å¾æ¶ˆèå®éªŒ (Ablation Study)
æµ‹è¯•ä¸åŒç‰¹å¾ç»„åˆçš„æ£€æµ‹æ•ˆæœï¼š
1. SAS only - å•ç‹¬è¯„ä¼°è¯­ä¹‰å¯¹é½çš„æ£€æµ‹èƒ½åŠ›
2. PRD only - å•ç‹¬è¯„ä¼°æ³¨æ„åŠ›è·¯å¾„è¿‡åº¦ä¾èµ–çš„æ£€æµ‹èƒ½åŠ›  
3. PRD + SAS (GGA-Core) - æœºåˆ¶ç‰¹å¾ç»„åˆï¼Œå…·å¤‡è§£é‡Šæ€§å’Œä¸€å®šæ£€æµ‹èƒ½åŠ›
4. PRD + SAS + Surface (GGA-Full) - ä¸»æ–¹æ³•ï¼Œæ£€æµ‹æ€§èƒ½æœ€ä½³ï¼Œå…¼å…·è§£é‡Šæ€§ä¸å®ç”¨æ€§
"""

import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc
from datetime import datetime
import os
import joblib
import warnings
warnings.filterwarnings('ignore')

class GGAAblationStudy:
    """GGAç‰¹å¾æ¶ˆèå®éªŒç±»"""
    
    def __init__(self):
        self.results = {}
        self.models = {}
        self.scalers = {}
        
        # ç‰¹å¾åç§°æ˜ å°„ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.feature_names_readable = {
            'gass_score': 'SAS Score',
            'prd_score': 'PRD Score', 
            'output_length': 'Answer Length',
            'repetition_score': 'Repetition Ratio',
            'avg_word_length': 'Avg Word Length',
            'unique_word_ratio': 'Unique Word Ratio',
            'has_ans_prefix': 'Has "Ans:" Prefix',
            'comma_count': 'Comma Count',
            'question_mark_count': 'Question Mark Count'
        }
        
        # å®šä¹‰ç‰¹å¾ç»„åˆ
        self.feature_configs = {
            'SAS_only': {
                'features': ['gass_score'],
                'description': 'å•ç‹¬è¯„ä¼°è¯­ä¹‰å¯¹é½çš„æ£€æµ‹èƒ½åŠ›'
            },
            'PRD_only': {
                'features': ['prd_score'], 
                'description': 'å•ç‹¬è¯„ä¼°æ³¨æ„åŠ›è·¯å¾„è¿‡åº¦ä¾èµ–çš„æ£€æµ‹èƒ½åŠ›'
            },
            'GGA_Core': {
                'features': ['gass_score', 'prd_score'],
                'description': 'æœºåˆ¶ç‰¹å¾ç»„åˆï¼Œå…·å¤‡è§£é‡Šæ€§å’Œä¸€å®šæ£€æµ‹èƒ½åŠ›'
            },
            'GGA_Full': {
                'features': ['gass_score', 'prd_score', 'output_length', 'repetition_score', 
                           'avg_word_length', 'unique_word_ratio', 'has_ans_prefix', 'comma_count', 'question_mark_count'],
                'description': 'ä¸»æ–¹æ³•ï¼Œæ£€æµ‹æ€§èƒ½æœ€ä½³ï¼Œå…¼å…·è§£é‡Šæ€§ä¸å®ç”¨æ€§'
            }
        }
    
    def extract_features(self, data, feature_set):
        """æ ¹æ®ç‰¹å¾é›†æå–ç›¸åº”ç‰¹å¾"""
        features = []
        labels = []
        
        for item in data:
            feature_dict = {
                'gass_score': item.get('gass_score', 0),
                'prd_score': item.get('prd_score', 0),
            }
            
            # è¡¨é¢ç‰¹å¾ï¼ˆåªåœ¨GGA_Fullæ—¶éœ€è¦ï¼‰
            if any(f in feature_set for f in ['output_length', 'repetition_score', 'avg_word_length', 
                                            'unique_word_ratio', 'has_ans_prefix', 'comma_count', 'question_mark_count']):
                if 'model_output' in item:
                    output = item['model_output']
                    words = output.split()
                    feature_dict.update({
                        'output_length': len(words),
                        'repetition_score': self.calculate_repetition(output),
                        'avg_word_length': np.mean([len(w) for w in words]) if words else 0,
                        'unique_word_ratio': len(set(words)) / len(words) if words else 0,
                        'has_ans_prefix': 1 if 'ans:' in output.lower() else 0,
                        'comma_count': output.count(','),
                        'question_mark_count': output.count('?'),
                    })
                else:
                    # é»˜è®¤å€¼
                    feature_dict.update({
                        'output_length': 0,
                        'repetition_score': 0,
                        'avg_word_length': 0,
                        'unique_word_ratio': 0,
                        'has_ans_prefix': 0,
                        'comma_count': 0,
                        'question_mark_count': 0,
                    })
            
            # åªé€‰æ‹©æŒ‡å®šçš„ç‰¹å¾
            selected_features = [feature_dict[f] for f in feature_set]
            features.append(selected_features)
            
            # æå–æ ‡ç­¾
            if 'squad_evaluation' in item:
                is_hallucination = item['squad_evaluation'].get('squad_is_hallucination', False)
            else:
                is_hallucination = not item.get('metrics', {}).get('hit@1', False)
            labels.append(int(is_hallucination))
        
        X = np.array(features)
        y = np.array(labels)
        
        # å¤„ç†NaNå’Œæ— ç©·å¤§å€¼
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        return X, y
    
    def calculate_repetition(self, text):
        """è®¡ç®—é‡å¤ç‡"""
        words = text.lower().split()
        if len(words) <= 1:
            return 0
        unique_words = len(set(words))
        return 1 - (unique_words / len(words))
    
    def optimize_threshold(self, y_true, y_proba):
        """ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼"""
        thresholds = np.arange(0.1, 0.9, 0.01)
        best_threshold = 0.5
        best_f1 = 0
        
        for threshold in thresholds:
            y_pred = (y_proba >= threshold).astype(int)
            f1 = f1_score(y_true, y_pred)
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
        
        return best_threshold, best_f1
    
    def train_single_config(self, config_name, X_train, y_train, X_val, y_val, X_test, y_test):
        """è®­ç»ƒå•ä¸ªç‰¹å¾é…ç½®çš„æ¨¡å‹"""
        print(f"\nğŸ” è®­ç»ƒ {config_name}...")
        print(f"   ç‰¹å¾: {self.feature_configs[config_name]['features']}")
        print(f"   è¯´æ˜: {self.feature_configs[config_name]['description']}")
        
        # ä½¿ç”¨XGBoostä½œä¸ºä¸»è¦æ¨¡å‹ï¼ˆè¡¨ç°é€šå¸¸æœ€å¥½ï¼‰
        model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=5,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42,
            eval_metric='logloss'
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # åœ¨éªŒè¯é›†ä¸Šä¼˜åŒ–é˜ˆå€¼
        y_val_proba = model.predict_proba(X_val)[:, 1]
        threshold, _ = self.optimize_threshold(y_val, y_val_proba)
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        y_test_proba = model.predict_proba(X_test)[:, 1]
        y_test_pred = (y_test_proba >= threshold).astype(int)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'auc': roc_auc_score(y_test, y_test_proba),
            'f1_score': f1_score(y_test, y_test_pred),
            'precision': precision_score(y_test, y_test_pred),
            'recall': recall_score(y_test, y_test_pred),
            'accuracy': accuracy_score(y_test, y_test_pred),
            'threshold': threshold,
            'feature_count': len(self.feature_configs[config_name]['features'])
        }
        
        print(f"   AUC: {metrics['auc']:.4f}")
        print(f"   F1-Score: {metrics['f1_score']:.4f}")
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall: {metrics['recall']:.4f}")
        
        self.results[config_name] = metrics
        self.models[config_name] = model
        
        return metrics
    
    def run_ablation_study(self, train_file, test_file):
        """è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒ"""
        print("ğŸš€ GGAç‰¹å¾æ¶ˆèå®éªŒ")
        print("="*60)
        
        # åŠ è½½æ•°æ®
        print("ğŸ“¥ åŠ è½½æ•°æ®...")
        with open(train_file, 'r', encoding='utf-8') as f:
            next(f)  # è·³è¿‡é…ç½®è¡Œ
            train_data = [json.loads(line) for line in f if line.strip()]
        
        with open(test_file, 'r', encoding='utf-8') as f:
            next(f)  # è·³è¿‡é…ç½®è¡Œ
            test_data = [json.loads(line) for line in f if line.strip()]
        
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_data)}")
        print(f"æµ‹è¯•æ ·æœ¬: {len(test_data)}")
        
        # å¯¹æ¯ä¸ªç‰¹å¾é…ç½®è¿›è¡Œå®éªŒ
        for config_name, config in self.feature_configs.items():
            feature_set = config['features']
            
            # æå–ç‰¹å¾
            X_train, y_train = self.extract_features(train_data, feature_set)
            X_test, y_test = self.extract_features(test_data, feature_set)
            
            # ç‰¹å¾æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            self.scalers[config_name] = scaler
            
            # è®­ç»ƒéªŒè¯åˆ†å‰²
            X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
                X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train
            )
            
            # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¯¹äºå•ç‰¹å¾å¯èƒ½ä¼šæœ‰é—®é¢˜ï¼Œæ‰€ä»¥åŠ try-exceptï¼‰
            try:
                smote = SMOTE(random_state=42)
                X_train_balanced, y_train_balanced = smote.fit_resample(X_train_split, y_train_split)
            except ValueError as e:
                print(f"   âš ï¸ SMOTEå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                X_train_balanced, y_train_balanced = X_train_split, y_train_split
            
            # è®­ç»ƒå’Œè¯„ä¼°
            self.train_single_config(config_name, X_train_balanced, y_train_balanced, 
                                   X_val_split, y_val_split, X_test_scaled, y_test)
    
    def plot_ablation_results(self):
        """ç»˜åˆ¶æ¶ˆèå®éªŒç»“æœ"""
        fig, axes = plt.subplots(1, 2, figsize=(6.5, 3))  # å‚è€ƒrun_rq1çš„å°ºå¯¸
        
        configs = list(self.results.keys())
        metrics = ['auc', 'recall']
        metric_names = ['AUC', 'Recall']
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        
        for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            ax = axes[i]
            
            values = [self.results[config][metric] for config in configs]
            bars = ax.bar(range(len(configs)), values, color=colors, alpha=0.8)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, (bar, value) in enumerate(zip(bars, values)):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
            
            ax.set_ylabel(metric_name)
            ax.set_title(f'{metric_name} - Feature Ablation Study')
            ax.set_xticks(range(len(configs)))
            ax.set_xticklabels(configs, rotation=45, ha='right')
            ax.set_ylim(0, 1.1)
            ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡ - æœ€é«˜åˆ†è¾¨ç‡ä¸”åŒ…å«PDF
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "results/ablation_study"
        os.makedirs(output_dir, exist_ok=True)
        
        output_path_png = f"{output_dir}/gga_ablation_study_{timestamp}.png"
        output_path_pdf = f"{output_dir}/gga_ablation_study_{timestamp}.pdf"
        
        plt.savefig(output_path_png, dpi=600, bbox_inches='tight')  # æœ€é«˜åˆ†è¾¨ç‡
        plt.savefig(output_path_pdf, bbox_inches='tight')  # ä¿å­˜PDF
        
        print(f"\nğŸ’¾ æ¶ˆèå®éªŒå›¾è¡¨ä¿å­˜è‡³: {output_path_png}")
        print(f"ğŸ’¾ PDFç‰ˆæœ¬ä¿å­˜è‡³: {output_path_pdf}")
        
        plt.show()
        
        return timestamp
    
    def plot_feature_importance_detailed(self, train_data, test_data):
        """ç»˜åˆ¶GGA-Fullçš„ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("ğŸ“Š ç”Ÿæˆç‰¹å¾é‡è¦æ€§å¯è§†åŒ–...")
        
        # æå–GGA-Fullç‰¹å¾
        feature_set = self.feature_configs['GGA_Full']['features']
        X_train, y_train = self.extract_features(train_data, feature_set)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # è®­ç»ƒéªŒè¯åˆ†å‰²
        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
            X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        # SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        try:
            smote = SMOTE(random_state=42)
            X_train_balanced, y_train_balanced = smote.fit_resample(X_train_split, y_train_split)
        except:
            X_train_balanced, y_train_balanced = X_train_split, y_train_split
        
        # è®­ç»ƒXGBoostæ¨¡å‹
        model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=5,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42,
            eval_metric='logloss'
        )
        
        model.fit(X_train_balanced, y_train_balanced)
        
        # è·å–feature importance
        feature_names = ['SAS', 'PRD', 'Output Length', 'Repetition', 
                        'Avg Word Length', 'Unique Ratio', 'Has Prefix', 
                        'Comma Count', 'Question Mark Count']
        importance_scores = model.feature_importances_
        
        # åˆ›å»ºå›¾è¡¨
        fig, ax = plt.subplots(1, 1, figsize=(6.5, 3))
        bars = ax.barh(feature_names, importance_scores)
        ax.set_xlabel('Feature Importance')
        ax.set_title('Feature Importance in GGA-Full')
        
        # ç”¨ä¸åŒé¢œè‰²åŒºåˆ†mechanistic vs surface features
        colors = ['red', 'red'] + ['lightblue'] * 7  # å‰ä¸¤ä¸ªæ˜¯mechanistic
        for bar, color in zip(bars, colors):
            bar.set_color(color)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "results/ablation_study"
        os.makedirs(output_dir, exist_ok=True)
        
        plt.savefig(f"{output_dir}/feature_importance_{timestamp}.png", 
                   dpi=600, bbox_inches='tight')  # æœ€é«˜åˆ†è¾¨ç‡
        plt.savefig(f"{output_dir}/feature_importance_{timestamp}.pdf", 
                   bbox_inches='tight')
        
        print(f"ğŸ’¾ ç‰¹å¾é‡è¦æ€§å›¾ä¿å­˜è‡³: {output_dir}/feature_importance_{timestamp}.png")
        plt.show()
        
        return timestamp
    
    def plot_roc_comparison(self, train_data, test_data):
        """ç»˜åˆ¶æ‰€æœ‰é…ç½®çš„ROCå¯¹æ¯”"""
        print("ğŸ“Š ç”ŸæˆROCæ›²çº¿å¯¹æ¯”...")
        
        # ä¸ºä¸åŒé…ç½®è®¡ç®—ROC
        configs = ['SAS_only', 'PRD_only', 'GGA_Core', 'GGA_Full']
        config_labels = ['SAS-only', 'PRD-only', 'GGA-Core', 'GGA-Full']
        colors = ['blue', 'orange', 'green', 'red']
        
        fig, ax = plt.subplots(figsize=(6.5, 3))
        
        y_scores = {}  # å­˜å‚¨æ‰€æœ‰é…ç½®çš„é¢„æµ‹æ¦‚ç‡
        
        for config, label, color in zip(configs, config_labels, colors):
            feature_set = self.feature_configs[config]['features']
            
            # æå–ç‰¹å¾
            X_train, y_train = self.extract_features(train_data, feature_set)
            X_test, y_test = self.extract_features(test_data, feature_set)
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # è®­ç»ƒéªŒè¯åˆ†å‰²
            X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
                X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train
            )
            
            # SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            try:
                smote = SMOTE(random_state=42)
                X_train_balanced, y_train_balanced = smote.fit_resample(X_train_split, y_train_split)
            except:
                X_train_balanced, y_train_balanced = X_train_split, y_train_split
            
            # è®­ç»ƒæ¨¡å‹
            model = xgb.XGBClassifier(
                n_estimators=200,
                max_depth=5,
                learning_rate=0.1,
                subsample=0.8,
                random_state=42,
                eval_metric='logloss'
            )
            
            model.fit(X_train_balanced, y_train_balanced)
            
            # åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹
            y_proba = model.predict_proba(X_test_scaled)[:, 1]
            y_scores[label] = y_proba
            
            # è®¡ç®—ROCæ›²çº¿
            fpr, tpr, _ = roc_curve(y_test, y_proba)
            roc_auc = auc(fpr, tpr)
            ax.plot(fpr, tpr, color=color, lw=2, 
                     label=f'{label} (AUC = {roc_auc:.3f})')
        
        ax.plot([0, 1], [0, 1], 'k--', lw=2)
        ax.set_xlabel('False Positive Rate')
        ax.set_ylabel('True Positive Rate')
        ax.set_title('ROC Curves: Ablation Study')
        ax.legend(loc="lower right")
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "results/ablation_study"
        os.makedirs(output_dir, exist_ok=True)
        
        plt.savefig(f"{output_dir}/roc_comparison_{timestamp}.png", 
                   dpi=600, bbox_inches='tight')  # æœ€é«˜åˆ†è¾¨ç‡
        plt.savefig(f"{output_dir}/roc_comparison_{timestamp}.pdf", 
                   bbox_inches='tight')
        
        print(f"ğŸ’¾ ROCå¯¹æ¯”å›¾ä¿å­˜è‡³: {output_dir}/roc_comparison_{timestamp}.png")
        plt.show()
        
        return timestamp
    
    def generate_ablation_table(self):
        """ç”Ÿæˆæ¶ˆèå®éªŒç»“æœè¡¨æ ¼"""
        print("\nğŸ“Š GGAç‰¹å¾æ¶ˆèå®éªŒç»“æœè¡¨æ ¼")
        print("="*80)
        
        # åˆ›å»ºè¡¨æ ¼æ•°æ®
        table_data = []
        for config_name, metrics in self.results.items():
            config = self.feature_configs[config_name]
            table_data.append({
                'Feature Set': config_name,
                'AUC': f"{metrics['auc']:.3f}",
                'F1': f"{metrics['f1_score']:.3f}",
                'Precision': f"{metrics['precision']:.3f}",
                'Recall': f"{metrics['recall']:.3f}",
                'Features': len(config['features']),
                'ç”¨é€”å®šä½': config['description']
            })
        
        # æ‰“å°è¡¨æ ¼
        df = pd.DataFrame(table_data)
        print(df.to_string(index=False))
        
        return df
    
    def save_results(self, timestamp):
        """ä¿å­˜å®éªŒç»“æœ"""
        output_dir = "results/ablation_study"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_data = {
            'timestamp': timestamp,
            'experiment_type': 'gga_feature_ablation',
            'feature_configs': self.feature_configs,
            'results': self.results,
            'summary': {
                'best_auc': max(self.results.keys(), key=lambda x: self.results[x]['auc']),
                'best_f1': max(self.results.keys(), key=lambda x: self.results[x]['f1_score']),
                'best_precision': max(self.results.keys(), key=lambda x: self.results[x]['precision']),
                'best_recall': max(self.results.keys(), key=lambda x: self.results[x]['recall'])
            }
        }
        
        with open(f"{output_dir}/ablation_results_{timestamp}.json", 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ¨¡å‹
        models_dir = f"{output_dir}/models_{timestamp}"
        os.makedirs(models_dir, exist_ok=True)
        
        for config_name, model in self.models.items():
            joblib.dump(model, f"{models_dir}/{config_name}_model.joblib")
            joblib.dump(self.scalers[config_name], f"{models_dir}/{config_name}_scaler.joblib")
        
        print(f"\nğŸ’¾ å®éªŒç»“æœä¿å­˜è‡³: {output_dir}")
        print(f"ğŸ¤– æ¨¡å‹ä¿å­˜è‡³: {models_dir}")

def main():
    # åˆ‡æ¢åˆ°æ£€æµ‹å™¨ç›®å½•
    os.chdir('/mnt/d/experiments/GraphDeEP/detector/llama2-7b/metaqa-1hop')
    
    # åˆ›å»ºæ¶ˆèå®éªŒå¯¹è±¡
    ablation = GGAAblationStudy()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    train_file = '/mnt/d/experiments/GraphDeEP/experiment_records/inference_results/llama2-7b/colab_train_simple_part1&2.jsonl'
    test_file = '/mnt/d/experiments/GraphDeEP/experiment_records/inference_results/llama2-7b/colab_test_simple.jsonl'
    
    # è¿è¡Œæ¶ˆèå®éªŒ
    ablation.run_ablation_study(train_file, test_file)
    
    # ç”Ÿæˆç»“æœè¡¨æ ¼
    df = ablation.generate_ablation_table()
    
    # ç»˜åˆ¶ç»“æœå›¾è¡¨
    timestamp = ablation.plot_ablation_results()
    
    # åŠ è½½æ•°æ®ç”¨äºè¯¦ç»†å¯è§†åŒ–
    print("\n" + "="*60)
    print("ğŸ¨ ç”Ÿæˆè¯¦ç»†å¯è§†åŒ–å›¾è¡¨...")
    
    with open(train_file, 'r', encoding='utf-8') as f:
        next(f)
        train_data = [json.loads(line) for line in f if line.strip()]
    
    with open(test_file, 'r', encoding='utf-8') as f:
        next(f)
        test_data = [json.loads(line) for line in f if line.strip()]
    
    # ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾
    print("\nğŸ“Š Figure A: Feature Importance Analysis")
    ablation.plot_feature_importance_detailed(train_data, test_data)
    
    # ç”ŸæˆROCå¯¹æ¯”å›¾
    print("\nğŸ“Š Figure B: ROC Curve Comparison")
    ablation.plot_roc_comparison(train_data, test_data)
    
    # ä¿å­˜ç»“æœ
    ablation.save_results(timestamp)
    
    print(f"\nâœ… GGAç‰¹å¾æ¶ˆèå®éªŒå®Œæˆ!")
    
    # æ˜¾ç¤ºæœ€ä½³é…ç½®
    best_config = max(ablation.results.keys(), key=lambda x: ablation.results[x]['f1_score'])
    best_metrics = ablation.results[best_config]
    
    print(f"\nğŸ† æœ€ä½³é…ç½®: {best_config}")
    print(f"   AUC: {best_metrics['auc']:.4f}")
    print(f"   F1-Score: {best_metrics['f1_score']:.4f}")
    print(f"   Precision: {best_metrics['precision']:.4f}")
    print(f"   Recall: {best_metrics['recall']:.4f}")

if __name__ == "__main__":
    main()