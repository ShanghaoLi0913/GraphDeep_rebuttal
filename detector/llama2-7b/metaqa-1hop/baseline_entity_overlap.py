#!/usr/bin/env python3
"""
Entity Overlap Baseline - ç‹¬ç«‹è°ƒè¯•ç‰ˆæœ¬
åŸºäºç­”æ¡ˆä¸é—®é¢˜å®ä½“é‡å åº¦æ£€æµ‹å¹»è§‰
"""

import numpy as np
import re
from baseline_base import BaselineBase
import argparse
from tqdm import tqdm

class EntityOverlapBaseline(BaselineBase):
    """Entity Overlap baselineæ£€æµ‹å™¨"""
    
    def __init__(self):
        super().__init__()
        print("âœ… Entity Overlap detector initialized!")
    
    def extract_entities(self, text):
        """ç®€å•çš„å®ä½“æå–ï¼šæå–å¤§å†™å­—æ¯å¼€å¤´çš„è¯å’Œå¼•å·å†…å®¹"""
        if not text:
            return set()
        
        entities = set()
        
        # æå–å¼•å·å†…çš„å†…å®¹
        quoted_entities = re.findall(r'"([^"]*)"', text)
        entities.update([e.lower().strip() for e in quoted_entities if e.strip()])
        
        # æå–å¤§å†™å­—æ¯å¼€å¤´çš„è¯ï¼ˆå¯èƒ½æ˜¯ä¸“æœ‰åè¯/å®ä½“ï¼‰
        capitalized_words = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        entities.update([e.lower().strip() for e in capitalized_words])
        
        # æå–æ•°å­—
        numbers = re.findall(r'\b\d+\b', text)
        entities.update(numbers)
        
        return entities
    
    def calculate_entity_overlap(self, question, answer):
        """è®¡ç®—é—®é¢˜å’Œç­”æ¡ˆä¹‹é—´çš„å®ä½“é‡å åº¦"""
        if not question or not answer:
            return 0.5  # é»˜è®¤ä¸­ç­‰é‡å åº¦
        
        question_entities = self.extract_entities(question)
        answer_entities = self.extract_entities(answer)
        
        if not question_entities and not answer_entities:
            return 0.5
        
        if not question_entities or not answer_entities:
            return 0.1  # ä¸€æ–¹æ²¡æœ‰å®ä½“ï¼Œé‡å åº¦å¾ˆä½
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(question_entities.intersection(answer_entities))
        union = len(question_entities.union(answer_entities))
        
        if union == 0:
            return 0.5
        
        overlap = intersection / union
        
        # è¿”å›çœŸå®çš„é‡å åº¦ï¼Œä¸åŠ ä»»ä½•äººå·¥ä¿®æ”¹
        return max(0.0, min(1.0, overlap))
    
    def extract_entity_overlap_features(self, samples):
        """æå–Entity Overlapç‰¹å¾"""
        features = []
        
        print("ğŸ” Calculating entity overlap...")
        for sample in tqdm(samples, desc="Entity Overlap"):
            question = sample.get('question', '')
            answer = sample.get('answer', '')
            
            overlap = self.calculate_entity_overlap(question, answer)
            # ğŸ”§ åè½¬é€»è¾‘ï¼šé«˜é‡å å¯èƒ½è¡¨ç¤ºå¹»è§‰ï¼ˆé‡å¤é—®é¢˜ä¸­çš„å®ä½“ï¼‰
            # ä½é‡å å¯èƒ½è¡¨ç¤ºæ­£ç¡®ï¼ˆæä¾›äº†ä¸åŒçš„ä¿¡æ¯ï¼‰
            inverted_overlap = 1.0 - overlap
            features.append(inverted_overlap)
        
        return np.array(features)
    
    def run_evaluation(self, max_train_samples=100, max_test_samples=50):
        """è¿è¡ŒEntity Overlapè¯„ä¼°"""
        print("ğŸš€ Entity Overlap Baseline Evaluation")
        print("=" * 50)
        
        # åŠ è½½æ•°æ®
        train_samples, test_samples = self.load_data(max_train_samples, max_test_samples)
        if train_samples is None or test_samples is None:
            return None
        
        # è¯„ä¼°æ–¹æ³•
        results = self.evaluate_method(
            train_samples, 
            test_samples, 
            "Entity Question Overlap",
            self.extract_entity_overlap_features
        )
        
        # ä¿å­˜ç»“æœ
        output_file = self.save_results("Entity_Question_Overlap", results)
        
        # é¢å¤–çš„åˆ†æ
        print("\nğŸ“Š Feature Analysis:")
        train_features = self.extract_entity_overlap_features(train_samples)
        test_features = self.extract_entity_overlap_features(test_samples)
        
        print(f"ğŸ“ˆ Train overlap - Range: [{train_features.min():.3f}, {train_features.max():.3f}], Mean: {train_features.mean():.3f}")
        print(f"ğŸ“ˆ Test overlap - Range: [{test_features.min():.3f}, {test_features.max():.3f}], Mean: {test_features.mean():.3f}")
        
        # æŒ‰æ ‡ç­¾åˆ†æ
        train_labels = self.extract_labels(train_samples)
        test_labels = self.extract_labels(test_samples)
        
        train_correct_features = train_features[train_labels == 0]
        train_halluc_features = train_features[train_labels == 1]
        test_correct_features = test_features[test_labels == 0]
        test_halluc_features = test_features[test_labels == 1]
        
        print(f"ğŸ” Train - Correct: {train_correct_features.mean():.3f}Â±{train_correct_features.std():.3f}")
        print(f"ğŸ” Train - Hallucination: {train_halluc_features.mean():.3f}Â±{train_halluc_features.std():.3f}")
        print(f"ğŸ” Test - Correct: {test_correct_features.mean():.3f}Â±{test_correct_features.std():.3f}")
        print(f"ğŸ” Test - Hallucination: {test_halluc_features.mean():.3f}Â±{test_halluc_features.std():.3f}")
        
        # æ˜¾ç¤ºä¸€äº›æ ·æœ¬åˆ†æ
        print("\nğŸ” Sample Analysis:")
        for i, sample in enumerate(train_samples[:3]):
            question = sample.get('question', '')
            answer = sample.get('answer', '')
            label = "Correct" if train_labels[i] == 0 else "Hallucination"
            overlap = train_features[i]
            
            q_entities = self.extract_entities(question)
            a_entities = self.extract_entities(answer)
            
            print(f"  Sample {i+1} ({label}):")
            print(f"    Question: {question[:80]}...")
            print(f"    Answer: {answer[:80]}...")
            print(f"    Q-entities: {q_entities}")
            print(f"    A-entities: {a_entities}")
            print(f"    Overlap: {overlap:.3f}")
            print()
        
        return results

def main():
    parser = argparse.ArgumentParser(description='Entity Overlap Baseline Evaluation')
    parser.add_argument('--train_samples', type=int, default=100, help='Number of training samples')
    parser.add_argument('--test_samples', type=int, default=50, help='Number of test samples')
    args = parser.parse_args()
    
    baseline = EntityOverlapBaseline()
    results = baseline.run_evaluation(args.train_samples, args.test_samples)
    
    if results:
        print(f"\nğŸ¯ Final F1-Score: {results['f1_score']:.4f}")
        print(f"ğŸ¯ Final AUC: {results['auc']:.4f}")

if __name__ == "__main__":
    main()