#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ£€æµ‹å™¨ï¼šä»…ä½¿ç”¨SAS (gass_score) å’Œ PRD (prd_score) ç‰¹å¾
ç”¨äºéªŒè¯æ ¸å¿ƒå›¾ç‰¹å¾çš„æœ‰æ•ˆæ€§
"""

import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import joblib
import warnings
warnings.filterwarnings('ignore')

class SasPrdOnlyDetector:
    """ä»…ä½¿ç”¨SASå’ŒPRDç‰¹å¾çš„æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.models = {}
        self.best_models = {}
        self.scaler = StandardScaler()
        self.results = {}
        self.feature_names = ['gass_score', 'prd_score']
        self.best_threshold = {}
        
    def extract_features(self, data):
        """æå–SASå’ŒPRDç‰¹å¾"""
        features = []
        labels = []
        
        for item in data:
            # åªæå–æ ¸å¿ƒå›¾ç‰¹å¾
            feature_dict = {
                'gass_score': item.get('gass_score', 0),
                'prd_score': item.get('prd_score', 0),
            }
            features.append(list(feature_dict.values()))
            
            # æå–æ ‡ç­¾
            if 'squad_evaluation' in item:
                is_hallucination = item['squad_evaluation'].get('squad_is_hallucination', False)
            else:
                is_hallucination = not item.get('metrics', {}).get('hit@1', False)
            labels.append(int(is_hallucination))
        
        X = np.array(features)
        y = np.array(labels)
        
        print(f"SAS+PRDç‰¹å¾: {len(self.feature_names)}")
        print(f"æ ·æœ¬æ•°é‡: {len(X)}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y)}")
        print(f"å¹»è§‰ç‡: {np.mean(y):.3f}")
        
        return X, y
    
    def optimize_threshold(self, y_true, y_proba, method_name):
        """ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼ä»¥æœ€å¤§åŒ–F1åˆ†æ•°"""
        from sklearn.metrics import f1_score
        
        thresholds = np.arange(0.1, 0.9, 0.01)
        best_threshold = 0.5
        best_f1 = 0
        
        for threshold in thresholds:
            y_pred = (y_proba >= threshold).astype(int)
            f1 = f1_score(y_true, y_pred)
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
        
        return best_threshold, best_f1
    
    def train_models(self, X_train, y_train, X_val, y_val):
        """è®­ç»ƒå¤šç§æ¨¡å‹"""
        print("\nğŸš€ è®­ç»ƒSAS+PRDæ£€æµ‹å™¨...")
        
        # å®šä¹‰æ¨¡å‹å‚æ•°
        model_configs = {
            'LogisticRegression': {
                'model': LogisticRegression(random_state=42, class_weight='balanced'),
                'params': {
                    'C': [0.01, 0.1, 1, 10, 100],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear']
                }
            },
            'RandomForest': {
                'model': RandomForestClassifier(random_state=42, class_weight='balanced'),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [3, 5, 7, None],
                    'min_samples_split': [2, 5, 10]
                }
            },
            'XGBoost': {
                'model': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [3, 5, 7],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'subsample': [0.8, 1.0]
                }
            },
            'LightGBM': {
                'model': lgb.LGBMClassifier(random_state=42, verbose=-1),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [3, 5, 7],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'num_leaves': [31, 50, 100]
                }
            }
        }
        
        # è®­ç»ƒå¹¶ä¼˜åŒ–æ¯ä¸ªæ¨¡å‹
        for name, config in model_configs.items():
            print(f"\nè®­ç»ƒ {name}...")
            
            # ç½‘æ ¼æœç´¢
            grid_search = GridSearchCV(
                config['model'], 
                config['params'], 
                cv=5, 
                scoring='f1',
                n_jobs=-1
            )
            
            grid_search.fit(X_train, y_train)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            self.best_models[name] = grid_search.best_estimator_
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            y_val_proba = grid_search.best_estimator_.predict_proba(X_val)[:, 1]
            
            # ä¼˜åŒ–é˜ˆå€¼
            threshold, f1_opt = self.optimize_threshold(y_val, y_val_proba, name)
            self.best_threshold[name] = threshold
            
            print(f"  æœ€ä½³å‚æ•°: {grid_search.best_params_}")
            print(f"  éªŒè¯é›†F1: {f1_opt:.4f}")
            print(f"  æœ€ä¼˜é˜ˆå€¼: {threshold:.3f}")
    
    def evaluate_models(self, X_test, y_test):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
        
        for name, model in self.best_models.items():
            print(f"\n{name}:")
            
            # é¢„æµ‹
            y_test_proba = model.predict_proba(X_test)[:, 1]
            y_test_pred = (y_test_proba >= self.best_threshold[name]).astype(int)
            
            # è®¡ç®—æŒ‡æ ‡
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            
            metrics = {
                'accuracy': accuracy_score(y_test, y_test_pred),
                'precision': precision_score(y_test, y_test_pred),
                'recall': recall_score(y_test, y_test_pred),
                'f1_score': f1_score(y_test, y_test_pred),
                'auc': roc_auc_score(y_test, y_test_proba)
            }
            
            self.results[name] = metrics
            
            print(f"  AUC: {metrics['auc']:.4f}")
            print(f"  F1-Score: {metrics['f1_score']:.4f}")
            print(f"  Precision: {metrics['precision']:.4f}")
            print(f"  Recall: {metrics['recall']:.4f}")
            print(f"  Accuracy: {metrics['accuracy']:.4f}")
    
    def plot_results(self):
        """ç»˜åˆ¶ç»“æœå¯¹æ¯”å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        models = list(self.results.keys())
        metrics = ['auc', 'f1_score', 'precision', 'recall']
        metric_names = ['AUC', 'F1-Score', 'Precision', 'Recall']
        
        for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            ax = axes[i//2, i%2]
            
            values = [self.results[model][metric] for model in models]
            bars = ax.bar(models, values, alpha=0.7)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{value:.3f}', ha='center', va='bottom')
            
            ax.set_ylabel(metric_name)
            ax.set_title(f'{metric_name} Comparison (SAS+PRD Only)')
            ax.set_ylim(0, 1.1)
            ax.grid(axis='y', alpha=0.3)
            
            # æ—‹è½¬xè½´æ ‡ç­¾
            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"results/sas_prd_only_results/sas_prd_only_comparison_{timestamp}.png"
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"\nğŸ’¾ ç»“æœå¯¹æ¯”å›¾ä¿å­˜è‡³: {output_path}")
        
        plt.show()
    
    def save_models_and_results(self):
        """ä¿å­˜æ¨¡å‹å’Œç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S") 
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = f"results/sas_prd_only_results/models"
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        for name, model in self.best_models.items():
            model_path = os.path.join(save_dir, f"{name}_sas_prd_only_{timestamp}.joblib")
            joblib.dump(model, model_path)
        
        # ä¿å­˜scaler
        scaler_path = os.path.join(save_dir, f"scaler_sas_prd_only_{timestamp}.joblib")
        joblib.dump(self.scaler, scaler_path)
        
        # ä¿å­˜é˜ˆå€¼
        threshold_path = os.path.join(save_dir, f"thresholds_sas_prd_only_{timestamp}.json")
        with open(threshold_path, 'w') as f:
            json.dump(self.best_threshold, f, indent=2)
        
        # ä¿å­˜ç»“æœ
        results_path = f"results/sas_prd_only_results/sas_prd_only_results_{timestamp}.json"
        results_data = {
            'timestamp': timestamp,
            'feature_names': self.feature_names,
            'model_results': self.results,
            'best_thresholds': self.best_threshold,
            'experiment_type': 'sas_prd_only_detection'
        }
        
        with open(results_path, 'w') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æ¨¡å‹å’Œç»“æœå·²ä¿å­˜:")
        print(f"  æ¨¡å‹ç›®å½•: {save_dir}")
        print(f"  ç»“æœæ–‡ä»¶: {results_path}")
        
        return results_data
    
    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("ğŸš€ å¼€å§‹SAS+PRDæ£€æµ‹å™¨å®éªŒ")
        print("="*50)
        
        # åŠ è½½æ•°æ®
        train_files = [
            '/mnt/d/experiments/GraphDeEP/experiment_records/inference_results/llama2-7b/colab_train_simple_part1&2.jsonl',
            '/mnt/d/experiments/GraphDeEP/experiment_records/inference_results/llama2-7b/colab_train_simple_part1.jsonl'
        ]
        
        test_files = [
            '/mnt/d/experiments/GraphDeEP/experiment_records/inference_results/llama2-7b/colab_test_simple.jsonl'
        ]
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_data = []
        for train_file in train_files:
            if os.path.exists(train_file):
                print(f"ğŸ“¥ åŠ è½½è®­ç»ƒæ•°æ®: {train_file}")
                with open(train_file, 'r', encoding='utf-8') as f:
                    next(f)  # è·³è¿‡é…ç½®è¡Œ
                    for line in f:
                        if line.strip():
                            train_data.append(json.loads(line))
                break
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = []
        for test_file in test_files:
            if os.path.exists(test_file):
                print(f"ğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
                with open(test_file, 'r', encoding='utf-8') as f:
                    next(f)  # è·³è¿‡é…ç½®è¡Œ
                    for line in f:
                        if line.strip():
                            test_data.append(json.loads(line))
                break
        
        if not train_data or not test_data:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return None
        
        # æå–ç‰¹å¾
        print(f"\nğŸ“Š æå–SAS+PRDç‰¹å¾...")
        X_train, y_train = self.extract_features(train_data)
        X_test, y_test = self.extract_features(test_data)
        
        # ç‰¹å¾ç¼©æ”¾
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # è®­ç»ƒéªŒè¯åˆ†å‰²
        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
            X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        print(f"è®­ç»ƒé›†: {len(X_train_split)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(X_val_split)} æ ·æœ¬") 
        print(f"æµ‹è¯•é›†: {len(X_test_scaled)} æ ·æœ¬")
        
        # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        smote = SMOTE(random_state=42)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_split, y_train_split)
        
        print(f"SMOTEåè®­ç»ƒé›†: {len(X_train_balanced)} æ ·æœ¬")
        print(f"å¹³è¡¡åæ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_train_balanced)}")
        
        # è®­ç»ƒæ¨¡å‹
        self.train_models(X_train_balanced, y_train_balanced, X_val_split, y_val_split)
        
        # è¯„ä¼°æ¨¡å‹
        self.evaluate_models(X_test_scaled, y_test)
        
        # ç»˜åˆ¶ç»“æœ
        self.plot_results()
        
        # ä¿å­˜æ¨¡å‹å’Œç»“æœ
        results_data = self.save_models_and_results()
        
        print(f"\nâœ… SAS+PRDæ£€æµ‹å™¨å®éªŒå®Œæˆ!")
        
        # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
        best_model_name = max(self.results.keys(), key=lambda x: self.results[x]['f1_score'])
        best_metrics = self.results[best_model_name]
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"  AUC: {best_metrics['auc']:.4f}")
        print(f"  F1-Score: {best_metrics['f1_score']:.4f}")
        print(f"  Precision: {best_metrics['precision']:.4f}")
        print(f"  Recall: {best_metrics['recall']:.4f}")
        
        return results_data

def main():
    # åˆ‡æ¢åˆ°æ£€æµ‹å™¨ç›®å½•
    os.chdir('/mnt/d/experiments/GraphDeEP/detector/llama2-7b/metaqa-1hop')
    
    # è¿è¡Œå®éªŒ
    detector = SasPrdOnlyDetector()
    results = detector.run_experiment()
    
    return results

if __name__ == "__main__":
    main()