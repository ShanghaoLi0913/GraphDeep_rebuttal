#!/usr/bin/env python3
"""
Embedding-Based Semantic Divergence Baseline - ç‹¬ç«‹è°ƒè¯•ç‰ˆæœ¬
åŸºäºé—®é¢˜å’Œç­”æ¡ˆåœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„åç¦»ç¨‹åº¦æ£€æµ‹å¹»è§‰
"""

import numpy as np
from sentence_transformers import SentenceTransformer
from baseline_base import BaselineBase
import argparse
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import jensenshannon
import torch

class EmbeddingSemanticDivergenceBaseline(BaselineBase):
    """Embedding-Based Semantic Divergence baselineæ£€æµ‹å™¨"""
    
    def __init__(self):
        super().__init__()
        print("ğŸ“¥ Loading SentenceTransformer model for semantic divergence...")
        # ğŸš€ ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹å’ŒGPUä¼˜åŒ–
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)
        if torch.cuda.is_available():
            print(f"ğŸš€ SentenceTransformer loaded on {self.device}")
        print("âœ… SentenceTransformer loaded!")
    
    def calculate_semantic_divergence(self, question, answer):
        """è®¡ç®—é—®é¢˜å’Œç­”æ¡ˆä¹‹é—´çš„è¯­ä¹‰åç¦»åº¦"""
        if not question or not answer:
            return 0.5  # é»˜è®¤ä¸­ç­‰åç¦»åº¦
        
        try:
            # è·å–é—®é¢˜å’Œç­”æ¡ˆçš„è¯­ä¹‰å‘é‡
            question_embedding = self.sentence_model.encode([question])
            answer_embedding = self.sentence_model.encode([answer])
            
            # æ–¹æ³•1: ä½™å¼¦è·ç¦» (1 - ä½™å¼¦ç›¸ä¼¼åº¦)
            cosine_sim = cosine_similarity(question_embedding, answer_embedding)[0][0]
            cosine_divergence = 1.0 - cosine_sim
            
            # æ–¹æ³•2: æ¬§å‡ é‡Œå¾—è·ç¦»å½’ä¸€åŒ–
            euclidean_dist = np.linalg.norm(question_embedding[0] - answer_embedding[0])
            # å½’ä¸€åŒ–åˆ°[0,1]ï¼Œå‡è®¾æœ€å¤§è·ç¦»çº¦ä¸º2*sqrt(384) â‰ˆ 39.2 (å¯¹äº384ç»´å‘é‡)
            max_possible_dist = 2 * np.sqrt(question_embedding[0].shape[0])
            euclidean_divergence = min(euclidean_dist / max_possible_dist, 1.0)
            
            # æ–¹æ³•3: å‘é‡è§’åº¦åç¦»
            # è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è§’åº¦(å¼§åº¦)ï¼Œç„¶åå½’ä¸€åŒ–åˆ°[0,1]
            dot_product = np.dot(question_embedding[0], answer_embedding[0])
            norms = np.linalg.norm(question_embedding[0]) * np.linalg.norm(answer_embedding[0])
            cos_angle = np.clip(dot_product / norms, -1.0, 1.0)
            angle_radians = np.arccos(cos_angle)
            angle_divergence = angle_radians / np.pi  # å½’ä¸€åŒ–åˆ°[0,1]
            
            # æ–¹æ³•4: åŸºäºæ¦‚ç‡åˆ†å¸ƒçš„Jensen-Shannonæ•£åº¦
            # å°†embeddingè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆé€šè¿‡softmaxï¼‰
            q_probs = torch.softmax(torch.tensor(question_embedding[0]), dim=0).numpy()
            a_probs = torch.softmax(torch.tensor(answer_embedding[0]), dim=0).numpy()
            js_divergence = jensenshannon(q_probs, a_probs)
            
            # ç»¼åˆå¤šç§åç¦»åº¦åº¦é‡ï¼Œç»™ä¸åŒæ–¹æ³•ä¸åŒæƒé‡
            combined_divergence = (
                0.4 * cosine_divergence +      # ä½™å¼¦è·ç¦»æƒé‡æœ€é«˜
                0.2 * euclidean_divergence +   # æ¬§å‡ é‡Œå¾—è·ç¦»
                0.2 * angle_divergence +       # è§’åº¦åç¦»
                0.2 * js_divergence           # JSæ•£åº¦
            )
            
            return max(0.0, min(1.0, combined_divergence))
            
        except Exception as e:
            print(f"âš ï¸ Error calculating semantic divergence: {e}")
            return 0.5
    
    def extract_semantic_divergence_features(self, samples):
        """æå–Embedding-Based Semantic Divergenceç‰¹å¾"""
        features = []
        
        print("ğŸ” Calculating embedding-based semantic divergence...")
        for sample in tqdm(samples, desc="Semantic Divergence"):
            question = sample.get('question', '')
            answer = sample.get('answer', '')
            
            divergence = self.calculate_semantic_divergence(question, answer)
            # ğŸ”§ ç›´æ¥ä½¿ç”¨divergenceï¼šé«˜åç¦»åº¦è¡¨ç¤ºå¹»è§‰ï¼Œä½åç¦»åº¦è¡¨ç¤ºæ­£ç¡®
            # divergenceè¶Šå¤§ = è¯­ä¹‰å·®å¼‚è¶Šå¤§ = æ›´å¯èƒ½æ˜¯å¹»è§‰
            features.append(divergence)
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if len(features) % 20 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return np.array(features)
    
    def run_evaluation(self, max_train_samples=100, max_test_samples=50):
        """è¿è¡ŒEmbedding-Based Semantic Divergenceè¯„ä¼°"""
        print("ğŸš€ Embedding-Based Semantic Divergence Baseline Evaluation")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        train_samples, test_samples = self.load_data(max_train_samples, max_test_samples)
        if train_samples is None or test_samples is None:
            return None
        
        # è¯„ä¼°æ–¹æ³•
        results = self.evaluate_method(
            train_samples, 
            test_samples, 
            "Embedding-Based Semantic Divergence",
            self.extract_semantic_divergence_features
        )
        
        # ä¿å­˜ç»“æœ
        output_file = self.save_results("Embedding_Based_Semantic_Divergence", results)
        
        # é¢å¤–çš„åˆ†æ
        print("\nğŸ“Š Feature Analysis:")
        train_features = self.extract_semantic_divergence_features(train_samples)
        test_features = self.extract_semantic_divergence_features(test_samples)
        
        print(f"ğŸ“ˆ Train divergence - Range: [{train_features.min():.3f}, {train_features.max():.3f}], Mean: {train_features.mean():.3f}")
        print(f"ğŸ“ˆ Test divergence - Range: [{test_features.min():.3f}, {test_features.max():.3f}], Mean: {test_features.mean():.3f}")
        
        # æŒ‰æ ‡ç­¾åˆ†æ
        train_labels = self.extract_labels(train_samples)
        test_labels = self.extract_labels(test_samples)
        
        train_correct_features = train_features[train_labels == 0]
        train_halluc_features = train_features[train_labels == 1]
        test_correct_features = test_features[test_labels == 0]
        test_halluc_features = test_features[test_labels == 1]
        
        print(f"ğŸ” Train - Correct: {train_correct_features.mean():.3f}Â±{train_correct_features.std():.3f}")
        print(f"ğŸ” Train - Hallucination: {train_halluc_features.mean():.3f}Â±{train_halluc_features.std():.3f}")
        print(f"ğŸ” Test - Correct: {test_correct_features.mean():.3f}Â±{test_correct_features.std():.3f}")
        print(f"ğŸ” Test - Hallucination: {test_halluc_features.mean():.3f}Â±{test_halluc_features.std():.3f}")
        
        # æ˜¾ç¤ºæ–¹æ³•ç»„æˆåˆ†æ
        print("\nğŸ” Divergence Method Analysis:")
        sample_q = test_samples[0].get('question', '') if test_samples else "What is AI?"
        sample_a = test_samples[0].get('answer', '') if test_samples else "Artificial Intelligence"
        
        if sample_q and sample_a:
            try:
                q_emb = self.sentence_model.encode([sample_q])
                a_emb = self.sentence_model.encode([sample_a])
                
                cosine_sim = cosine_similarity(q_emb, a_emb)[0][0]
                cosine_div = 1.0 - cosine_sim
                
                euclidean_dist = np.linalg.norm(q_emb[0] - a_emb[0])
                max_dist = 2 * np.sqrt(q_emb[0].shape[0])
                euclidean_div = min(euclidean_dist / max_dist, 1.0)
                
                print(f"  Sample question: {sample_q[:60]}...")
                print(f"  Sample answer: {sample_a[:60]}...")
                print(f"  Cosine divergence: {cosine_div:.3f}")
                print(f"  Euclidean divergence: {euclidean_div:.3f}")
                print(f"  Combined divergence: {self.calculate_semantic_divergence(sample_q, sample_a):.3f}")
                
            except Exception as e:
                print(f"  Analysis error: {e}")
        
        return results

def main():
    parser = argparse.ArgumentParser(description='Embedding-Based Semantic Divergence Baseline Evaluation')
    parser.add_argument('--train_samples', type=int, default=100, help='Number of training samples')
    parser.add_argument('--test_samples', type=int, default=50, help='Number of test samples')
    args = parser.parse_args()
    
    baseline = EmbeddingSemanticDivergenceBaseline()
    results = baseline.run_evaluation(args.train_samples, args.test_samples)
    
    if results:
        print(f"\nğŸ¯ Final F1-Score: {results['f1_score']:.4f}")
        print(f"ğŸ¯ Final AUC: {results['auc']:.4f}")

if __name__ == "__main__":
    main()