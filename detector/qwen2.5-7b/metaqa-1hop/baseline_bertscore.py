#!/usr/bin/env python3
"""
BERTScore Baseline - ç‹¬ç«‹è°ƒè¯•ç‰ˆæœ¬
åŸºäºå›ç­”ä¸é—®é¢˜çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹å¹»è§‰
"""

import numpy as np
from sentence_transformers import SentenceTransformer
from baseline_base import BaselineBase
import argparse
from tqdm import tqdm

class BERTScoreBaseline(BaselineBase):
    """BERTScoreç›¸ä¼¼åº¦baselineæ£€æµ‹å™¨"""
    
    def __init__(self):
        super().__init__()
        print("ğŸ“¥ Loading BERTScore model...")
        try:
            from bert_score import score
            print("âœ… BERTScore library loaded!")
            self.use_real_bertscore = True
        except ImportError:
            print("âš ï¸ bert_score not available, using SentenceTransformer similarity")
            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
            self.use_real_bertscore = False
        print("âœ… BERTScore model ready!")
    
    def extract_bertscore_features(self, samples):
        """æå–BERTScoreç‰¹å¾"""
        features = []
        
        if self.use_real_bertscore:
            print("ğŸ” Calculating real BERTScore F1...")
            try:
                from bert_score import score
                questions = [sample.get('question', '') for sample in samples]
                answers = [sample.get('answer', '') for sample in samples]
                
                # è¿‡æ»¤ç©ºæ–‡æœ¬
                valid_pairs = [(q, a) for q, a in zip(questions, answers) if q.strip() and a.strip()]
                if not valid_pairs:
                    return np.array([0.0] * len(samples))
                
                valid_questions, valid_answers = zip(*valid_pairs)
                
                # è®¡ç®—çœŸæ­£çš„BERTScore (candidate, reference)
                P, R, F1 = score(list(valid_answers), list(valid_questions), 
                                lang="en", verbose=False, device=self.device)
                
                # å°†ç»“æœæ˜ å°„å›åŸå§‹æ ·æœ¬
                bert_scores = F1.cpu().numpy().tolist()
                
                # å¤„ç†æ— æ•ˆæ ·æœ¬
                result_idx = 0
                for sample in samples:
                    q, a = sample.get('question', ''), sample.get('answer', '')
                    if q.strip() and a.strip():
                        features.append(bert_scores[result_idx])
                        result_idx += 1
                    else:
                        features.append(0.0)
                        
                print(f"âœ… Calculated BERTScore for {len(valid_pairs)} valid pairs")
                
            except Exception as e:
                print(f"âš ï¸ Error with real BERTScore: {e}, falling back to similarity")
                self.use_real_bertscore = False
                
        if not self.use_real_bertscore:
            print("ğŸ” Calculating sentence similarity (fallback)...")
            for sample in tqdm(samples, desc="Sentence Similarity"):
                question = sample.get('question', '')
                answer = sample.get('answer', '')
                
                if not question or not answer:
                    features.append(0.0)
                    continue
                
                try:
                    # è®¡ç®—é—®é¢˜å’Œç­”æ¡ˆçš„å‘é‡è¡¨ç¤º
                    question_embedding = self.sentence_model.encode([question])
                    answer_embedding = self.sentence_model.encode([answer])
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(question_embedding[0], answer_embedding[0]) / (
                        np.linalg.norm(question_embedding[0]) * np.linalg.norm(answer_embedding[0])
                    )
                    
                    features.append(float(similarity))
                    
                except Exception as e:
                    print(f"âš ï¸ Error calculating similarity for sample: {e}")
                    features.append(0.0)
        
        return np.array(features)
    
    def run_evaluation(self, max_train_samples=100, max_test_samples=50):
        """è¿è¡ŒBERTScoreè¯„ä¼°"""
        print("ğŸš€ BERTScore Baseline Evaluation")
        print("=" * 50)
        
        # åŠ è½½æ•°æ®
        train_samples, test_samples = self.load_data(max_train_samples, max_test_samples)
        if train_samples is None or test_samples is None:
            return None
        
        # è¯„ä¼°æ–¹æ³•
        results = self.evaluate_method(
            train_samples, 
            test_samples, 
            "BERTScore vs Question",
            self.extract_bertscore_features
        )
        
        # ä¿å­˜ç»“æœ
        output_file = self.save_results("BERTScore_vs_Question", results)
        
        # é¢å¤–çš„åˆ†æ
        print("\nğŸ“Š Feature Analysis:")
        train_features = self.extract_bertscore_features(train_samples)
        test_features = self.extract_bertscore_features(test_samples)
        
        print(f"ğŸ“ˆ Train features - Range: [{train_features.min():.3f}, {train_features.max():.3f}], Mean: {train_features.mean():.3f}")
        print(f"ğŸ“ˆ Test features - Range: [{test_features.min():.3f}, {test_features.max():.3f}], Mean: {test_features.mean():.3f}")
        
        # æŒ‰æ ‡ç­¾åˆ†æ
        train_labels = self.extract_labels(train_samples)
        test_labels = self.extract_labels(test_samples)
        
        train_correct_features = train_features[train_labels == 0]
        train_halluc_features = train_features[train_labels == 1]
        test_correct_features = test_features[test_labels == 0]
        test_halluc_features = test_features[test_labels == 1]
        
        print(f"ğŸ” Train - Correct: {train_correct_features.mean():.3f}Â±{train_correct_features.std():.3f}")
        print(f"ğŸ” Train - Hallucination: {train_halluc_features.mean():.3f}Â±{train_halluc_features.std():.3f}")
        print(f"ğŸ” Test - Correct: {test_correct_features.mean():.3f}Â±{test_correct_features.std():.3f}")
        print(f"ğŸ” Test - Hallucination: {test_halluc_features.mean():.3f}Â±{test_halluc_features.std():.3f}")
        
        return results

def main():
    parser = argparse.ArgumentParser(description='BERTScore Baseline Evaluation')
    parser.add_argument('--train_samples', type=int, default=100, help='Number of training samples')
    parser.add_argument('--test_samples', type=int, default=50, help='Number of test samples')
    args = parser.parse_args()
    
    baseline = BERTScoreBaseline()
    results = baseline.run_evaluation(args.train_samples, args.test_samples)
    
    if results:
        print(f"\nğŸ¯ Final F1-Score: {results['f1_score']:.4f}")
        print(f"ğŸ¯ Final AUC: {results['auc']:.4f}")

if __name__ == "__main__":
    main()