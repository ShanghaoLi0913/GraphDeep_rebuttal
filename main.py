"""
ä¸»ç¨‹åºå…¥å£

ç”¨äºŽè®¡ç®—Triple Utilization Score (TUS) å’Œ FFN-Gold Alignment Score (FGAS)ï¼ŒåŸºäºŽLlama-2æ¨¡åž‹çš„attention patternså’Œhidden states

ä½¿ç”¨æ–¹æ³•:
    1. å¤„ç†æ‰€æœ‰æ ·æœ¬:
       python main.py
    
    2. å¤„ç†æŒ‡å®šæ•°é‡çš„æ ·æœ¬(å¦‚100ä¸ª):
       python main.py -n 100
    
    3. å¼€å¯è°ƒè¯•æ¨¡å¼å¤„ç†æ ·æœ¬:
       python main.py -n 10 -d
    
è¾“å‡ºæ–‡ä»¶æ ¼å¼(inference_results_YYYYMMDD_HHMMSS.jsonl):
    - ç¬¬ä¸€è¡Œ: é…ç½®ä¿¡æ¯(æ—¶é—´æˆ³ã€æ¨¡åž‹åç§°ã€æ ·æœ¬æ•°é‡ç­‰)
    - ä¸­é—´è¡Œ: æ¯ä¸ªæ ·æœ¬çš„å¤„ç†ç»“æžœ(åŒ…å«é—®é¢˜ã€ç­”æ¡ˆã€TUSåˆ†æ•°ã€FGASåˆ†æ•°ç­‰)
    - æœ€åŽè¡Œ: ç»Ÿè®¡ä¿¡æ¯(æ€»æ ·æœ¬æ•°ã€æœ‰æ•ˆæ ·æœ¬æ•°ã€å¹³å‡TUSåˆ†æ•°ã€å¹³å‡FGASåˆ†æ•°ç­‰)
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import os
import re  # æ·»åŠ reæ¨¡å—å¯¼å…¥
from datetime import datetime
from tqdm import tqdm
from dataset_processor import build_external_context, load_entities_and_relations
from redeep_metrics import calculate_tus, calculate_ntus
from fgas_metrics import calculate_fgas  # æ·»åŠ FGASå¯¼å…¥
from typing import List, Dict
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
def setup_logging(debug_mode):
    if debug_mode:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    return logging.getLogger(__name__)

def clean_answer(answer):
    """æ¸…ç†å•ä¸ªç­”æ¡ˆ"""
    # ç§»é™¤å¤šä½™çš„å¥å­ç»“æž„
    answer = re.sub(r'.*(stars in|acts in|appears in)\s*', '', answer, flags=re.IGNORECASE)
    
    # ç§»é™¤å¹´ä»½
    answer = re.sub(r'\s*\(\d{4}\)', '', answer)
    
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·
    answer = answer.strip('.,!?')
    
    # è½¬æ¢ä¸ºå°å†™å¹¶åŽ»é™¤é¦–å°¾ç©ºæ ¼
    return answer.lower().strip()

def extract_answer(text):
    """ä¿®å¤çš„ç­”æ¡ˆæå–å‡½æ•°"""
    answers = []
    
    # å…ˆæŒ‰è¡Œåˆ†å‰²
    lines = text.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        
        if line.startswith('ans:'):
            content = line[4:].strip()  # åŽ»æŽ‰ "ans:" å‰ç¼€
            
            # å¦‚æžœå†…å®¹åŒ…å«é€—å·ï¼Œè¯´æ˜Žæ˜¯å¤šä¸ªç­”æ¡ˆ
            if ',' in content:
                # æŒ‰é€—å·åˆ†å‰²å¹¶æ¸…ç†æ¯ä¸ªç­”æ¡ˆ
                parts = [part.strip() for part in content.split(',')]
                for part in parts:
                    if part:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                        cleaned = clean_answer(part)
                        if cleaned:
                            answers.append(cleaned)
            else:
                # å•ä¸ªç­”æ¡ˆ
                cleaned = clean_answer(content)
                if cleaned:
                    answers.append(cleaned)
    
    return answers

def is_answer_correct(predicted_answers: list, golden_answers: list) -> bool:
    """
    åˆ¤æ–­é¢„æµ‹ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼ˆHit@1ï¼‰
    
    å‚æ•°:
        predicted_answers: é¢„æµ‹çš„ç­”æ¡ˆåˆ—è¡¨
        golden_answers: æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨
    
    è¿”å›ž:
        bool: æ˜¯å¦æ­£ç¡®ï¼ˆåªè¦æœ‰ä¸€ä¸ªç­”æ¡ˆæ­£ç¡®å°±è¿”å›žTrueï¼‰
    """
    # é¢„å¤„ç†æ‰€æœ‰ç­”æ¡ˆ
    processed_pred = [ans.lower().strip() for ans in predicted_answers]
    processed_gold = [ans.lower().strip() for ans in golden_answers]
    
    # è§„èŒƒåŒ–ç©ºæ ¼
    pred_norm = [' '.join(ans.split()) for ans in processed_pred]
    gold_norm = [' '.join(ans.split()) for ans in processed_gold]
    
    # åªè¦æœ‰ä¸€ä¸ªé¢„æµ‹ç­”æ¡ˆåœ¨æ ‡å‡†ç­”æ¡ˆä¸­å°±ç®—å¯¹
    return any(pred in gold_norm for pred in pred_norm)

def calculate_metrics(predicted_answers: list, golden_answers: list) -> dict:
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    
    å‚æ•°:
        predicted_answers: é¢„æµ‹çš„ç­”æ¡ˆåˆ—è¡¨
        golden_answers: æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨
    
    è¿”å›ž:
        dict: åŒ…å«Hit@1æŒ‡æ ‡çš„å­—å…¸
    """
    if not predicted_answers or not golden_answers:
        return {'hit@1': False}
        
    # é¢„å¤„ç†æ‰€æœ‰ç­”æ¡ˆï¼ˆè½¬æ¢ä¸ºå°å†™å¹¶æ ‡å‡†åŒ–ç©ºæ ¼ï¼‰
    # å¯¹äºŽé¢„æµ‹ç­”æ¡ˆï¼Œæˆ‘ä»¬åªå–ç¬¬ä¸€ä¸ªç­”æ¡ˆï¼ˆå¦‚æžœåŒ…å«å¤šä¸ªç­”æ¡ˆï¼Œå–ç¬¬ä¸€ä¸ªï¼‰
    first_pred = predicted_answers[0].lower().strip()
    if ',' in first_pred:
        first_pred = first_pred.split(',')[0].strip()
    first_pred = ' '.join(first_pred.split())
    
    gold_norm = [' '.join(ans.lower().strip().split()) for ans in golden_answers]
    
    # è®¡ç®—Hit@1ï¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªé¢„æµ‹æ˜¯å¦åœ¨æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨ä¸­
    hit_at_1 = first_pred in gold_norm
    
    # å°†é¢„æµ‹ç­”æ¡ˆè½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼
    pred_list = []
    for pred in predicted_answers:
        if ',' in pred:
            pred_list.extend([p.strip() for p in pred.split(',')])
        else:
            pred_list.append(pred.strip())
    
    print(f"\n=== DEBUG: Hit@1 Calculation ===")
    print(f"First prediction: {first_pred}")
    print(f"All predictions: {pred_list}")
    print(f"Gold answers: {gold_norm}")
    print(f"Hit@1: {hit_at_1}")
    print("=== End of Debug ===\n")
    
    return {
        'hit@1': hit_at_1
    }

def find_answer_position(output_text: str, answer_text: str, tokenizer) -> dict:
    """
    åœ¨è¾“å‡ºæ–‡æœ¬ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®
    
    å‚æ•°:
        output_text: æ¨¡åž‹ç”Ÿæˆçš„å®Œæ•´è¾“å‡ºæ–‡æœ¬
        answer_text: æå–å‡ºçš„ç­”æ¡ˆæ–‡æœ¬
        tokenizer: åˆ†è¯å™¨
    
    è¿”å›ž:
        åŒ…å«ç­”æ¡ˆèµ·å§‹å’Œç»“æŸä½ç½®çš„å­—å…¸
    """
    # å¦‚æžœç­”æ¡ˆä¸ºç©ºï¼Œè¿”å›žé»˜è®¤ä½ç½®
    if not answer_text:
        return {'start_idx': 0, 'end_idx': 0}
    
    # ç¼–ç å®Œæ•´è¾“å‡ºæ–‡æœ¬
    output_tokens = tokenizer.encode(output_text, add_special_tokens=False)
    
    # ç¼–ç ç­”æ¡ˆæ–‡æœ¬
    answer_tokens = tokenizer.encode(answer_text, add_special_tokens=False)
    
    # åœ¨è¾“å‡ºtokensä¸­æŸ¥æ‰¾ç­”æ¡ˆtokens
    for i in range(len(output_tokens) - len(answer_tokens) + 1):
        if output_tokens[i:i+len(answer_tokens)] == answer_tokens:
            return {
                'start_idx': i,
                'end_idx': i + len(answer_tokens) - 1
            }
    
    # å¦‚æžœæ‰¾ä¸åˆ°å®Œå…¨åŒ¹é…ï¼Œè¿”å›žé»˜è®¤ä½ç½®
    return {'start_idx': 0, 'end_idx': len(output_tokens) - 1}

def process_sample(sample, model, tokenizer, entity_list, relation_list, debug=False):
    """å¤„ç†å•ä¸ªæ ·æœ¬"""
    logger = logging.getLogger(__name__)
    try:
        # æž„å»ºç³»ç»Ÿæç¤º
        system_prompt = """Based on the triples retrieved from a knowledge graph, please answer the question. Please return ONLY the answer entities as a list, each prefixed with "ans:". Do not include explanations or reasoning."""

        # æž„å»ºä¸‰å…ƒç»„æ–‡æœ¬
        if not sample.get('trimmed_triples'):
            return {
                'error': 'No trimmed triples found',
                'tus_score': 0.0,
                'fgas_score': 0.0,
                'metrics': {'hit@1': False}
            }
            
        triples_text = "Knowledge:\n"
        for h, r, t in sample['trimmed_triples']:
            triples_text += f"{h} {r} {t}\n"
        
        # æž„å»ºç”¨æˆ·è¾“å…¥ï¼ˆåŒ…å«ä¸‰å…ƒç»„å’Œé—®é¢˜ï¼‰
        user_content = f"{triples_text}\nQuestion: {sample['question']}\nAnswer:"
        
        # æž„å»ºå®Œæ•´çš„æç¤º
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]
        
        # å‡†å¤‡è¾“å…¥
        inputs = tokenizer.apply_chat_template(messages, return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆç­”æ¡ˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                num_return_sequences=1,
                output_attentions=True,
                temperature=0.7,
                top_p=0.9
            )
        
        # æ‰¾åˆ°ç”Ÿæˆæ–‡æœ¬çš„å¼€å§‹ä½ç½®ï¼ˆåœ¨[/INST]ä¹‹åŽï¼‰
        input_length = len(inputs['input_ids'][0])
        generated_ids = outputs[0][input_length:]
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        print("\n=== DEBUG: Tokenizer Decoding ===")
        output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
        print(f"Raw decoded text: {repr(output_text)}")
        print("=== End of Decoding Debug ===\n")
        
        # æå–ç­”æ¡ˆ
        predicted_answers = extract_answer(output_text)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = calculate_metrics(predicted_answers, sample['golden_texts'])
        
        # èŽ·å–æ³¨æ„åŠ›æƒé‡
        attention_weights = torch.stack([output.attentions[-1] for output in outputs])
        
        # æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®
        answer_start_idx = len(inputs['input_ids'][0]) - 1  # æœ€åŽä¸€ä¸ªè¾“å…¥tokençš„ä½ç½®
        answer_end_idx = len(outputs[0]) - 1  # æœ€åŽä¸€ä¸ªç”Ÿæˆtokençš„ä½ç½®
        
        # è®¡ç®—TUSåˆ†æ•°
        tus_score = calculate_tus(
            attention_weights=attention_weights,
            external_context=build_external_context(sample, tokenizer, entity_list, relation_list),
            gold_triples=sample['gold_triples'],
            answer_start_idx=answer_start_idx,
            answer_end_idx=answer_end_idx,
            input_ids=outputs[0],
            tokenizer=tokenizer,
            debug=debug
        )
        
        # è®¡ç®—nTUSåˆ†æ•°ï¼ˆå½’ä¸€åŒ–çš„TUSï¼‰
        ntus_score = calculate_ntus(
            attention_weights=attention_weights,
            external_context=build_external_context(sample, tokenizer, entity_list, relation_list),
            gold_triples=sample['gold_triples'],
            trimmed_triples=sample['trimmed_triples'],
            answer_start_idx=answer_start_idx,
            answer_end_idx=answer_end_idx,
            input_ids=outputs[0],
            tokenizer=tokenizer,
            debug=debug
        )
        
        # è®¡ç®—FGASåˆ†æ•° - ä½¿ç”¨æ­£ç¡®çš„ç­”æ¡ˆä½ç½®å’ŒGolden Expansion Set
        fgas_answer_start = len(inputs['input_ids'][0])  # ç”Ÿæˆæ–‡æœ¬çš„çœŸæ­£å¼€å§‹ä½ç½®
        fgas_answer_end = len(outputs[0]) - 1  # ç”Ÿæˆæ–‡æœ¬çš„ç»“æŸä½ç½®
        
        # ðŸ”¥ FGASä½¿ç”¨æ‰©å±•é›†åˆï¼ˆè¯­ä¹‰ä¸°å¯Œåº¦ï¼‰
        fgas_golden_triples = sample.get('golden_expansion_set', sample['gold_triples'])
        
        fgas_score = calculate_fgas(
            model=model,
            tokenizer=tokenizer,
            input_ids=outputs[0],
            gold_triples=fgas_golden_triples,
            answer_start_idx=fgas_answer_start,
            answer_end_idx=fgas_answer_end,
            debug=debug
        )
        
        return {
            'question': sample['question'],
            'model_input': user_content,
            'model_output': output_text,
            'answer': predicted_answers,
            'golden_answers': sample['golden_texts'],
            'metrics': metrics,
            'tus_score': tus_score,
            'ntus_score': ntus_score,
            'fgas_score': fgas_score,
            'extracted_answers': predicted_answers,
            'raw_model_output': output_text
        }
            
    except Exception as e:
        logger.error(f"å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {str(e)}")
        return {
            'error': str(e),
            'tus_score': 0.0,
            'ntus_score': 0.0,
            'fgas_score': 0.0,
            'metrics': {'hit@1': False}
        }

def main(num_samples=None, debug=False):
    """
    ä¸»å‡½æ•°
    
    å‚æ•°:
        num_samples: è¦å¤„ç†çš„æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ ·æœ¬
        debug: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
    """
    # 1. è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®æ‰¹å¤„ç†å¤§å°
    batch_size = 2  # å‡å°æ‰¹å¤„ç†å¤§å°ä»¥é€‚åº”æ˜¾å­˜
    
    # 2. åŠ è½½æ¨¡åž‹å’Œtokenizer
    model_name = "meta-llama/Llama-2-7b-chat-hf"
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,  # åœ¨GPUä¸Šä½¿ç”¨åŠç²¾åº¦
        device_map="auto",  # è‡ªåŠ¨å¤„ç†è®¾å¤‡æ˜ å°„
        use_cache=True  # å¯ç”¨KVç¼“å­˜
    )  # ç§»é™¤.to(device)ï¼Œå› ä¸ºdevice_map="auto"ä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡åˆ†é…
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # è®¾ç½®tokenizeré…ç½®
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'  # å¯¹äºŽdecoder-onlyæ¨¡åž‹ï¼Œä½¿ç”¨å·¦ä¾§padding
    
    # 3. åŠ è½½å®žä½“å’Œå…³ç³»åˆ—è¡¨
    data_dir = "/mnt/d/datasets/GraphTruth/metaqa-1hop/metaqa-1hop"
    try:
        entity_list, relation_list = load_entities_and_relations(data_dir)
        print(f"åŠ è½½äº† {len(entity_list)} ä¸ªå®žä½“å’Œ {len(relation_list)} ä¸ªå…³ç³»")
    except Exception as e:
        print(f"åŠ è½½å®žä½“å’Œå…³ç³»æ—¶å‡ºé”™: {str(e)}")
        return
    
    # 4. åŠ è½½æ•°æ®
    samples = []
    try:
        # ðŸ”¥ ä½¿ç”¨æœ€æ–°çš„TUSä¸€è‡´æ€§ç­–ç•¥æ•°æ®æ–‡ä»¶ï¼ˆä¿®å¤äº†FGASæ‰©å±•é—®é¢˜ï¼‰
        data_file = "experiment_records/trimming_results_tus_consistent_20250623_203829.jsonl"
        with open(data_file, 'r') as f:
            # è·³è¿‡é…ç½®è¡Œ
            next(f)
            # è¯»å–æŒ‡å®šæ•°é‡çš„æ ·æœ¬
            for line in f:
                data = json.loads(line)
                if 'sample_id' in data:  # ç¡®ä¿æ˜¯æ ·æœ¬æ•°æ®è€Œä¸æ˜¯ç»Ÿè®¡ä¿¡æ¯
                    samples.append(data)
                    if num_samples and len(samples) >= num_samples:
                        break
    except Exception as e:
        print(f"åŠ è½½æ•°æ®é›†æ—¶å‡ºé”™: {str(e)}")
        return
    
    print(f"\nå¤„ç† {len(samples)} ä¸ªæ ·æœ¬...")
    print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
    if device.type == "cuda":
        print(f"å½“å‰CUDAè®¾å¤‡: {torch.cuda.current_device()}")
        print(f"å¯ç”¨æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    # 5. åˆ›å»ºç»“æžœç›®å½•
    os.makedirs('experiment_records', exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = f'experiment_records/inference_results_{timestamp}.jsonl'
    
    # 6. å¤„ç†æ ·æœ¬å¹¶ä¿å­˜ç»“æžœ
    results = []
    total_tus = 0
    total_fgas = 0
    valid_samples = 0
    failed_samples = []
    
    with open(result_file, 'w', encoding='utf-8') as f:
        # å†™å…¥é…ç½®ä¿¡æ¯
        config = {
        'timestamp': timestamp,
        'model': model_name,
            'num_samples': num_samples,
            'data_source': data_file,
        'device': str(device),
            'batch_size': batch_size
        }
        f.write(json.dumps({'config': config}, ensure_ascii=False) + '\n')
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†æ ·æœ¬
        for i in tqdm(range(0, len(samples), batch_size), desc="å¤„ç†æ‰¹æ¬¡"):
            batch_samples = samples[i:i+batch_size]
            batch_results = []
            
            # æ‰¹é‡å¤„ç†è¾“å…¥
            batch_inputs = []
            for sample in batch_samples:
                try:
                    # æž„å»ºç³»ç»Ÿæç¤º
                    system_prompt = """Based on the triples retrieved from a knowledge graph, please answer the question. Please return ONLY the answer entities as a list, each prefixed with "ans:". Do not include explanations or reasoning."""
                    
                    # æž„å»ºä¸‰å…ƒç»„æ–‡æœ¬
                    if not sample.get('trimmed_triples'):
                        failed_samples.append({
                            'sample_id': sample.get('sample_id', -1),
                            'question': sample.get('question', ''),
                            'answer': sample.get('golden_texts', [''])[0],
                            'reason': 'No trimmed triples found'
                        })
                        continue
                        
                    triples_text = "Knowledge:\n"
                    for h, r, t in sample['trimmed_triples']:
                        triples_text += f"{h} {r} {t}\n"
                    
                    # æž„å»ºç”¨æˆ·è¾“å…¥ï¼ˆåŒ…å«ä¸‰å…ƒç»„å’Œé—®é¢˜ï¼‰
                    user_content = f"{triples_text}\nQuestion: {sample['question']}\nAnswer:"
                    
                    # æž„å»ºå®Œæ•´è¾“å…¥
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_content}
                    ]
                    
                    # ä½¿ç”¨chat template
                    full_input = tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    
                    batch_inputs.append((sample, full_input))
                except Exception as e:
                    failed_samples.append({
                        'sample_id': sample.get('sample_id', -1),
                        'question': sample.get('question', ''),
                        'answer': sample.get('golden_texts', [''])[0],
                        'reason': str(e)
                    })
            
            if not batch_inputs:
                continue
                
            # æ‰¹é‡ç¼–ç 
            input_texts = [x[1] for x in batch_inputs]
            inputs = tokenizer(
                input_texts,
                padding=True,
                truncation=True,
                max_length=512,  # è®¾ç½®æœ€å¤§é•¿åº¦
                return_tensors="pt"
            )
            # å°†æ‰€æœ‰tensorç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                # é¦–å…ˆç”¨forwardèŽ·å–attention weights
                forward_outputs = model(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    output_attentions=True
                )
                
                # èŽ·å–æœ€åŽ4å±‚çš„attention weightså¹¶æ­£ç¡®å¤„ç†
                all_attentions = forward_outputs.attentions  # è¿™æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€å±‚çš„attention
                last_4_attentions = all_attentions[-4:]  # å–æœ€åŽ4å±‚
                attention_weights = torch.stack(last_4_attentions, dim=0)  # [4, batch, heads, seq_len, seq_len]
                
                # ç„¶åŽç”¨generateç”Ÿæˆç­”æ¡ˆ
                outputs = model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=50,  # å‡å°ç”Ÿæˆé•¿åº¦ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦å®žä½“å
                    do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç 
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=True  # å¯ç”¨KVç¼“å­˜
                )
            
            # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„è¾“å‡º
            for idx, (sample, full_input) in enumerate(batch_inputs):
                try:
                    # èŽ·å–å½“å‰æ ·æœ¬çš„attention weights
                    sample_attention = attention_weights[:, idx]  # [4, heads, seq_len, seq_len]
                    
                    # è§£ç è¾“å‡º
                    if isinstance(outputs, tuple):
                        output_ids = outputs[0][idx]
                    else:
                        output_ids = outputs[idx]
                    
                    # èŽ·å–ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®
                    answer_start_idx = len(inputs['input_ids'][idx])
                    answer_end_idx = len(output_ids)
                    
                    # è®¡ç®—TUSåˆ†æ•°
                    tus_score = calculate_tus(
                        attention_weights=sample_attention,
                        external_context=build_external_context(sample, tokenizer, entity_list, relation_list),
                        gold_triples=sample['gold_triples'],
                        answer_start_idx=answer_start_idx,
                        answer_end_idx=answer_end_idx,
                        input_ids=inputs['input_ids'][idx],
                        tokenizer=tokenizer,
                        debug=debug
                    )
                    
                    # è®¡ç®—nTUSåˆ†æ•°ï¼ˆå½’ä¸€åŒ–çš„TUSï¼‰
                    ntus_score = calculate_ntus(
                        attention_weights=sample_attention,
                        external_context=build_external_context(sample, tokenizer, entity_list, relation_list),
                        gold_triples=sample['gold_triples'],
                        trimmed_triples=sample['trimmed_triples'],
                        answer_start_idx=answer_start_idx,
                        answer_end_idx=answer_end_idx,
                        input_ids=inputs['input_ids'][idx],
                        tokenizer=tokenizer,
                        debug=debug
                    )
                    
                    # è®¡ç®—FGASåˆ†æ•° - ä½¿ç”¨æ­£ç¡®çš„ç­”æ¡ˆä½ç½®å’ŒGolden Expansion Set
                    fgas_answer_start = len(inputs['input_ids'][idx])  # ç”Ÿæˆæ–‡æœ¬çš„çœŸæ­£å¼€å§‹ä½ç½®
                    fgas_answer_end = len(output_ids) - 1  # ç”Ÿæˆæ–‡æœ¬çš„ç»“æŸä½ç½®
                    
                    # ðŸ”¥ FGASä½¿ç”¨æ‰©å±•é›†åˆï¼ˆè¯­ä¹‰ä¸°å¯Œåº¦ï¼‰
                    fgas_golden_triples = sample.get('golden_expansion_set', sample['gold_triples'])
                    
                    fgas_score = calculate_fgas(
                        model=model,
                        tokenizer=tokenizer,
                        input_ids=output_ids,
                        gold_triples=fgas_golden_triples,
                        answer_start_idx=fgas_answer_start,
                        answer_end_idx=fgas_answer_end,
                        debug=debug
                    )
                    
                    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                    generated_text = tokenizer.decode(output_ids[answer_start_idx:answer_end_idx], skip_special_tokens=True)
                    predicted_answers = [ans.strip() for ans in generated_text.lower().split('ans:') if ans.strip()]
                    if not predicted_answers:
                        predicted_answers = [generated_text.strip()]
                    
                    # è®¡ç®—æŒ‡æ ‡
                    answer_texts = sample['golden_texts']
                    metrics = calculate_metrics(predicted_answers, answer_texts)
                    
                    result = {
                        'question': sample['question'],
                        'answer': predicted_answers[0],
                        'golden_answers': answer_texts,
                        'metrics': metrics,
                        'tus_score': tus_score,
                        'ntus_score': ntus_score,
                        'fgas_score': fgas_score,
                        'model_input': full_input,
                        'model_output': generated_text,
                        'extracted_answers': predicted_answers,
                        'raw_model_output': generated_text
                    }
                    
                    valid_samples += 1
                    batch_results.append(result)
                    
                    # ä¿å­˜ç»“æžœ
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
                except Exception as e:
                    failed_samples.append({
                        'sample_id': sample.get('sample_id', -1),
                        'question': sample['question'],
                        'answer': sample['golden_texts'][0],
                        'reason': str(e)
                    })
            
            results.extend(batch_results)
            
            # æ¸…ç†æ˜¾å­˜
            if device.type == "cuda":
                torch.cuda.empty_cache()
    
    print("\n====================================================================================================")
    print("è¯„ä¼°ç»“æžœ:")
    print("----------------------------------------------------------------------------------------------------\n")
    
    # è¯¦ç»†ä¿¡æ¯
    print("è¯¦ç»†ä¿¡æ¯:")
    print("----------------------------------------------------------------------------------------------------")
    for i, result in enumerate(results, 1):
        print(f"\næ ·æœ¬ {i}:")
        print(f"é—®é¢˜: {result['question']}")
        print(f"æ¨¡åž‹è¾“å…¥:\n{result.get('model_input', '(æ— æ¨¡åž‹è¾“å…¥)')}")
        print(f"æ¨¡åž‹è¾“å‡º:\n{result.get('model_output', '(æ— åŽŸå§‹è¾“å‡º)')}")
        print(f"æå–ç­”æ¡ˆ: {result.get('answer', [])}")
        print(f"æ ‡å‡†ç­”æ¡ˆ: {', '.join(result.get('golden_answers', []))}")
        print(f"è¯„ä¼°ç»“æžœ: {'âœ“ æ­£ç¡®' if result['metrics']['hit@1'] else 'âœ— é”™è¯¯'} (TUS={result.get('tus_score', 0):.3f}, nTUS={result.get('ntus_score', 0):.3f}, FGAS={result.get('fgas_score', 0):.3f})")
        print("--------------------------------------------------")
    
    # æ±‡æ€»ç»Ÿè®¡
    total_samples = len(results)
    valid_samples = len([r for r in results if 'error' not in r])
    failed_samples = len(failed_samples)
    hit_at_1 = sum(1 for r in results if r['metrics']['hit@1']) / total_samples * 100 if total_samples > 0 else 0
    avg_tus = sum(r.get('tus_score', 0) for r in results) / total_samples if total_samples > 0 else 0
    avg_ntus = sum(r.get('ntus_score', 0) for r in results) / total_samples if total_samples > 0 else 0
    avg_fgas = sum(r.get('fgas_score', 0) for r in results) / total_samples if total_samples > 0 else 0
    
    print("\n==================================================")
    print("æ±‡æ€»ç»Ÿè®¡:")
    print("--------------------------------------------------")
    print("| æŒ‡æ ‡              | å€¼              |")
    print("|------------------|----------------|")
    print(f"| æ€»æ ·æœ¬æ•°          |{total_samples:>15} |")
    print(f"| æœ‰æ•ˆæ ·æœ¬æ•°        |{valid_samples:>15} |")
    print(f"| å¤±è´¥æ ·æœ¬æ•°        |{failed_samples:>15} |")
    print(f"| Hit@1           |{hit_at_1:>14.2f}% |")
    print(f"| å¹³å‡TUSåˆ†æ•°      |{avg_tus:>14.2f}  |")
    print(f"| å¹³å‡nTUSåˆ†æ•°     |{avg_ntus:>14.2f}  |")
    print(f"| å¹³å‡FGASåˆ†æ•°    |{avg_fgas:>14.2f}  |")
    print("--------------------------------------------------\n")
    
    # è¯„ä¼°ç»“æžœè¡¨æ ¼
    print("\n" + "="*140)
    print("è¯„ä¼°ç»“æžœè¡¨æ ¼:")
    print("-"*140)
    print("| {:^4} | {:^30} | {:^40} | {:^8} | {:^9} | {:^9} | {:^9} |".format(
        "åºå·", "é—®é¢˜", "æå–ç­”æ¡ˆ", "æ­£ç¡®æ€§", "TUSåˆ†æ•°", "nTUSåˆ†æ•°", "FGASåˆ†æ•°"
    ))
    print("|" + "-"*6 + "|" + "-"*32 + "|" + "-"*42 + "|" + "-"*10 + "|" + "-"*11 + "|" + "-"*11 + "|" + "-"*11 + "|")
    
    # éåŽ†æ¯ä¸ªæ ·æœ¬çš„ç»“æžœå¹¶æ‰“å°è¡¨æ ¼è¡Œ
    for i, result in enumerate(results, 1):
        question = result['question']
        if len(question) > 28:
            question = question[:25] + "..."
            
        # ä¿®æ”¹è¿™é‡Œï¼šç›´æŽ¥æ˜¾ç¤ºç­”æ¡ˆåˆ—è¡¨
        extracted_answers = str(result.get('answer', []))
        if len(extracted_answers) > 38:
            extracted_answers = extracted_answers[:35] + "..."
            
        is_correct = "âœ“" if result['metrics']['hit@1'] else "âœ—"
        tus_score = result.get('tus_score', 0)
        ntus_score = result.get('ntus_score', 0)
        fgas_score = result.get('fgas_score', 0)
        
        print("| {:4d} | {:<30} | {:<40} | {:^8} | {:9.3f} | {:9.3f} | {:9.3f} |".format(
            i, question, extracted_answers, is_correct, tus_score, ntus_score, fgas_score
        ))
    
    print("|" + "-"*6 + "|" + "-"*32 + "|" + "-"*42 + "|" + "-"*10 + "|" + "-"*11 + "|" + "-"*11 + "|" + "-"*11 + "|")
    print("\n")
    
    # ä¿å­˜ç»“æžœ
    print(f"ç»“æžœå·²ä¿å­˜è‡³: {result_file}")
    
    return {
        'total_samples': total_samples,
        'valid_samples': valid_samples,
        'failed_samples': failed_samples,
        'hit_at_1': hit_at_1,
        'avg_tus': avg_tus,
        'avg_ntus': avg_ntus,
        'avg_fgas': avg_fgas
    }

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='è®¡ç®—Triple Utilization Score (TUS)')
    parser.add_argument('-n', '--num_samples', type=int, default=None, help='è¦å¤„ç†çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('-d', '--debug', action='store_true', help='æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯')
    args = parser.parse_args()
    
    # åœ¨å‚æ•°è§£æžåŽè®¾ç½®æ—¥å¿—çº§åˆ«
    logger = setup_logging(args.debug)
    
    main(args.num_samples, args.debug)