#!/usr/bin/env python3
"""
Gold Expansion Set å·¥å…·æ¨¡å— v2.0 (æ”¹è¿›ç‰ˆ)

ä¸ºFGASè®¡ç®—æ„å»ºè¯­ä¹‰ä¸°å¯Œçš„ä¸‰å…ƒç»„æ‰©å±•é›†åˆï¼Œè§£å†³MetaQA-2hopä¸­GASSåˆ†æ•°åˆ†å¸ƒå¼‚å¸¸é—®é¢˜ã€‚

æ”¹è¿›ç‰ˆæœ¬ç‰¹æ€§:
- åŸºäºInfoScoreçš„ä¿¡æ¯é‡è¯„ä¼° (IDF + è¯­ä¹‰å…³ç³»æƒé‡)
- åŸºäºDiversityBonusçš„å¤šæ ·æ€§ä¿è¯ (ä½™å¼¦ç›¸ä¼¼åº¦)
- è´ªå¿ƒé€‰æ‹©ç®—æ³•é¿å…è¯­ä¹‰å†—ä½™
- é’ˆå¯¹MetaQA-2hopä¼˜åŒ–çš„å‚æ•°è®¾ç½® (max_size=8, max_per_entity=2)

æ ¸å¿ƒç®—æ³•:
1. å€™é€‰ä¸‰å…ƒç»„å‘ç° (ä¸æ ¸å¿ƒå®ä½“ç›¸å…³)
2. InfoScoreè®¡ç®— (å®ä½“/å…³ç³»IDF + è¯­ä¹‰å…³ç³»åŠ åˆ†)
3. åµŒå…¥å‘é‡ç”Ÿæˆ (SentenceTransformer)
4. è´ªå¿ƒé€‰æ‹© (InfoScore + DiversityBonus)
5. å®ä½“çº§åˆ«é™åˆ¶æ§åˆ¶

é¢„æœŸæ•ˆæœ:
- GESå¤§å°ä»10-12å‡å°‘åˆ°6-8
- å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦ä»>0.9é™ä½åˆ°~0.7
- GASSåˆ†å¸ƒä»é›†ä¸­åœ¨1.0è½¬ä¸ºæ­£å¸¸åˆ†å¸ƒ0.4-0.6

ä½œè€…: GraphDeEP Team
åˆ›å»ºæ—¥æœŸ: 2025-01-15
ç‰ˆæœ¬: 2.0
"""

from typing import List, Dict, Set, Tuple, Any
import json
import math
import numpy as np
from collections import Counter
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€åµŒå…¥æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_embedding_model = None

def get_embedding_model():
    """å»¶è¿ŸåŠ è½½åµŒå…¥æ¨¡å‹"""
    global _embedding_model
    if _embedding_model is None:
        logger.info("Loading SentenceTransformer model for diversity calculation...")
        _embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    return _embedding_model

# å®šä¹‰è¯­ä¹‰å¼ºçš„å…³ç³»ç±»å‹
SEMANTIC_RICH_RELATIONS = {
    'directed_by', 'acted_in', 'genre', 'release_year', 'language',
    'produced_by', 'written_by', 'cinematography', 'music_by',
    'starring', 'has_genre', 'release_date', 'runtime', 'budget',
    'award', 'nomination', 'country', 'company', 'sequel_to',
    'prequel_to', 'based_on', 'remake_of', 'character', 'role'
}

# æ”¹è¿›ç‰ˆæœ¬çš„é»˜è®¤å‚æ•°ï¼ˆé’ˆå¯¹MetaQA-2hopä¼˜åŒ–ï¼‰
DEFAULT_MAX_EXPANSION_SIZE = 20  # ä¸v1ä¿æŒä¸€è‡´ï¼Œæœ€å¤š20æ¡
DEFAULT_MAX_PER_ENTITY = 3       # ä¸v1ä¿æŒä¸€è‡´ï¼Œæ¯ä¸ªæ ¸å¿ƒå®ä½“æœ€å¤š3æ¡
DEFAULT_SELECTION_THRESHOLD = 0.5  # é€‰æ‹©é˜ˆå€¼

def calculate_idf_scores(all_triples: List[List[str]]) -> Dict[str, float]:
    """
    è®¡ç®—å®ä½“å’Œå…³ç³»çš„IDFåˆ†æ•°
    
    Args:
        all_triples: æ‰€æœ‰ä¸‰å…ƒç»„åˆ—è¡¨
        
    Returns:
        IDFåˆ†æ•°å­—å…¸ {entity/relation: idf_score}
    """
    # ç»Ÿè®¡æ¯ä¸ªå®ä½“/å…³ç³»çš„å‡ºç°æ¬¡æ•°
    entity_counts = Counter()
    relation_counts = Counter()
    
    total_triples = len(all_triples)
    
    for h, r, t in all_triples:
        entity_counts[h] += 1
        entity_counts[t] += 1
        relation_counts[r] += 1
    
    # è®¡ç®—IDFåˆ†æ•°
    idf_scores = {}
    
    # å®ä½“IDF
    for entity, count in entity_counts.items():
        idf_scores[entity] = math.log(total_triples / (count + 1))
    
    # å…³ç³»IDF
    for relation, count in relation_counts.items():
        idf_scores[relation] = math.log(total_triples / (count + 1))
    
    return idf_scores

def is_semantic_relation(relation: str) -> bool:
    """
    åˆ¤æ–­å…³ç³»æ˜¯å¦å…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰è¡¨è¾¾
    
    Args:
        relation: å…³ç³»åç§°
        
    Returns:
        æ˜¯å¦ä¸ºè¯­ä¹‰ä¸°å¯Œçš„å…³ç³»
    """
    relation_lower = relation.lower()
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯­ä¹‰ä¸°å¯Œçš„å…³é”®è¯
    for semantic_rel in SEMANTIC_RICH_RELATIONS:
        if semantic_rel in relation_lower:
            return True
    return False

def calculate_info_score(triple: List[str], idf_scores: Dict[str, float]) -> float:
    """
    è®¡ç®—ä¸‰å…ƒç»„çš„ä¿¡æ¯é‡åˆ†æ•°
    
    Args:
        triple: ä¸‰å…ƒç»„ [h, r, t]
        idf_scores: IDFåˆ†æ•°å­—å…¸
        
    Returns:
        ä¿¡æ¯é‡åˆ†æ•°
    """
    h, r, t = triple
    
    # ç»¼åˆå¤´å®ä½“ã€å…³ç³»ã€å°¾å®ä½“çš„IDFåˆ†æ•°
    h_idf = idf_scores.get(h, 0)
    r_idf = idf_scores.get(r, 0)
    t_idf = idf_scores.get(t, 0)
    
    # åŠ æƒå¹³å‡ï¼šå…³ç³»æƒé‡æ›´é«˜
    info_score = 0.3 * h_idf + 0.4 * r_idf + 0.3 * t_idf
    
    # è¯­ä¹‰ä¸°å¯Œå…³ç³»é¢å¤–åŠ åˆ†
    if is_semantic_relation(r):
        info_score += 0.5
    
    return info_score

def calculate_diversity_bonus(candidate_triple: List[str], 
                            selected_triples: List[List[str]], 
                            triple_embeddings: Dict[str, np.ndarray]) -> float:
    """
    è®¡ç®—å€™é€‰ä¸‰å…ƒç»„ç›¸å¯¹äºå·²é€‰æ‹©ä¸‰å…ƒç»„çš„å¤šæ ·æ€§å¥–åŠ±
    
    Args:
        candidate_triple: å€™é€‰ä¸‰å…ƒç»„
        selected_triples: å·²é€‰æ‹©çš„ä¸‰å…ƒç»„åˆ—è¡¨
        triple_embeddings: ä¸‰å…ƒç»„åµŒå…¥å‘é‡å­—å…¸
        
    Returns:
        å¤šæ ·æ€§å¥–åŠ±åˆ†æ•°
    """
    if not selected_triples:
        return 1.0  # ç¬¬ä¸€ä¸ªä¸‰å…ƒç»„æ²¡æœ‰å¤šæ ·æ€§æƒ©ç½š
    
    candidate_key = tuple(candidate_triple)
    if candidate_key not in triple_embeddings:
        return 0.5  # æ— æ³•è®¡ç®—ç›¸ä¼¼åº¦æ—¶ç»™ä¸­ç­‰åˆ†æ•°
    
    candidate_embedding = triple_embeddings[candidate_key]
    
    # è®¡ç®—ä¸æ‰€æœ‰å·²é€‰æ‹©ä¸‰å…ƒç»„çš„æœ€å¤§ç›¸ä¼¼åº¦
    max_similarity = 0.0
    
    for selected_triple in selected_triples:
        selected_key = tuple(selected_triple)
        if selected_key in triple_embeddings:
            selected_embedding = triple_embeddings[selected_key]
            similarity = cosine_similarity(
                candidate_embedding.reshape(1, -1),
                selected_embedding.reshape(1, -1)
            )[0][0]
            max_similarity = max(max_similarity, similarity)
    
    # å¤šæ ·æ€§å¥–åŠ± = 1 - max_similarity
    diversity_bonus = 1.0 - max_similarity
    
    return diversity_bonus

def create_triple_embeddings(triples: List[List[str]]) -> Dict[str, np.ndarray]:
    """
    ä¸ºä¸‰å…ƒç»„åˆ›å»ºåµŒå…¥å‘é‡
    
    Args:
        triples: ä¸‰å…ƒç»„åˆ—è¡¨
        
    Returns:
        ä¸‰å…ƒç»„åµŒå…¥å­—å…¸
    """
    model = get_embedding_model()
    
    # å°†ä¸‰å…ƒç»„è½¬æ¢ä¸ºæ–‡æœ¬
    triple_texts = []
    triple_keys = []
    
    for triple in triples:
        h, r, t = triple
        # æ„é€ è¯­ä¹‰åŒ–çš„ä¸‰å…ƒç»„æ–‡æœ¬
        text = f"{h} {r} {t}"
        triple_texts.append(text)
        triple_keys.append(tuple(triple))
    
    # ç”ŸæˆåµŒå…¥
    embeddings = model.encode(triple_texts)
    
    # åˆ›å»ºæ˜ å°„å­—å…¸
    triple_embeddings = {}
    for i, key in enumerate(triple_keys):
        triple_embeddings[key] = embeddings[i]
    
    return triple_embeddings

def extract_core_entities(gold_triples: List[List[str]]) -> Set[str]:
    """
    ä»gold triplesä¸­æå–æ ¸å¿ƒå®ä½“
    
    Args:
        gold_triples: åŸå§‹goldä¸‰å…ƒç»„åˆ—è¡¨ [[h, r, t], ...]
        
    Returns:
        æ ¸å¿ƒå®ä½“é›†åˆ
    """
    core_entities = set()
    for h, r, t in gold_triples:
        core_entities.add(h)
        core_entities.add(t)
    return core_entities

def find_candidate_triples(core_entities: Set[str], 
                         all_triples: List[List[str]], 
                         gold_triples: List[List[str]]) -> List[List[str]]:
    """
    æŸ¥æ‰¾å€™é€‰æ‰©å±•ä¸‰å…ƒç»„
    
    Args:
        core_entities: æ ¸å¿ƒå®ä½“é›†åˆ
        all_triples: å®Œæ•´çš„ä¸‰å…ƒç»„åˆ—è¡¨
        gold_triples: åŸå§‹goldä¸‰å…ƒç»„ï¼ˆéœ€è¦æ’é™¤ï¼‰
        
    Returns:
        å€™é€‰ä¸‰å…ƒç»„åˆ—è¡¨
    """
    # å°†gold triplesè½¬æ¢ä¸ºé›†åˆä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
    gold_set = set(tuple(triple) for triple in gold_triples)
    
    candidate_triples = []
    
    for triple in all_triples:
        h, r, t = triple
        triple_tuple = tuple(triple)
        
        # è·³è¿‡å·²åœ¨gold triplesä¸­çš„
        if triple_tuple in gold_set:
            continue
        
        # æ£€æŸ¥æ˜¯å¦ä¸æ ¸å¿ƒå®ä½“ç›¸å…³
        if h in core_entities or t in core_entities:
            candidate_triples.append(triple)
    
    return candidate_triples

def create_gold_expansion_set_v2(gold_triples: List[List[str]], 
                               trimmed_triples: List[List[str]],
                               max_expansion_size: int = DEFAULT_MAX_EXPANSION_SIZE,
                               max_per_entity: int = DEFAULT_MAX_PER_ENTITY,
                               selection_threshold: float = DEFAULT_SELECTION_THRESHOLD) -> List[List[str]]:
    """
    åˆ›å»ºæ”¹è¿›ç‰ˆçš„gold_expansion_set (v2.0)
    
    ä½¿ç”¨InfoScoreå’ŒDiversityBonusçš„è´ªå¿ƒé€‰æ‹©ç®—æ³•
    
    Args:
        gold_triples: åŸå§‹goldä¸‰å…ƒç»„
        trimmed_triples: å®Œæ•´çš„trimmedå­å›¾ä¸‰å…ƒç»„
        max_expansion_size: gold_expansion_setçš„æœ€å¤§å¤§å°
        max_per_entity: æ¯ä¸ªæ ¸å¿ƒå®ä½“æœ€å¤šæ‰©å±•çš„ä¸‰å…ƒç»„æ•°
        selection_threshold: é€‰æ‹©é˜ˆå€¼
        
    Returns:
        gold_expansion_setä¸‰å…ƒç»„åˆ—è¡¨
    """
    logger.info(f"Creating GES v2.0 with improved algorithm")
    logger.info(f"Parameters: max_size={max_expansion_size}, max_per_entity={max_per_entity}, threshold={selection_threshold}")

    # å¦‚æœåŸå§‹ gold triples å·²ç»è¾¾åˆ°æˆ–è¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œåˆ™ç›´æ¥è¿”å›ï¼Œè§†ä½œå®Œæ•´ GES
    if len(gold_triples) >= max_expansion_size:
        logger.info(
            "Gold triples length %d â‰¥ max_expansion_size %d. "
            "Skip expansion and use gold triples as GES.",
            len(gold_triples), max_expansion_size
        )
        return gold_triples
    
    # Step 1: æå–æ ¸å¿ƒå®ä½“
    core_entities = extract_core_entities(gold_triples)
    logger.info(f"Found {len(core_entities)} core entities: {core_entities}")
    
    # Step 2: æŸ¥æ‰¾å€™é€‰ä¸‰å…ƒç»„
    candidate_triples = find_candidate_triples(core_entities, trimmed_triples, gold_triples)
    logger.info(f"Found {len(candidate_triples)} candidate triples")
    
    if not candidate_triples:
        logger.warning("No candidate triples found, returning original gold triples")
        return gold_triples
    
    # Step 3: è®¡ç®—IDFåˆ†æ•°
    idf_scores = calculate_idf_scores(trimmed_triples)
    logger.info(f"Calculated IDF scores for {len(idf_scores)} entities/relations")
    
    # Step 4: ä¸ºæ‰€æœ‰ç›¸å…³ä¸‰å…ƒç»„åˆ›å»ºåµŒå…¥
    all_relevant_triples = gold_triples + candidate_triples
    triple_embeddings = create_triple_embeddings(all_relevant_triples)
    logger.info(f"Created embeddings for {len(triple_embeddings)} triples")
    
    # Step 5: è´ªå¿ƒé€‰æ‹©ç®—æ³•
    selected_expansion = []
    entity_expansion_count = {entity: 0 for entity in core_entities}
    
    # é¢„è®¡ç®—æ‰€æœ‰å€™é€‰ä¸‰å…ƒç»„çš„InfoScore
    candidate_info_scores = []
    for candidate in candidate_triples:
        info_score = calculate_info_score(candidate, idf_scores)
        candidate_info_scores.append((candidate, info_score))
    
    # æŒ‰InfoScoreæ’åº
    candidate_info_scores.sort(key=lambda x: x[1], reverse=True)
    logger.info(f"Sorted {len(candidate_info_scores)} candidates by InfoScore")
    
    # è´ªå¿ƒé€‰æ‹©
    for candidate, info_score in candidate_info_scores:
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€»ä½“é™åˆ¶
        if len(selected_expansion) >= max_expansion_size - len(gold_triples):
            logger.info(f"Reached max expansion size limit: {max_expansion_size}")
            break
        
        # æ£€æŸ¥å®ä½“é™åˆ¶
        h, r, t = candidate
        h_count = entity_expansion_count.get(h, 0)
        t_count = entity_expansion_count.get(t, 0)
        
        if h in core_entities and h_count >= max_per_entity:
            continue
        if t in core_entities and t_count >= max_per_entity:
            continue
        
        # è®¡ç®—å¤šæ ·æ€§å¥–åŠ±
        diversity_bonus = calculate_diversity_bonus(
            candidate, 
            gold_triples + selected_expansion, 
            triple_embeddings
        )
        
        # ç»¼åˆå¾—åˆ†ï¼šInfoScore + DiversityBonus
        total_score = info_score + diversity_bonus
        
        # é€‰æ‹©é˜ˆå€¼åˆ¤æ–­
        if total_score > selection_threshold:
            selected_expansion.append(candidate)
            
            # æ›´æ–°å®ä½“è®¡æ•°
            if h in core_entities:
                entity_expansion_count[h] += 1
            if t in core_entities:
                entity_expansion_count[t] += 1
            
            logger.debug(f"Selected triple: {candidate}, InfoScore: {info_score:.3f}, DiversityBonus: {diversity_bonus:.3f}, Total: {total_score:.3f}")
    
    # Step 6: æ„å»ºæœ€ç»ˆçš„gold_expansion_set
    gold_expansion_set = gold_triples + selected_expansion
    
    logger.info(f"Final GES: {len(gold_triples)} gold + {len(selected_expansion)} expansion = {len(gold_expansion_set)} total")
    logger.info(f"Entity expansion counts: {entity_expansion_count}")
    
    return gold_expansion_set

def analyze_expansion_quality_v2(gold_triples: List[List[str]], 
                               gold_expansion_set: List[List[str]],
                               triple_embeddings: Dict[str, np.ndarray] = None) -> Dict[str, Any]:
    """
    åˆ†ægold_expansion_setçš„è´¨é‡æŒ‡æ ‡ (v2.0å¢å¼ºç‰ˆ)
    
    Args:
        gold_triples: åŸå§‹goldä¸‰å…ƒç»„
        gold_expansion_set: gold_expansion_setä¸‰å…ƒç»„åˆ—è¡¨
        triple_embeddings: ä¸‰å…ƒç»„åµŒå…¥å­—å…¸ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        è´¨é‡åˆ†æç»“æœ
    """
    # åŸºç¡€ç»Ÿè®¡
    original_size = len(gold_triples)
    ges_size = len(gold_expansion_set)
    expansion_size = ges_size - original_size
    
    # å…³ç³»ç»Ÿè®¡
    gold_relations = [triple[1] for triple in gold_triples]
    ges_relations = [triple[1] for triple in gold_expansion_set]
    
    # è¯­ä¹‰ä¸°å¯Œå…³ç³»æ¯”ä¾‹
    semantic_in_gold = sum(1 for r in gold_relations if is_semantic_relation(r))
    semantic_in_ges = sum(1 for r in ges_relations if is_semantic_relation(r))
    
    gold_semantic_ratio = semantic_in_gold / original_size if original_size > 0 else 0
    ges_semantic_ratio = semantic_in_ges / ges_size if ges_size > 0 else 0
    
    # å¤šæ ·æ€§åˆ†æï¼ˆå¦‚æœæœ‰åµŒå…¥ï¼‰
    avg_similarity = None
    if triple_embeddings and len(gold_expansion_set) > 1:
        similarities = []
        for i, triple1 in enumerate(gold_expansion_set):
            for j, triple2 in enumerate(gold_expansion_set):
                if i < j:
                    key1, key2 = tuple(triple1), tuple(triple2)
                    if key1 in triple_embeddings and key2 in triple_embeddings:
                        sim = cosine_similarity(
                            triple_embeddings[key1].reshape(1, -1),
                            triple_embeddings[key2].reshape(1, -1)
                        )[0][0]
                        similarities.append(sim)
        
        if similarities:
            avg_similarity = np.mean(similarities)
    
    # è®¡ç®—å¤šæ ·æ€§åˆ†æ•°ï¼ˆ1 - å¹³å‡ç›¸ä¼¼åº¦ï¼‰
    diversity_score = 1.0 - avg_similarity if avg_similarity is not None else None
    
    # å°† numpy ç±»å‹è½¬æ¢ä¸º Python åŸç”Ÿç±»å‹ï¼Œé¿å… JSON åºåˆ—åŒ–æŠ¥é”™
    def to_py(value):
        if isinstance(value, (np.floating, np.integer)):
            return value.item()
        return value
    
    return {
        'original_size': original_size,
        'ges_size': ges_size,
        'expansion_size': expansion_size,
        'expansion_ratio': float(ges_size / original_size) if original_size > 0 else 0.0,
        'gold_semantic_ratio': float(gold_semantic_ratio),
        'ges_semantic_ratio': float(ges_semantic_ratio),
        'semantic_improvement': float(ges_semantic_ratio - gold_semantic_ratio),
        'avg_cosine_similarity': to_py(avg_similarity),
        'diversity_score': to_py(diversity_score)
    }

def process_sample_with_ges_v2(sample: Dict[str, Any], 
                             max_expansion_size: int = DEFAULT_MAX_EXPANSION_SIZE,
                             max_per_entity: int = DEFAULT_MAX_PER_ENTITY,
                             selection_threshold: float = DEFAULT_SELECTION_THRESHOLD) -> Dict[str, Any]:
    """
    ä¸ºå•ä¸ªæ ·æœ¬åˆ›å»ºGES v2.0å¹¶æ›´æ–°æ ·æœ¬æ•°æ®
    
    Args:
        sample: trimmingç»“æœä¸­çš„æ ·æœ¬
        max_expansion_size: GESæœ€å¤§å¤§å°
        max_per_entity: æ¯ä¸ªå®ä½“æœ€å¤§æ‰©å±•æ•°
        selection_threshold: é€‰æ‹©é˜ˆå€¼
        
    Returns:
        æ›´æ–°åçš„æ ·æœ¬ï¼ŒåŒ…å«GES v2.0å­—æ®µ
    """
    gold_triples = sample.get('gold_triples', [])
    trimmed_triples = sample.get('trimmed_triples', [])
    
    # åˆ›å»ºGES v2.0
    ges_v2 = create_gold_expansion_set_v2(
        gold_triples, trimmed_triples, 
        max_expansion_size, max_per_entity, selection_threshold
    )
    
    # åˆ›å»ºåµŒå…¥ç”¨äºè´¨é‡åˆ†æ
    triple_embeddings = create_triple_embeddings(ges_v2)
    
    # åˆ†æè´¨é‡
    quality_metrics = analyze_expansion_quality_v2(gold_triples, ges_v2, triple_embeddings)
    
    # æ›´æ–°æ ·æœ¬
    updated_sample = sample.copy()
    # ä¸v1ä¿æŒä¸€è‡´çš„å­—æ®µå‘½å
    updated_sample['gold_expansion_set'] = ges_v2
    updated_sample['ges_quality_metrics'] = quality_metrics
    
    return updated_sample

def batch_create_ges_v2(input_file: str, 
                      output_file: str,
                      max_expansion_size: int = DEFAULT_MAX_EXPANSION_SIZE,
                      max_per_entity: int = DEFAULT_MAX_PER_ENTITY,
                      selection_threshold: float = DEFAULT_SELECTION_THRESHOLD) -> str:
    """
    æ‰¹é‡ä¸ºtrimmingç»“æœåˆ›å»ºGES v2.0
    
    Args:
        input_file: è¾“å…¥çš„trimmingç»“æœæ–‡ä»¶
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        max_expansion_size: GESæœ€å¤§å¤§å°
        max_per_entity: æ¯ä¸ªå®ä½“æœ€å¤§æ‰©å±•æ•°
        selection_threshold: é€‰æ‹©é˜ˆå€¼
        
    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print(f"æ­£åœ¨ä¸º {input_file} åˆ›å»ºGold Expansion Set v2.0...")
    
    processed_count = 0
    total_expansion_ratio = 0
    total_semantic_improvement = 0
    total_diversity_score = 0
    total_avg_similarity = 0
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line_num, line in enumerate(f_in, 1):
            if not line.strip():
                continue
                
            try:
                data = json.loads(line)
                
                # è·³è¿‡é…ç½®å’Œç»Ÿè®¡è¡Œ
                if 'config' in data or 'final_stats' in data or 'batch_stats' in data:
                    f_out.write(line)
                    continue
                
                # å¤„ç†æ ·æœ¬
                if 'sample_id' in data:
                    updated_sample = process_sample_with_ges_v2(
                        data, max_expansion_size, max_per_entity, selection_threshold
                    )
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    quality_metrics = updated_sample['ges_quality_metrics']
                    total_expansion_ratio += quality_metrics['expansion_ratio']
                    total_semantic_improvement += quality_metrics['semantic_improvement']
                    
                    if quality_metrics['diversity_score'] is not None:
                        total_diversity_score += quality_metrics['diversity_score']
                    if quality_metrics['avg_cosine_similarity'] is not None:
                        total_avg_similarity += quality_metrics['avg_cosine_similarity']
                    
                    processed_count += 1
                    
                    f_out.write(json.dumps(updated_sample, ensure_ascii=False) + '\n')
                    
                    if processed_count % 100 == 0:
                        print(f"å·²å¤„ç† {processed_count} ä¸ªæ ·æœ¬...")
                        
                else:
                    f_out.write(line)
                    
            except json.JSONDecodeError as e:
                print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                f_out.write(line)
                continue
            except Exception as e:
                print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡Œå¤„ç†å¤±è´¥: {e}")
                f_out.write(line)
                continue
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if processed_count > 0:
        avg_expansion_ratio = total_expansion_ratio / processed_count
        avg_semantic_improvement = total_semantic_improvement / processed_count
        avg_diversity_score = total_diversity_score / processed_count
        avg_avg_similarity = total_avg_similarity / processed_count
        
        print(f"\n=== GES v2.0 åˆ›å»ºå®Œæˆ ===")
        print(f"å¤„ç†æ ·æœ¬æ•°: {processed_count}")
        print(f"å¹³å‡æ‰©å±•æ¯”ä¾‹: {avg_expansion_ratio:.2f}x")
        print(f"å¹³å‡è¯­ä¹‰å…³ç³»æ”¹å–„: {avg_semantic_improvement:.3f}")
        print(f"å¹³å‡å¤šæ ·æ€§åˆ†æ•°: {avg_diversity_score:.3f}")
        print(f"å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {avg_avg_similarity:.3f}")
        print(f"ç»“æœä¿å­˜è‡³: {output_file}")
    
    return output_file

def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    import argparse
    import os
    from datetime import datetime
    
    parser = argparse.ArgumentParser(
        description='ä¸ºJSONLæ–‡ä»¶æ·»åŠ Gold Expansion Set v2.0 (æ”¹è¿›ç‰ˆ)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python gold_expansion_utils_v2.py input.jsonl
  python gold_expansion_utils_v2.py input.jsonl --output custom_output.jsonl
  python gold_expansion_utils_v2.py input.jsonl --max-size 6 --max-per-entity 1 --threshold 0.6
        """
    )
    
    parser.add_argument('input_file', nargs='?',
                       help='è¾“å…¥çš„trimmingç»“æœJSONLæ–‡ä»¶è·¯å¾„ (å¯çœç•¥ï¼Œè‹¥çœç•¥åˆ™æ ¹æ® --dataset è‡ªåŠ¨æ¨æ–­)')
    parser.add_argument('--dataset', default='metaqa-2hop',
                       help='æ•°æ®é›†åç§° (é»˜è®¤: metaqa-2hop)ï¼Œå½“æœªæ˜¾å¼æä¾› input_file æ—¶ä½¿ç”¨è¯¥æ•°æ®é›†çš„é»˜è®¤ trimming ç»“æœè·¯å¾„')
    parser.add_argument('--output', '-o', 
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: åŸæ–‡ä»¶å_with_ges_v2.jsonl)')
    parser.add_argument('--max-size', type=int, default=DEFAULT_MAX_EXPANSION_SIZE,
                       help=f'GESæœ€å¤§å¤§å° (é»˜è®¤: {DEFAULT_MAX_EXPANSION_SIZE})')
    parser.add_argument('--max-per-entity', type=int, default=DEFAULT_MAX_PER_ENTITY,
                       help=f'æ¯ä¸ªå®ä½“æœ€å¤§æ‰©å±•æ•° (é»˜è®¤: {DEFAULT_MAX_PER_ENTITY})')
    parser.add_argument('--threshold', type=float, default=DEFAULT_SELECTION_THRESHOLD,
                       help=f'é€‰æ‹©é˜ˆå€¼ (é»˜è®¤: {DEFAULT_SELECTION_THRESHOLD})')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # å¦‚æœæœªæä¾› input_fileï¼Œåˆ™æ ¹æ®æ•°æ®é›†åç§°è‡ªåŠ¨ç”Ÿæˆ
    if args.input_file:
        input_file = args.input_file
    else:
        input_file = f"experiment_records/trimming_results/{args.dataset}/dev_simple_trimming_results.jsonl"

    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return 1
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if args.output:
        output_file = args.output
    else:
        input_base = os.path.splitext(input_file)[0]
        output_file = f"{input_base}_with_ges.jsonl"
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(output_file) and not args.force:
        print(f"âŒ é”™è¯¯: è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨: {output_file}")
        print("ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶è¦†ç›–ï¼Œæˆ–ä½¿ç”¨ --output æŒ‡å®šä¸åŒçš„è¾“å‡ºæ–‡ä»¶å")
        return 1
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("="*70)
    print("ï¿½ï¿½ Gold Expansion Set v2.0 ç”Ÿæˆå·¥å…· (æ”¹è¿›ç‰ˆ)")
    print("="*70)
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"âš™ï¸  æœ€å¤§GESå¤§å°: {args.max_size}")
    print(f"âš™ï¸  æ¯å®ä½“æœ€å¤§æ‰©å±•æ•°: {args.max_per_entity}")
    print(f"âš™ï¸  é€‰æ‹©é˜ˆå€¼: {args.threshold}")
    print(f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-"*70)
    
    try:
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        start_time = datetime.now()
        result_file = batch_create_ges_v2(
            input_file, 
            output_file,
            args.max_size,
            args.max_per_entity,
            args.threshold
        )
        end_time = datetime.now()
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (end_time - start_time).total_seconds()
        
        print("-"*70)
        print(f"âœ… å¤„ç†å®Œæˆ!")
        print(f"ğŸ• æ€»å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
        print("="*70)
        
        # è·å–æ•°æ®é›†åç§°å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
        dataset_name = args.dataset
        print(f"ğŸ“Š æ•°æ®é›†: {dataset_name} | ç”Ÿæˆæ–‡ä»¶: {output_file}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main()) 