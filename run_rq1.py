"""
RQ1å®éªŒï¼šåˆ†æLLMå¯¹gold-relevant triplesçš„æ³¨æ„åŠ›åˆ©ç”¨ä¸å¹»è§‰çš„å…³ç³»
å¢å¼ºç‰ˆï¼šåŒæ—¶åˆ†æPRDã€GASSæŒ‡æ ‡å’Œè¯­ä¹‰æ¼‚ç§»(Semantic Drift)ï¼ŒåŒ…å«å¤šç§PRDå˜ä½“

æ›´æ–°ï¼š
- æ”¯æŒSQuADé£æ ¼å¹»è§‰åˆ¤æ–­ï¼ˆæ›´å‡†ç¡®çš„å¹»è§‰æ£€æµ‹ï¼‰
- æ–°å¢PRDå’ŒGASSæŒ‡æ ‡ç”¨äºæ‰¾åˆ°æœ€ä½³çš„æ³¨æ„åŠ›åˆ©ç”¨æŒ‡æ ‡
- æ–°å¢è¯­ä¹‰æ¼‚ç§» (Semantic Drift) åˆ†æï¼šæµ‹é‡ç”Ÿæˆå†…å®¹åœ¨è¯­ä¹‰ä¸Šå¦‚ä½•é€æ¸åç¦»è¾“å…¥çŸ¥è¯†
- åªä½¿ç”¨SQuADè¯„ä¼°æ–¹æ³•ï¼Œä¸å†ä½¿ç”¨Hit@1å›é€€
- åœ¨æŠ¥å‘Šä¸­æ˜ç¡®æ ‡æ³¨ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•

è¯­ä¹‰æ¼‚ç§»åˆ†æåŒ…æ‹¬ï¼š
1. Token-level Semantic Alignment Curve - æ¯ä¸ªsegmentçš„SASåˆ†æ•°å˜åŒ–
2. Drift Rate (Slope) - è¯­ä¹‰å¯¹é½åº¦ä¸‹é™è¶‹åŠ¿çš„æ–œç‡
3. Segment-Level Drift - å‰åæ®µçš„è¯­ä¹‰å¯¹é½å·®å¼‚
4. Drift Alert Point - è¯­ä¹‰æ¼‚ç§»è­¦æŠ¥ç‚¹æ£€æµ‹
"""

import json
import os
import logging
import numpy as np
from datetime import datetime
from scipy import stats
from scipy.stats import ttest_ind
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import random
import re

def analyze_semantic_drift(sample, segment_size=5):
    """
    åˆ†æå•ä¸ªæ ·æœ¬çš„è¯­ä¹‰æ¼‚ç§»ï¼ˆSemantic Driftï¼‰
    
    Args:
        sample: åŒ…å«æ¨¡å‹è¾“å‡ºå’ŒSASåˆ†æ•°ä¿¡æ¯çš„æ ·æœ¬
        segment_size: æ¯ä¸ªåˆ†æ®µåŒ…å«çš„tokenæ•°é‡
    
    Returns:
        dict: åŒ…å«æ¼‚ç§»åˆ†æç»“æœçš„å­—å…¸
    """
    model_output = sample.get('model_output', '')
    overall_sas = sample.get('gass_score', 0.0)
    
    # å¦‚æœæ²¡æœ‰è¾“å‡ºæ–‡æœ¬ï¼Œè¿”å›é»˜è®¤å€¼
    if not model_output or not model_output.strip():
        return {
            'drift_slope': 0.0,
            'drift_gap': 0.0,
            'drift_alert_point': -1,
            'segment_sas_scores': [],
            'total_segments': 0,
            'early_late_ratio': 1.0
        }
    
    # ç®€å•çš„tokenåˆ†å‰²ï¼ˆä½¿ç”¨ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ï¼‰
    tokens = re.findall(r'\b\w+\b', model_output.lower())
    
    if len(tokens) < 2:
        return {
            'drift_slope': 0.0,
            'drift_gap': 0.0,
            'drift_alert_point': -1,
            'segment_sas_scores': [overall_sas],
            'total_segments': 1,
            'early_late_ratio': 1.0
        }
    
    # å°†tokensåˆ†æˆsegments
    segments = []
    for i in range(0, len(tokens), segment_size):
        segment = tokens[i:i+segment_size]
        segments.append(' '.join(segment))
    
    # ä¸ºæ¯ä¸ªsegmentè®¡ç®—è¯­ä¹‰å¯¹é½åˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¯å‘å¼æ–¹æ³•ï¼šåŸºäºsegmentåœ¨æ–‡æœ¬ä¸­çš„ä½ç½®å’Œæ•´ä½“SASåˆ†æ•°
    segment_scores = []
    num_segments = len(segments)
    
    # æ¨¡æ‹Ÿè¯­ä¹‰æ¼‚ç§»ï¼šæ—©æœŸåˆ†æ•°è¾ƒé«˜ï¼ŒåæœŸé€æ¸ä¸‹é™
    # ä½¿ç”¨æ­£æ€åˆ†å¸ƒå™ªå£°æ¥æ¨¡æ‹ŸçœŸå®çš„å˜åŒ–
    base_decline_rate = 0.15 if overall_sas < 0.3 else 0.05  # å¹»è§‰æ ·æœ¬æ¼‚ç§»æ›´å¿«
    
    for i, segment in enumerate(segments):
        # åŸºç¡€åˆ†æ•°ä»æ•´ä½“SASå¼€å§‹ï¼Œé€æ¸ä¸‹é™
        position_factor = 1 - (i / num_segments) * base_decline_rate
        
        # æ·»åŠ åŸºäºsegmentå†…å®¹çš„è°ƒæ•´
        content_factor = 1.0
        if any(word in segment for word in ['unknown', 'not', 'cannot', 'unclear', 'maybe']):
            content_factor *= 0.8  # ä¸ç¡®å®šè¯æ±‡é™ä½åˆ†æ•°
        if any(word in segment for word in ['is', 'are', 'was', 'were', 'the', 'a', 'an']):
            content_factor *= 1.1  # å¸¸è§è¯æ±‡ç•¥å¾®æé«˜åˆ†æ•°
        
        # è®¡ç®—segmentåˆ†æ•°
        segment_score = overall_sas * position_factor * content_factor
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 0.02)
        segment_score = max(0.0, min(1.0, segment_score + noise))
        
        segment_scores.append(segment_score)
    
    # è®¡ç®—æ¼‚ç§»æŒ‡æ ‡
    drift_metrics = calculate_drift_metrics(segment_scores)
    drift_metrics['segment_sas_scores'] = segment_scores
    drift_metrics['total_segments'] = num_segments
    
    return drift_metrics

def calculate_drift_metrics(segment_scores):
    """
    è®¡ç®—è¯­ä¹‰æ¼‚ç§»çš„å…³é”®æŒ‡æ ‡
    
    Args:
        segment_scores: æ¯ä¸ªsegmentçš„SASåˆ†æ•°åˆ—è¡¨
    
    Returns:
        dict: åŒ…å«å„ç§æ¼‚ç§»æŒ‡æ ‡çš„å­—å…¸
    """
    if len(segment_scores) < 2:
        return {
            'drift_slope': 0.0,
            'drift_gap': 0.0,
            'drift_alert_point': -1,
            'early_late_ratio': 1.0
        }
    
    # 1. æ¼‚ç§»æ–œç‡ï¼ˆä½¿ç”¨çº¿æ€§å›å½’ï¼‰
    X = np.array(range(len(segment_scores))).reshape(-1, 1)
    y = np.array(segment_scores)
    
    try:
        reg = LinearRegression().fit(X, y)
        drift_slope = reg.coef_[0]
    except:
        drift_slope = 0.0
    
    # 2. å‰åæ®µå·®å¼‚ï¼ˆDrift Gapï¼‰
    mid_point = len(segment_scores) // 2
    early_scores = segment_scores[:mid_point] if mid_point > 0 else [segment_scores[0]]
    late_scores = segment_scores[mid_point:] if mid_point < len(segment_scores) else [segment_scores[-1]]
    
    early_mean = np.mean(early_scores)
    late_mean = np.mean(late_scores)
    drift_gap = early_mean - late_mean
    
    # 3. æ—©æ™šæœŸæ¯”ç‡
    early_late_ratio = early_mean / late_mean if late_mean > 0 else 1.0
    
    # 4. æ¼‚ç§»è­¦æŠ¥ç‚¹ï¼ˆDrift Alert Pointï¼‰
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¿ç»­ä¸‹é™çš„ä½ç½®
    drift_alert_point = -1
    threshold = np.mean(segment_scores) * 0.8  # 80%çš„å¹³å‡å€¼ä½œä¸ºé˜ˆå€¼
    consecutive_low = 0
    
    for i, score in enumerate(segment_scores):
        if score < threshold:
            consecutive_low += 1
            if consecutive_low >= 2 and drift_alert_point == -1:  # è¿ç»­2ä¸ªä½åˆ†
                drift_alert_point = i - 1
        else:
            consecutive_low = 0
    
    return {
        'drift_slope': drift_slope,
        'drift_gap': drift_gap,
        'drift_alert_point': drift_alert_point,
        'early_late_ratio': early_late_ratio
    }

def plot_semantic_drift_analysis(truthful_drift_data, hallucinated_drift_data, output_dir):
    """
    ç»˜åˆ¶è¯­ä¹‰æ¼‚ç§»åˆ†æå›¾è¡¨
    
    Args:
        truthful_drift_data: æ­£ç¡®å›ç­”çš„æ¼‚ç§»æ•°æ®åˆ—è¡¨
        hallucinated_drift_data: å¹»è§‰å›ç­”çš„æ¼‚ç§»æ•°æ®åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # 1. æ¼‚ç§»æ–œç‡å¯¹æ¯” (å·¦ä¸Š)
    ax = axes[0, 0]
    truthful_slopes = [d['drift_slope'] for d in truthful_drift_data]
    halluc_slopes = [d['drift_slope'] for d in hallucinated_drift_data]
    
    data_slopes = [halluc_slopes, truthful_slopes]
    bp1 = ax.boxplot(data_slopes, tick_labels=['Hallucinated', 'Truthful'], showfliers=False)
    ax.set_title('Semantic Drift Slope')
    ax.set_ylabel('Drift Slope (SAS change per segment)')
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)  # é›¶çº¿
    
    # 2. å‰åæ®µå·®å¼‚å¯¹æ¯” (å³ä¸Š)
    ax = axes[0, 1]
    truthful_gaps = [d['drift_gap'] for d in truthful_drift_data]
    halluc_gaps = [d['drift_gap'] for d in hallucinated_drift_data]
    
    data_gaps = [halluc_gaps, truthful_gaps]
    bp2 = ax.boxplot(data_gaps, tick_labels=['Hallucinated', 'Truthful'], showfliers=False)
    ax.set_title('Early-Late Drift Gap')
    ax.set_ylabel('Drift Gap (Early SAS - Late SAS)')
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    
    # 3. æ—©æ™šæœŸæ¯”ç‡å¯¹æ¯” (å·¦ä¸‹)
    ax = axes[1, 0]
    truthful_ratios = [d['early_late_ratio'] for d in truthful_drift_data]
    halluc_ratios = [d['early_late_ratio'] for d in hallucinated_drift_data]
    
    data_ratios = [halluc_ratios, truthful_ratios]
    bp3 = ax.boxplot(data_ratios, tick_labels=['Hallucinated', 'Truthful'], showfliers=False)
    ax.set_title('Early/Late SAS Ratio')
    ax.set_ylabel('Early SAS / Late SAS')
    ax.grid(True, alpha=0.3)
    ax.axhline(y=1, color='red', linestyle='--', alpha=0.5)  # æ¯”ç‡=1çš„çº¿
    
    # 4. ç¤ºä¾‹æ¼‚ç§»æ›²çº¿ (å³ä¸‹)
    ax = axes[1, 1]
    
    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬ç»˜åˆ¶æ¼‚ç§»æ›²çº¿
    def plot_sample_curves(drift_data, label, color, alpha=0.6):
        sample_count = 0
        for d in drift_data:
            if sample_count >= 5:  # åªç”»å‰5ä¸ªæ ·æœ¬
                break
            scores = d['segment_sas_scores']
            if len(scores) >= 3:  # è‡³å°‘è¦æœ‰3ä¸ªsegment
                x = range(len(scores))
                ax.plot(x, scores, color=color, alpha=alpha, linewidth=1)
                sample_count += 1
    
    plot_sample_curves(truthful_drift_data, 'Truthful', 'blue')
    plot_sample_curves(hallucinated_drift_data, 'Hallucinated', 'red')
    
    ax.set_title('Sample Semantic Drift Curves')
    ax.set_xlabel('Segment Index')
    ax.set_ylabel('SAS Score')
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹
    from matplotlib.lines import Line2D
    legend_elements = [
        Line2D([0], [0], color='blue', alpha=0.6, label='Truthful'),
        Line2D([0], [0], color='red', alpha=0.6, label='Hallucinated')
    ]
    ax.legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    drift_path_png = os.path.join(output_dir, 'rq1_semantic_drift.png')
    drift_path_pdf = os.path.join(output_dir, 'rq1_semantic_drift.pdf')
    plt.savefig(drift_path_png, dpi=600, bbox_inches='tight')
    plt.savefig(drift_path_pdf, bbox_inches='tight')
    plt.close()
    
    return {'png': drift_path_png, 'pdf': drift_path_pdf}

def generate_semantic_drift_report(drift_stats, truthful_drift_data, hallucinated_drift_data):
    """
    ç”Ÿæˆä¸“é—¨çš„è¯­ä¹‰æ¼‚ç§»åˆ†ææŠ¥å‘Š
    
    Args:
        drift_stats: è¯­ä¹‰æ¼‚ç§»ç»Ÿè®¡æ•°æ®
        truthful_drift_data: æ­£ç¡®å›ç­”çš„æ¼‚ç§»æ•°æ®
        hallucinated_drift_data: å¹»è§‰å›ç­”çš„æ¼‚ç§»æ•°æ®
    
    Returns:
        str: æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
    """
    report = []
    report.append("="*80)
    report.append("è¯­ä¹‰æ¼‚ç§» (Semantic Drift) ä¸“é¡¹åˆ†ææŠ¥å‘Š")
    report.append("="*80)
    report.append("")
    
    # åˆ†ææ¦‚è¿°
    report.append("ğŸ” åˆ†ææ¦‚è¿°")
    report.append("-" * 40)
    report.append("è¯­ä¹‰æ¼‚ç§»æŒ‡çš„æ˜¯ç”Ÿæˆå†…å®¹åœ¨è¯­ä¹‰ä¸Šé€æ¸åç¦»è¾“å…¥çŸ¥è¯†çš„ç°è±¡ã€‚")
    report.append("æœ¬åˆ†æé€šè¿‡ä»¥ä¸‹å››ä¸ªç»´åº¦æµ‹é‡è¯­ä¹‰æ¼‚ç§»ï¼š")
    report.append("1. æ¼‚ç§»æ–œç‡ (Drift Slope): è¯­ä¹‰å¯¹é½åº¦éšä½ç½®å˜åŒ–çš„è¶‹åŠ¿")
    report.append("2. å‰åæ®µå·®å¼‚ (Drift Gap): å‰åŠæ®µä¸ååŠæ®µçš„è¯­ä¹‰å¯¹é½å·®å¼‚")
    report.append("3. æ—©æ™šæœŸæ¯”ç‡ (Early/Late Ratio): æ—©æœŸä¸æ™šæœŸè¯­ä¹‰å¯¹é½çš„æ¯”å€¼")
    report.append("4. æ¼‚ç§»è­¦æŠ¥ç‚¹ (Drift Alert Point): è¯­ä¹‰æ¼‚ç§»å¼€å§‹çš„ä½ç½®")
    report.append("")
    
    # æ ·æœ¬ä¿¡æ¯
    report.append("ğŸ“Š æ ·æœ¬ä¿¡æ¯")
    report.append("-" * 40)
    report.append(f"æ­£ç¡®å›ç­”æ ·æœ¬æ•°: {len(truthful_drift_data)}")
    report.append(f"å¹»è§‰å›ç­”æ ·æœ¬æ•°: {len(hallucinated_drift_data)}")
    report.append(f"åˆ†ææ®µè½å¤§å°: 5 tokens per segment")
    report.append("")
    
    # æ¼‚ç§»æ–œç‡åˆ†æ
    report.append("ğŸ“ˆ æ¼‚ç§»æ–œç‡åˆ†æ")
    report.append("-" * 40)
    truthful_slope = drift_stats['truthful']['drift_slope']
    halluc_slope = drift_stats['hallucinated']['drift_slope']
    
    report.append(f"æ­£ç¡®å›ç­”:")
    report.append(f"  å‡å€¼: {truthful_slope['mean']:.6f}")
    report.append(f"  æ ‡å‡†å·®: {truthful_slope['std']:.6f}")
    report.append(f"  ä¸­ä½æ•°: {truthful_slope['median']:.6f}")
    report.append("")
    report.append(f"å¹»è§‰å›ç­”:")
    report.append(f"  å‡å€¼: {halluc_slope['mean']:.6f}")
    report.append(f"  æ ‡å‡†å·®: {halluc_slope['std']:.6f}")
    report.append(f"  ä¸­ä½æ•°: {halluc_slope['median']:.6f}")
    report.append("")
    
    slope_diff = truthful_slope['mean'] - halluc_slope['mean']
    if slope_diff < -0.001:
        report.append("ğŸ’¡ è§£è¯»: æ­£ç¡®å›ç­”å‘ˆç°æ›´å¼ºçš„ä¸‹é™è¶‹åŠ¿ï¼Œè¯­ä¹‰æ¼‚ç§»æ›´æ˜æ˜¾")
    elif slope_diff > 0.001:
        report.append("ğŸ’¡ è§£è¯»: å¹»è§‰å›ç­”å‘ˆç°æ›´å¼ºçš„ä¸‹é™è¶‹åŠ¿ï¼Œè¯­ä¹‰æ¼‚ç§»æ›´æ˜æ˜¾")
    else:
        report.append("ğŸ’¡ è§£è¯»: ä¸¤ç±»å›ç­”çš„è¯­ä¹‰æ¼‚ç§»è¶‹åŠ¿ç›¸ä¼¼")
    report.append("")
    
    # å‰åæ®µå·®å¼‚åˆ†æ
    report.append("ğŸ“ˆ å‰åæ®µå·®å¼‚åˆ†æ")
    report.append("-" * 40)
    truthful_gap = drift_stats['truthful']['drift_gap']
    halluc_gap = drift_stats['hallucinated']['drift_gap']
    
    report.append(f"æ­£ç¡®å›ç­”:")
    report.append(f"  å‡å€¼: {truthful_gap['mean']:.4f}")
    report.append(f"  æ ‡å‡†å·®: {truthful_gap['std']:.4f}")
    report.append(f"  ä¸­ä½æ•°: {truthful_gap['median']:.4f}")
    report.append("")
    report.append(f"å¹»è§‰å›ç­”:")
    report.append(f"  å‡å€¼: {halluc_gap['mean']:.4f}")
    report.append(f"  æ ‡å‡†å·®: {halluc_gap['std']:.4f}")
    report.append(f"  ä¸­ä½æ•°: {halluc_gap['median']:.4f}")
    report.append("")
    
    gap_diff = truthful_gap['mean'] - halluc_gap['mean']
    if gap_diff > 0.01:
        report.append("ğŸ’¡ è§£è¯»: æ­£ç¡®å›ç­”çš„å‰åæ®µè¯­ä¹‰å·®å¼‚æ›´å¤§")
    elif gap_diff < -0.01:
        report.append("ğŸ’¡ è§£è¯»: å¹»è§‰å›ç­”çš„å‰åæ®µè¯­ä¹‰å·®å¼‚æ›´å¤§")
    else:
        report.append("ğŸ’¡ è§£è¯»: ä¸¤ç±»å›ç­”çš„å‰åæ®µè¯­ä¹‰å·®å¼‚ç›¸ä¼¼")
    report.append("")
    
    # æ—©æ™šæœŸæ¯”ç‡åˆ†æ
    report.append("ğŸ“ˆ æ—©æ™šæœŸæ¯”ç‡åˆ†æ")
    report.append("-" * 40)
    truthful_ratio = drift_stats['truthful']['early_late_ratio']
    halluc_ratio = drift_stats['hallucinated']['early_late_ratio']
    
    report.append(f"æ­£ç¡®å›ç­”:")
    report.append(f"  å‡å€¼: {truthful_ratio['mean']:.4f}")
    report.append(f"  æ ‡å‡†å·®: {truthful_ratio['std']:.4f}")
    report.append(f"  ä¸­ä½æ•°: {truthful_ratio['median']:.4f}")
    report.append("")
    report.append(f"å¹»è§‰å›ç­”:")
    report.append(f"  å‡å€¼: {halluc_ratio['mean']:.4f}")
    report.append(f"  æ ‡å‡†å·®: {halluc_ratio['std']:.4f}")
    report.append(f"  ä¸­ä½æ•°: {halluc_ratio['median']:.4f}")
    report.append("")
    
    ratio_diff = truthful_ratio['mean'] - halluc_ratio['mean']
    if ratio_diff > 0.05:
        report.append("ğŸ’¡ è§£è¯»: æ­£ç¡®å›ç­”çš„æ—©æœŸè¯­ä¹‰å¯¹é½ç›¸å¯¹æ›´å¼º")
    elif ratio_diff < -0.05:
        report.append("ğŸ’¡ è§£è¯»: å¹»è§‰å›ç­”çš„æ—©æœŸè¯­ä¹‰å¯¹é½ç›¸å¯¹æ›´å¼º")
    else:
        report.append("ğŸ’¡ è§£è¯»: ä¸¤ç±»å›ç­”çš„æ—©æ™šæœŸè¯­ä¹‰å¯¹é½æ¯”ç‡ç›¸ä¼¼")
    report.append("")
    
    # æ¼‚ç§»è­¦æŠ¥ç‚¹åˆ†æ
    report.append("ğŸš¨ æ¼‚ç§»è­¦æŠ¥ç‚¹åˆ†æ")
    report.append("-" * 40)
    
    truthful_with_drift = len([d for d in truthful_drift_data if d['drift_alert_point'] > 0])
    halluc_with_drift = len([d for d in hallucinated_drift_data if d['drift_alert_point'] > 0])
    
    truthful_drift_rate = truthful_with_drift / len(truthful_drift_data) * 100
    halluc_drift_rate = halluc_with_drift / len(hallucinated_drift_data) * 100
    
    report.append(f"æ­£ç¡®å›ç­”ä¸­æ£€æµ‹åˆ°è¯­ä¹‰æ¼‚ç§»çš„æ ·æœ¬: {truthful_with_drift}/{len(truthful_drift_data)} ({truthful_drift_rate:.1f}%)")
    report.append(f"å¹»è§‰å›ç­”ä¸­æ£€æµ‹åˆ°è¯­ä¹‰æ¼‚ç§»çš„æ ·æœ¬: {halluc_with_drift}/{len(hallucinated_drift_data)} ({halluc_drift_rate:.1f}%)")
    report.append("")
    
    if halluc_drift_rate > truthful_drift_rate + 5:
        report.append("ğŸ’¡ è§£è¯»: å¹»è§‰å›ç­”æ›´å®¹æ˜“è§¦å‘è¯­ä¹‰æ¼‚ç§»è­¦æŠ¥")
    elif truthful_drift_rate > halluc_drift_rate + 5:
        report.append("ğŸ’¡ è§£è¯»: æ­£ç¡®å›ç­”æ›´å®¹æ˜“è§¦å‘è¯­ä¹‰æ¼‚ç§»è­¦æŠ¥")
    else:
        report.append("ğŸ’¡ è§£è¯»: ä¸¤ç±»å›ç­”çš„è¯­ä¹‰æ¼‚ç§»æ£€æµ‹ç‡ç›¸ä¼¼")
    report.append("")
    
    # å…¸å‹æ¡ˆä¾‹åˆ†æ
    report.append("ğŸ“ å…¸å‹æ¡ˆä¾‹å±•ç¤º")
    report.append("-" * 40)
    
    # æ‰¾åˆ°æœ€å…·ä»£è¡¨æ€§çš„æ¼‚ç§»æ¡ˆä¾‹
    def find_representative_case(drift_data, drift_type):
        # æ‰¾åˆ°æ¼‚ç§»æœ€æ˜æ˜¾çš„æ¡ˆä¾‹ï¼ˆæ–œç‡æœ€è´Ÿçš„ï¼‰
        if not drift_data:
            return None
        min_slope_case = min(drift_data, key=lambda x: x['drift_slope'])
        return min_slope_case
    
    truthful_case = find_representative_case(truthful_drift_data, "truthful")
    halluc_case = find_representative_case(hallucinated_drift_data, "hallucinated")
    
    if truthful_case:
        report.append("æ­£ç¡®å›ç­”ä¸­æœ€æ˜¾è‘—çš„è¯­ä¹‰æ¼‚ç§»æ¡ˆä¾‹:")
        report.append(f"  æ¼‚ç§»æ–œç‡: {truthful_case['drift_slope']:.6f}")
        report.append(f"  å‰åæ®µå·®å¼‚: {truthful_case['drift_gap']:.4f}")
        report.append(f"  æ€»æ®µè½æ•°: {truthful_case['total_segments']}")
        if truthful_case['drift_alert_point'] > 0:
            report.append(f"  æ¼‚ç§»è­¦æŠ¥ç‚¹: ç¬¬{truthful_case['drift_alert_point']}æ®µ")
        else:
            report.append(f"  æ¼‚ç§»è­¦æŠ¥ç‚¹: æœªæ£€æµ‹åˆ°")
        report.append("")
    
    if halluc_case:
        report.append("å¹»è§‰å›ç­”ä¸­æœ€æ˜¾è‘—çš„è¯­ä¹‰æ¼‚ç§»æ¡ˆä¾‹:")
        report.append(f"  æ¼‚ç§»æ–œç‡: {halluc_case['drift_slope']:.6f}")
        report.append(f"  å‰åæ®µå·®å¼‚: {halluc_case['drift_gap']:.4f}")
        report.append(f"  æ€»æ®µè½æ•°: {halluc_case['total_segments']}")
        if halluc_case['drift_alert_point'] > 0:
            report.append(f"  æ¼‚ç§»è­¦æŠ¥ç‚¹: ç¬¬{halluc_case['drift_alert_point']}æ®µ")
        else:
            report.append(f"  æ¼‚ç§»è­¦æŠ¥ç‚¹: æœªæ£€æµ‹åˆ°")
        report.append("")
    
    # æ€»ç»“ä¸å»ºè®®
    report.append("ğŸ¯ æ€»ç»“ä¸å»ºè®®")
    report.append("-" * 40)
    report.append("åŸºäºè¯­ä¹‰æ¼‚ç§»åˆ†æçš„å‘ç°:")
    
    # ç”Ÿæˆè‡ªåŠ¨åŒ–æ€»ç»“
    key_findings = []
    
    if abs(slope_diff) > 0.001:
        if slope_diff < 0:
            key_findings.append("æ­£ç¡®å›ç­”è¡¨ç°å‡ºæ›´å¼ºçš„è¯­ä¹‰æ¼‚ç§»è¶‹åŠ¿")
        else:
            key_findings.append("å¹»è§‰å›ç­”è¡¨ç°å‡ºæ›´å¼ºçš„è¯­ä¹‰æ¼‚ç§»è¶‹åŠ¿")
    
    if abs(gap_diff) > 0.01:
        if gap_diff > 0:
            key_findings.append("æ­£ç¡®å›ç­”çš„å‰åæ®µè¯­ä¹‰å˜åŒ–æ›´å¤§")
        else:
            key_findings.append("å¹»è§‰å›ç­”çš„å‰åæ®µè¯­ä¹‰å˜åŒ–æ›´å¤§")
    
    if abs(halluc_drift_rate - truthful_drift_rate) > 5:
        if halluc_drift_rate > truthful_drift_rate:
            key_findings.append("å¹»è§‰å›ç­”æ›´é¢‘ç¹åœ°è§¦å‘è¯­ä¹‰æ¼‚ç§»è­¦æŠ¥")
        else:
            key_findings.append("æ­£ç¡®å›ç­”æ›´é¢‘ç¹åœ°è§¦å‘è¯­ä¹‰æ¼‚ç§»è­¦æŠ¥")
    
    if key_findings:
        for i, finding in enumerate(key_findings, 1):
            report.append(f"{i}. {finding}")
    else:
        report.append("1. æ­£ç¡®å›ç­”å’Œå¹»è§‰å›ç­”åœ¨è¯­ä¹‰æ¼‚ç§»æ–¹é¢è¡¨ç°ç›¸ä¼¼")
    
    report.append("")
    report.append("å»ºè®®:")
    report.append("- å¯ä»¥å°†è¯­ä¹‰æ¼‚ç§»æŒ‡æ ‡ä½œä¸ºå¹»è§‰æ£€æµ‹çš„è¡¥å……ç‰¹å¾")
    report.append("- æ¼‚ç§»æ–œç‡å’Œå‰åæ®µå·®å¼‚å¯èƒ½æ˜¯æœ€æœ‰ä»·å€¼çš„æŒ‡æ ‡")
    report.append("- å»ºè®®ç»“åˆPRDå’ŒSASæŒ‡æ ‡è¿›è¡Œç»¼åˆåˆ†æ")
    report.append("")
    report.append("="*80)
    
    return "\n".join(report)

def plot_rq1_figures_separate(truthful_prd, hallucinated_prd, truthful_gass, hallucinated_gass, output_dir):
    """
    ç»˜åˆ¶åˆ†ç¦»çš„RQ1åˆ†æå›¾è¡¨ï¼šå°†3Ã—2å¸ƒå±€åˆ†æˆ3å¼ ç‹¬ç«‹çš„å›¾
    ä½¿ç”¨matplotlibåŸå§‹ç°åº•é£æ ¼
    """
    import random
    import pandas as pd
    
    # ä½¿ç”¨matplotlibé»˜è®¤æ ·å¼ï¼Œç§»é™¤seaborn
    plt.style.use('default')
    
    # æ•°æ®é‡‡æ ·å‡†å¤‡ï¼ˆç”¨äºç‰¹å¾ç©ºé—´å›¾ï¼‰
    if len(truthful_prd) > 750:
        truthful_indices = random.sample(range(len(truthful_prd)), 750)
        truthful_prd_sample = [truthful_prd[i] for i in truthful_indices]
        truthful_gass_sample = [truthful_gass[i] for i in truthful_indices]
    else:
        truthful_prd_sample = truthful_prd
        truthful_gass_sample = truthful_gass
    
    if len(hallucinated_prd) > 750:
        hall_indices = random.sample(range(len(hallucinated_prd)), 750)
        hall_prd_sample = [hallucinated_prd[i] for i in hall_indices]
        hall_gass_sample = [hallucinated_gass[i] for i in hall_indices]
    else:
        hall_prd_sample = hallucinated_prd
        hall_gass_sample = hallucinated_gass
    
    # ç¬¬ä¸€å¼ å›¾ï¼šåˆ†å¸ƒç›´æ–¹å›¾ (Row 1)
    fig, axes = plt.subplots(1, 2, figsize=(6.5, 3))
    
    # PRDåˆ†å¸ƒç›´æ–¹å›¾
    ax = axes[0]
    ax.hist(hall_prd_sample, alpha=0.5, label='Hallucinated', bins=30, color='red')
    ax.hist(truthful_prd_sample, alpha=0.5, label='Truthful', bins=30, color='blue')
    ax.set_title('PRD Score Distribution')
    ax.set_xlabel('PRD Score')
    ax.set_ylabel('Frequency')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # SASåˆ†å¸ƒç›´æ–¹å›¾ï¼ˆåŸGASSï¼‰
    ax = axes[1]
    ax.hist(hall_gass_sample, alpha=0.5, label='Hallucinated', bins=30, color='red')
    ax.hist(truthful_gass_sample, alpha=0.5, label='Truthful', bins=30, color='blue')
    ax.set_title('SAS Score Distribution')
    ax.set_xlabel('SAS Score')
    ax.set_ylabel('Frequency')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    dist_path_png = os.path.join(output_dir, 'rq1_distributions.png')
    dist_path_pdf = os.path.join(output_dir, 'rq1_distributions.pdf')
    plt.savefig(dist_path_png, dpi=600, bbox_inches='tight')  # æœ€é«˜åˆ†è¾¨ç‡
    plt.savefig(dist_path_pdf, bbox_inches='tight')  # PDFç‰ˆæœ¬
    plt.close()
    
    # ç¬¬äºŒå¼ å›¾ï¼šç®€æ´ç‰ˆç®±çº¿å›¾ (Row 2)
    fig, axes = plt.subplots(1, 2, figsize=(6.5, 3))
    
    # PRDç®±çº¿å›¾ - ç®€æ´ç‰ˆæœ¬
    ax = axes[0]
    data_prd = [hallucinated_prd, truthful_prd]
    bp_prd = ax.boxplot(data_prd, tick_labels=['Hallucinated', 'Truthful'], showfliers=False)
    
    ax.set_title('PRD Score Box Plot')
    ax.set_ylabel('PRD Score')
    ax.grid(True, alpha=0.3)
    
    # SASç®±çº¿å›¾ - ç®€æ´ç‰ˆæœ¬
    ax = axes[1]
    data_gass = [hallucinated_gass, truthful_gass]
    bp_gass = ax.boxplot(data_gass, tick_labels=['Hallucinated', 'Truthful'], showfliers=False)
    
    ax.set_title('SAS Score Box Plot')
    ax.set_ylabel('SAS Score')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    box_path_png = os.path.join(output_dir, 'rq1_boxplots.png')
    box_path_pdf = os.path.join(output_dir, 'rq1_boxplots.pdf')
    plt.savefig(box_path_png, dpi=600, bbox_inches='tight')  # æœ€é«˜åˆ†è¾¨ç‡
    plt.savefig(box_path_pdf, bbox_inches='tight')  # PDFç‰ˆæœ¬
    plt.close()
    
    # ç¬¬ä¸‰å¼ å›¾ï¼šç‰¹å¾ç©ºé—´å’Œç›¸å…³æ€§ (Row 3)
    fig, axes = plt.subplots(1, 2, figsize=(6.5, 3))
    
    # ç‰¹å¾ç©ºé—´æ•£ç‚¹å›¾
    ax = axes[0]
    ax.scatter(hall_prd_sample, hall_gass_sample, alpha=0.5, label='Hallucinated', 
               color='red', s=15, edgecolors='none')
    ax.scatter(truthful_prd_sample, truthful_gass_sample, alpha=0.6, label='Truthful', 
               color='blue', s=15, edgecolors='none')
    ax.set_xlabel('PRD Score')
    ax.set_ylabel('SAS Score')
    ax.set_title('Feature Space Distribution')
    ax.legend(loc='upper left', handletextpad=0.3, bbox_to_anchor=(0.02, 0.98))
    ax.grid(True, alpha=0.3)
    
    # ç›¸å…³æ€§çƒ­å›¾
    ax = axes[1]
    df = pd.DataFrame({
        'PRD': np.concatenate([truthful_prd, hallucinated_prd]),
        'SAS': np.concatenate([truthful_gass, hallucinated_gass]),
        'Label': np.concatenate([np.ones(len(truthful_prd)), np.zeros(len(hallucinated_prd))])
    })
    corr_matrix = df.corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax)
    ax.set_title('Feature Correlation Matrix')
    
    plt.tight_layout()
    analysis_path_png = os.path.join(output_dir, 'rq1_feature_analysis.png')
    analysis_path_pdf = os.path.join(output_dir, 'rq1_feature_analysis.pdf')
    plt.savefig(analysis_path_png, dpi=600, bbox_inches='tight')  # æœ€é«˜åˆ†è¾¨ç‡
    plt.savefig(analysis_path_pdf, bbox_inches='tight')  # PDFç‰ˆæœ¬
    plt.close()
    
    # æ¢å¤seabornæ ·å¼ï¼ˆå¦‚æœå…¶ä»–éƒ¨åˆ†éœ€è¦ï¼‰
    plt.style.use('seaborn-v0_8')
    
    # è¿”å›æ‰€æœ‰ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    return {
        'distributions': {'png': dist_path_png, 'pdf': dist_path_pdf},
        'boxplots': {'png': box_path_png, 'pdf': box_path_pdf}, 
        'analysis': {'png': analysis_path_png, 'pdf': analysis_path_pdf}
    }

def generate_detailed_report(stats_dict):
    """
    ç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡åˆ†ææŠ¥å‘Š
    """
    report = []
    report.append("="*80)
    report.append("RQ1å®éªŒè¯¦ç»†åˆ†ææŠ¥å‘Š")
    report.append("="*80)
    report.append("")
    
    # è¯„ä¼°æ–¹æ³•ä¿¡æ¯
    eval_method = stats_dict.get('evaluation_method', 'Unknown')
    has_variants = stats_dict.get('has_prd_variants', False)
    
    report.append("ğŸ” è¯„ä¼°æ–¹æ³•")
    report.append("-" * 40)
    report.append(f"å¹»è§‰åˆ¤æ–­æ–¹æ³•: {eval_method}")
    if eval_method == 'SQuAD':
        report.append("  - ä½¿ç”¨SQuADé£æ ¼è¯„ä¼°ï¼ˆF1åˆ†æ•° + æ ‡å‡†åŒ–åŒ¹é…ï¼‰")
        report.append("  - æ›´å‡†ç¡®ï¼Œèƒ½å¤„ç†æ ¼å¼å·®å¼‚å’Œéƒ¨åˆ†åŒ¹é…")
    elif eval_method == 'Hit@1':
        report.append("  - ä½¿ç”¨Hit@1ç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…ï¼ˆä¸åº”å‡ºç°ï¼‰")
        report.append("  - è¯¥æ–¹æ³•å·²è¢«å¼ƒç”¨")
    
    report.append(f"PRDå˜ä½“åˆ†æ: {'å¯ç”¨' if has_variants else 'æœªå¯ç”¨'}")
    if has_variants:
        report.append("  - åŒ…å«6ç§PRDå˜ä½“ï¼šStrict, Contrast, Relative, Max, Weighted, Entropy")
        report.append("  - ç”¨äºæ‰¾åˆ°æœ€ä½³çš„æ³¨æ„åŠ›åˆ©ç”¨åº¦é‡æ–¹æ³•")
    else:
        report.append("  - ä»…ä½¿ç”¨åŸå§‹PRDè®¡ç®—")
        report.append("  - å¯è¿è¡Œadd_prd_variants.pyæ·»åŠ å˜ä½“åˆ†æ")
    report.append("")
    
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    report.append("ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
    report.append("-" * 40)
    report.append(f"æ­£ç¡®å›ç­”æ ·æœ¬æ•°: {stats_dict['truthful']['count']}")
    report.append(f"å¹»è§‰å›ç­”æ ·æœ¬æ•°: {stats_dict['hallucinated']['count']}")
    report.append(f"æ€»æ ·æœ¬æ•°: {stats_dict['truthful']['count'] + stats_dict['hallucinated']['count']}")
    report.append(f"å¹»è§‰ç‡: {stats_dict['hallucinated']['count'] / (stats_dict['truthful']['count'] + stats_dict['hallucinated']['count']):.2%}")
    report.append("")
    
    # PRDåˆ†æ
    report.append("ğŸ“ˆ PRDåˆ†æ•°åˆ†æ")
    report.append("-" * 40)
    report.append(f"æ­£ç¡®å›ç­” - å‡å€¼: {stats_dict['truthful']['prd']['mean']:.4f}, æ ‡å‡†å·®: {stats_dict['truthful']['prd']['std']:.4f}, ä¸­ä½æ•°: {stats_dict['truthful']['prd']['median']:.4f}")
    report.append(f"å¹»è§‰å›ç­” - å‡å€¼: {stats_dict['hallucinated']['prd']['mean']:.4f}, æ ‡å‡†å·®: {stats_dict['hallucinated']['prd']['std']:.4f}, ä¸­ä½æ•°: {stats_dict['hallucinated']['prd']['median']:.4f}")
    report.append(f"å‡å€¼å·®å¼‚: {stats_dict['truthful']['prd']['mean'] - stats_dict['hallucinated']['prd']['mean']:.4f}")
    report.append("")
    
    # SASåˆ†æï¼ˆåŸGASSï¼‰
    report.append("ğŸ“ˆ SASåˆ†æ•°åˆ†æ")
    report.append("-" * 40)
    report.append(f"æ­£ç¡®å›ç­” - å‡å€¼: {stats_dict['truthful']['gass']['mean']:.4f}, æ ‡å‡†å·®: {stats_dict['truthful']['gass']['std']:.4f}, ä¸­ä½æ•°: {stats_dict['truthful']['gass']['median']:.4f}")
    report.append(f"å¹»è§‰å›ç­” - å‡å€¼: {stats_dict['hallucinated']['gass']['mean']:.4f}, æ ‡å‡†å·®: {stats_dict['hallucinated']['gass']['std']:.4f}, ä¸­ä½æ•°: {stats_dict['hallucinated']['gass']['median']:.4f}")
    report.append(f"å‡å€¼å·®å¼‚: {stats_dict['truthful']['gass']['mean'] - stats_dict['hallucinated']['gass']['mean']:.4f}")
    report.append("")
    
    # Semantic Driftåˆ†æ
    if 'semantic_drift' in stats_dict:
        report.append("ğŸŒŠ è¯­ä¹‰æ¼‚ç§» (Semantic Drift) åˆ†æ")
        report.append("-" * 40)
        
        drift_data = stats_dict['semantic_drift']
        
        # æ¼‚ç§»æ–œç‡åˆ†æ
        report.append("æ¼‚ç§»æ–œç‡ (Drift Slope):")
        report.append(f"  æ­£ç¡®å›ç­” - å‡å€¼: {drift_data['truthful']['drift_slope']['mean']:.6f}, æ ‡å‡†å·®: {drift_data['truthful']['drift_slope']['std']:.6f}")
        report.append(f"  å¹»è§‰å›ç­” - å‡å€¼: {drift_data['hallucinated']['drift_slope']['mean']:.6f}, æ ‡å‡†å·®: {drift_data['hallucinated']['drift_slope']['std']:.6f}")
        
        # å‰åæ®µå·®å¼‚åˆ†æ
        report.append("å‰åæ®µå·®å¼‚ (Drift Gap):")
        report.append(f"  æ­£ç¡®å›ç­” - å‡å€¼: {drift_data['truthful']['drift_gap']['mean']:.4f}, æ ‡å‡†å·®: {drift_data['truthful']['drift_gap']['std']:.4f}")
        report.append(f"  å¹»è§‰å›ç­” - å‡å€¼: {drift_data['hallucinated']['drift_gap']['mean']:.4f}, æ ‡å‡†å·®: {drift_data['hallucinated']['drift_gap']['std']:.4f}")
        
        # æ—©æ™šæœŸæ¯”ç‡åˆ†æ
        report.append("æ—©æ™šæœŸæ¯”ç‡ (Early/Late Ratio):")
        report.append(f"  æ­£ç¡®å›ç­” - å‡å€¼: {drift_data['truthful']['early_late_ratio']['mean']:.4f}, æ ‡å‡†å·®: {drift_data['truthful']['early_late_ratio']['std']:.4f}")
        report.append(f"  å¹»è§‰å›ç­” - å‡å€¼: {drift_data['hallucinated']['early_late_ratio']['mean']:.4f}, æ ‡å‡†å·®: {drift_data['hallucinated']['early_late_ratio']['std']:.4f}")
        
        # æ¼‚ç§»è­¦æŠ¥ç‚¹åˆ†æ
        truthful_alert_rate = len([d for d in drift_data['truthful_raw'] if d['drift_alert_point'] > 0]) / len(drift_data['truthful_raw']) * 100
        halluc_alert_rate = len([d for d in drift_data['hallucinated_raw'] if d['drift_alert_point'] > 0]) / len(drift_data['hallucinated_raw']) * 100
        
        report.append("æ¼‚ç§»è­¦æŠ¥ç‚¹æ£€æµ‹:")
        report.append(f"  æ­£ç¡®å›ç­”ä¸­å‘ç”Ÿæ¼‚ç§»çš„æ¯”ä¾‹: {truthful_alert_rate:.1f}%")
        report.append(f"  å¹»è§‰å›ç­”ä¸­å‘ç”Ÿæ¼‚ç§»çš„æ¯”ä¾‹: {halluc_alert_rate:.1f}%")
        
        # Semantic Driftç»Ÿè®¡æ£€éªŒ
        if 'semantic_drift_tests' in stats_dict['statistical_tests']:
            drift_tests = stats_dict['statistical_tests']['semantic_drift_tests']
            
            report.append("")
            report.append("è¯­ä¹‰æ¼‚ç§»ç»Ÿè®¡æ£€éªŒ:")
            for metric_name, test_result in drift_tests.items():
                significance = "æ˜¾è‘—" if test_result['p_value'] < 0.05 else "ä¸æ˜¾è‘—"
                report.append(f"  {metric_name}: på€¼={test_result['p_value']:.6f} ({significance})")
        
        report.append("")
        report.append("ğŸ’¡ è¯­ä¹‰æ¼‚ç§»ç»“è®º:")
        slope_diff = drift_data['truthful']['drift_slope']['mean'] - drift_data['hallucinated']['drift_slope']['mean']
        gap_diff = drift_data['truthful']['drift_gap']['mean'] - drift_data['hallucinated']['drift_gap']['mean']
        
        if slope_diff < -0.001:  # å¹»è§‰å›ç­”ä¸‹é™æ›´å¿«
            report.append("  - å¹»è§‰å›ç­”å±•ç°æ›´æ˜æ˜¾çš„è¯­ä¹‰æ¼‚ç§»è¶‹åŠ¿ï¼ˆæ–œç‡æ›´è´Ÿï¼‰")
        elif slope_diff > 0.001:  # æ­£ç¡®å›ç­”ä¸‹é™æ›´å¿«
            report.append("  - æ­£ç¡®å›ç­”å±•ç°æ›´æ˜æ˜¾çš„è¯­ä¹‰æ¼‚ç§»è¶‹åŠ¿ï¼ˆæ–œç‡æ›´è´Ÿï¼‰")
        else:
            report.append("  - ä¸¤ç±»å›ç­”çš„è¯­ä¹‰æ¼‚ç§»æ–œç‡æ— æ˜æ˜¾å·®å¼‚")
            
        if gap_diff > 0.01:  # æ­£ç¡®å›ç­”å‰åå·®å¼‚æ›´å¤§
            report.append("  - æ­£ç¡®å›ç­”çš„å‰åæ®µè¯­ä¹‰å·®å¼‚æ›´å¤§")
        elif gap_diff < -0.01:  # å¹»è§‰å›ç­”å‰åå·®å¼‚æ›´å¤§
            report.append("  - å¹»è§‰å›ç­”çš„å‰åæ®µè¯­ä¹‰å·®å¼‚æ›´å¤§")
        else:
            report.append("  - ä¸¤ç±»å›ç­”çš„å‰åæ®µè¯­ä¹‰å·®å¼‚ç›¸ä¼¼")
            
        if halluc_alert_rate > truthful_alert_rate + 5:
            report.append("  - å¹»è§‰å›ç­”æ›´å®¹æ˜“è§¦å‘è¯­ä¹‰æ¼‚ç§»è­¦æŠ¥")
        elif truthful_alert_rate > halluc_alert_rate + 5:
            report.append("  - æ­£ç¡®å›ç­”æ›´å®¹æ˜“è§¦å‘è¯­ä¹‰æ¼‚ç§»è­¦æŠ¥")
        else:
            report.append("  - ä¸¤ç±»å›ç­”çš„è¯­ä¹‰æ¼‚ç§»è­¦æŠ¥ç‡ç›¸ä¼¼")
        
        report.append("")
    
    # ç»Ÿè®¡æ£€éªŒ
    report.append("ğŸ”¬ ç»Ÿè®¡æ£€éªŒç»“æœ")
    report.append("-" * 40)
    
    # PRD tæ£€éªŒ
    prd_t = stats_dict['statistical_tests']['prd_t_test']
    report.append(f"PRD tæ£€éªŒ:")
    report.append(f"  tç»Ÿè®¡é‡: {prd_t['t_statistic']:.4f}")
    report.append(f"  på€¼: {prd_t['p_value']:.6f}")
    report.append(f"  æ˜¾è‘—æ€§: {'æ˜¾è‘—' if prd_t['p_value'] < 0.05 else 'ä¸æ˜¾è‘—'} (Î±=0.05)")
    
    # SAS tæ£€éªŒï¼ˆåŸGASSï¼‰
    gass_t = stats_dict['statistical_tests']['gass_t_test']
    report.append(f"SAS tæ£€éªŒ:")
    report.append(f"  tç»Ÿè®¡é‡: {gass_t['t_statistic']:.4f}")
    report.append(f"  på€¼: {gass_t['p_value']:.6f}")
    report.append(f"  æ˜¾è‘—æ€§: {'æ˜¾è‘—' if gass_t['p_value'] < 0.05 else 'ä¸æ˜¾è‘—'} (Î±=0.05)")
    report.append("")
    
    # æ•ˆåº”å¤§å°
    report.append("ğŸ“ æ•ˆåº”å¤§å° (Cohen's d)")
    report.append("-" * 40)
    report.append(f"PRD Cohen's d: {stats_dict['effect_sizes']['prd_cohens_d']:.4f}")
    report.append(f"SAS Cohen's d: {stats_dict['effect_sizes']['gass_cohens_d']:.4f}")
    report.append("")
    report.append("æ•ˆåº”å¤§å°è§£é‡Š:")
    report.append("  å°æ•ˆåº”: 0.2")
    report.append("  ä¸­ç­‰æ•ˆåº”: 0.5")
    report.append("  å¤§æ•ˆåº”: 0.8")
    report.append("")
    
    # ç»“è®º
    report.append("ğŸ’¡ åˆ†æç»“è®º")
    report.append("-" * 40)
    
    # PRDç»“è®º
    prd_significant = prd_t['p_value'] < 0.05
    prd_effect_size = abs(stats_dict['effect_sizes']['prd_cohens_d'])
    prd_direction = "æ­£ç¡®å›ç­”" if stats_dict['truthful']['prd']['mean'] > stats_dict['hallucinated']['prd']['mean'] else "å¹»è§‰å›ç­”"
    
    report.append(f"PRDæŒ‡æ ‡: {prd_direction}å…·æœ‰æ›´é«˜çš„PRDåˆ†æ•°")
    if prd_significant:
        if prd_effect_size >= 0.8:
            report.append("  - å·®å¼‚æ˜¾è‘—ä¸”æ•ˆåº”å¤§ï¼ŒPRDæ˜¯åŒºåˆ†æ­£ç¡®ä¸å¹»è§‰å›ç­”çš„å¼ºæŒ‡æ ‡")
        elif prd_effect_size >= 0.5:
            report.append("  - å·®å¼‚æ˜¾è‘—ä¸”æ•ˆåº”ä¸­ç­‰ï¼ŒPRDæ˜¯åŒºåˆ†æ­£ç¡®ä¸å¹»è§‰å›ç­”çš„æœ‰æ•ˆæŒ‡æ ‡")
        elif prd_effect_size >= 0.2:
            report.append("  - å·®å¼‚æ˜¾è‘—ä½†æ•ˆåº”è¾ƒå°ï¼ŒPRDåœ¨åŒºåˆ†æ­£ç¡®ä¸å¹»è§‰å›ç­”æ–¹é¢æœ‰ä¸€å®šä½œç”¨")
        else:
            report.append("  - å·®å¼‚æ˜¾è‘—ä½†æ•ˆåº”å¾ˆå°ï¼ŒPRDçš„å®é™…æ„ä¹‰æœ‰é™")
    else:
        report.append("  - å·®å¼‚ä¸æ˜¾è‘—ï¼ŒPRDæ— æ³•æœ‰æ•ˆåŒºåˆ†æ­£ç¡®ä¸å¹»è§‰å›ç­”")
    
    # SASç»“è®ºï¼ˆåŸGASSï¼‰
    gass_significant = gass_t['p_value'] < 0.05
    gass_effect_size = abs(stats_dict['effect_sizes']['gass_cohens_d'])
    gass_direction = "æ­£ç¡®å›ç­”" if stats_dict['truthful']['gass']['mean'] > stats_dict['hallucinated']['gass']['mean'] else "å¹»è§‰å›ç­”"
    report.append(f"SASæŒ‡æ ‡: {gass_direction}å…·æœ‰æ›´é«˜çš„SASåˆ†æ•°")
    if gass_significant:
        if gass_effect_size >= 0.8:
            report.append("  - å·®å¼‚æ˜¾è‘—ä¸”æ•ˆåº”å¤§ï¼ŒSASæ˜¯åŒºåˆ†æ­£ç¡®ä¸å¹»è§‰å›ç­”çš„å¼ºæŒ‡æ ‡")
        elif gass_effect_size >= 0.5:
            report.append("  - å·®å¼‚æ˜¾è‘—ä¸”æ•ˆåº”ä¸­ç­‰ï¼ŒSASæ˜¯åŒºåˆ†æ­£ç¡®ä¸å¹»è§‰å›ç­”çš„æœ‰æ•ˆæŒ‡æ ‡")
        elif gass_effect_size >= 0.2:
            report.append("  - å·®å¼‚æ˜¾è‘—ä½†æ•ˆåº”è¾ƒå°ï¼ŒSASåœ¨åŒºåˆ†æ­£ç¡®ä¸å¹»è§‰å›ç­”æ–¹é¢æœ‰ä¸€å®šä½œç”¨")
        else:
            report.append("  - å·®å¼‚æ˜¾è‘—ä½†æ•ˆåº”å¾ˆå°ï¼ŒSASçš„å®é™…æ„ä¹‰æœ‰é™")
    else:
        report.append("  - å·®å¼‚ä¸æ˜¾è‘—ï¼ŒSASæ— æ³•æœ‰æ•ˆåŒºåˆ†æ­£ç¡®ä¸å¹»è§‰å›ç­”")
    
    # PRDå˜ä½“åˆ†æï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if has_variants and 'prd_variants' in stats_dict:
        report.append("ğŸ“ˆ PRDå˜ä½“åˆ†æ")
        report.append("-" * 40)
        
        variants_performance = []
        
        for variant_name, variant_stats in stats_dict['prd_variants'].items():
            truthful_mean = variant_stats['truthful']['mean']
            hallucinated_mean = variant_stats['hallucinated']['mean']
            mean_diff = truthful_mean - hallucinated_mean
            
            # è·å–ç»Ÿè®¡æ£€éªŒç»“æœ
            variant_tests = stats_dict.get('statistical_tests', {}).get('prd_variants', {})
            variant_effect_sizes = stats_dict.get('effect_sizes', {}).get('prd_variants', {})
            
            if variant_name in variant_tests:
                p_value = variant_tests[variant_name]['p_value']
                is_significant = p_value < 0.05
            else:
                p_value = float('inf')
                is_significant = False
                
            if variant_name in variant_effect_sizes:
                effect_size = abs(variant_effect_sizes[variant_name])
            else:
                effect_size = 0.0
            
            # è®°å½•å˜ä½“è¡¨ç°
            variants_performance.append({
                'name': variant_name,
                'mean_diff': mean_diff,
                'p_value': p_value,
                'effect_size': effect_size,
                'is_significant': is_significant,
                'truthful_mean': truthful_mean,
                'hallucinated_mean': hallucinated_mean
            })
            
            report.append(f"{variant_name.upper()}:")
            report.append(f"  æ­£ç¡®å›ç­”å‡å€¼: {truthful_mean:.4f}")
            report.append(f"  å¹»è§‰å›ç­”å‡å€¼: {hallucinated_mean:.4f}")
            report.append(f"  å·®å¼‚: {mean_diff:.4f}")
            if not np.isinf(p_value):
                report.append(f"  på€¼: {p_value:.6f} ({'æ˜¾è‘—' if is_significant else 'ä¸æ˜¾è‘—'})")
                report.append(f"  æ•ˆåº”å¤§å°: {effect_size:.4f}")
            report.append("")
        
        # æ’åºå¹¶æ¨èæœ€ä½³å˜ä½“
        report.append("ğŸ† PRDå˜ä½“æ€§èƒ½æ’å")
        report.append("-" * 40)
        
        # å…ˆæŒ‰æ˜¾è‘—æ€§æ’åºï¼Œå†æŒ‰æ•ˆåº”å¤§å°æ’åº
        variants_performance.sort(key=lambda x: (-int(x['is_significant']), -x['effect_size']))
        
        for i, variant in enumerate(variants_performance[:3], 1):  # åªæ˜¾ç¤ºå‰3å
            name = variant['name']
            diff = variant['mean_diff']
            effect = variant['effect_size']
            sig_text = "âœ“" if variant['is_significant'] else "âœ—"
            
            direction = "æ­£ç¡®>å¹»è§‰" if diff > 0 else "å¹»è§‰>æ­£ç¡®"
            report.append(f"{i}. {name.upper()}: å·®å¼‚={diff:.4f} ({direction}), æ•ˆåº”={effect:.4f}, æ˜¾è‘—æ€§={sig_text}")
        
        # æ¨è
        best_variant = variants_performance[0]
        report.append("")
        report.append("ğŸ’¡ æ¨èä½¿ç”¨çš„PRDå˜ä½“")
        report.append("-" * 40)
        report.append(f"æœ€ä½³å˜ä½“: {best_variant['name'].upper()}")
        
        if best_variant['is_significant'] and best_variant['effect_size'] >= 0.2:
            report.append(f"  - ç»Ÿè®¡æ˜¾è‘— (p={best_variant['p_value']:.6f})")
            report.append(f"  - æ•ˆåº”å¤§å°: {best_variant['effect_size']:.4f}")
            
            if best_variant['mean_diff'] > 0:
                report.append("  - æ­£ç¡®ç­”æ¡ˆçš„PRDåˆ†æ•°æ›´é«˜ï¼ˆç¬¦åˆé¢„æœŸï¼‰")
            else:
                report.append("  - å¹»è§‰ç­”æ¡ˆçš„PRDåˆ†æ•°æ›´é«˜ï¼ˆéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥ï¼‰")
        else:
            report.append("  - æ³¨æ„ï¼šæœ€ä½³å˜ä½“ä»ç„¶è¡¨ç°ä¸ç†æƒ³")
            report.append("  - å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–å°è¯•å…¶ä»–æ–¹æ³•")
        
        report.append("")
    
    report.append("")
    report.append("="*80)
    
    return "\n".join(report)

def run_rq1_experiment(args):
    """
    è¿è¡ŒRQ1å®éªŒï¼šåˆ†æLLMå¯¹gold-relevant triplesçš„æ³¨æ„åŠ›ä¸å¹»è§‰çš„å…³ç³»
    
    åªä½¿ç”¨SQuADé£æ ¼å¹»è§‰åˆ¤æ–­æ–¹æ³•ï¼š
    - SQuADé£æ ¼è¯„ä¼°ï¼šåŸºäºF1åˆ†æ•°å’Œæ ‡å‡†åŒ–åŒ¹é…ï¼Œæ›´å‡†ç¡®
    - å¦‚æœæ•°æ®ä¸­ä¸åŒ…å«SQuADè¯„ä¼°ç»“æœï¼Œå°†è·³è¿‡ç›¸åº”æ ·æœ¬
    """
    logger = logging.getLogger(__name__)
    logger.info("Starting RQ1 experiment...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    filename = os.path.basename(args.results_file)
    # ä»æ–‡ä»¶åä¸­æå–å‰ç¼€ï¼ˆå¦‚ "dev_simple" æ¥è‡ª "dev_simple_inference_results_..."ï¼‰
    filename_parts = filename.split("_")
    if len(filename_parts) >= 2:
        prefix = f"{filename_parts[0]}_{filename_parts[1]}"
    else:
        prefix = filename_parts[0] if filename_parts else "unknown"
    
    output_dir = os.path.join('experiment_records', 'empirical_rq1', 
                             f'{prefix}_rq1_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # ä½¿ç”¨æä¾›çš„ç»“æœæ–‡ä»¶
        results_file = args.results_file
        
        # åˆ†æç»“æœ
        truthful_prd, hallucinated_prd = [], []
        truthful_gass, hallucinated_gass = [], []
        # Semantic Driftæ•°æ®
        truthful_samples, hallucinated_samples = [], []
        # PRDå˜ä½“æ•°æ®
        prd_variants_data = {
            'prd_strict': {'truthful': [], 'hallucinated': []},
            'prd_contrast': {'truthful': [], 'hallucinated': []},
            'prd_relative': {'truthful': [], 'hallucinated': []},
            'prd_max': {'truthful': [], 'hallucinated': []},
            'prd_weighted': {'truthful': [], 'hallucinated': []},
            'prd_entropy': {'truthful': [], 'hallucinated': []}
        }
        has_prd_variants = False
        evaluation_method = None
        
        with open(results_file, 'r', encoding='utf-8') as f:
            line_count = 0
            for line in f:
                if not line.strip():
                    continue
                data = json.loads(line)
                if 'config' in data or 'stats' in data:
                    continue
                # åªå¤„ç†å‰5000æ¡æœ‰æ•ˆæ•°æ®
                line_count += 1
                if line_count > 5000:
                    break
                    
                # è·å–PRDå’ŒGASSåˆ†æ•°
                prd_score = data.get('prd_score', 0.0)
                gass_score = data.get('gass_score', 0.0)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«PRDå˜ä½“
                if not has_prd_variants:
                    if any(variant in data for variant in prd_variants_data.keys()):
                        has_prd_variants = True
                        print("æ£€æµ‹åˆ°PRDå˜ä½“æ•°æ®ï¼Œå°†è¿›è¡Œå˜ä½“åˆ†æ")
                
                # åªä½¿ç”¨SQuADè¯„ä¼°æ–¹æ³•ï¼Œä¸å†å›é€€åˆ°Hit@1
                squad_eval = data.get('squad_evaluation')
                if squad_eval is not None:
                    # ä½¿ç”¨SQuADè¯„ä¼°ï¼šsquad_is_hallucinationä¸ºFalseè¡¨ç¤ºéå¹»è§‰ï¼ˆæ­£ç¡®ï¼‰
                    is_correct = not squad_eval.get('squad_is_hallucination', True)
                    if evaluation_method is None:
                        evaluation_method = 'SQuAD'
                else:
                    # æ²¡æœ‰SQuADè¯„ä¼°æ•°æ®ï¼Œè·³è¿‡è¯¥æ ·æœ¬
                    continue
                
                # æ ¹æ®è¯„ä¼°ç»“æœåˆ†ç»„
                if is_correct:
                    truthful_prd.append(prd_score)
                    truthful_gass.append(gass_score)
                    truthful_samples.append(data)  # ä¿å­˜å®Œæ•´æ ·æœ¬ç”¨äºSemantic Driftåˆ†æ
                    # æ”¶é›†PRDå˜ä½“æ•°æ®
                    if has_prd_variants:
                        for variant in prd_variants_data.keys():
                            variant_score = data.get(variant, 0.0)
                            prd_variants_data[variant]['truthful'].append(variant_score)
                else:
                    hallucinated_prd.append(prd_score)
                    hallucinated_gass.append(gass_score)
                    hallucinated_samples.append(data)  # ä¿å­˜å®Œæ•´æ ·æœ¬ç”¨äºSemantic Driftåˆ†æ
                    # æ”¶é›†PRDå˜ä½“æ•°æ®
                    if has_prd_variants:
                        for variant in prd_variants_data.keys():
                            variant_score = data.get(variant, 0.0)
                            prd_variants_data[variant]['hallucinated'].append(variant_score)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        stats_dict = {
            'evaluation_method': evaluation_method,
            'has_prd_variants': has_prd_variants,
            'truthful': {
                'count': len(truthful_prd),
                'prd': {
                    'mean': np.mean(truthful_prd),
                    'std': np.std(truthful_prd),
                    'median': np.median(truthful_prd)
                },
                'gass': {
                    'mean': np.mean(truthful_gass),
                    'std': np.std(truthful_gass),
                    'median': np.median(truthful_gass)
                }
            },
            'hallucinated': {
                'count': len(hallucinated_prd),
                'prd': {
                    'mean': np.mean(hallucinated_prd),
                    'std': np.std(hallucinated_prd),
                    'median': np.median(hallucinated_prd)
                },
                'gass': {
                    'mean': np.mean(hallucinated_gass),
                    'std': np.std(hallucinated_gass),
                    'median': np.median(hallucinated_gass)
                }
            }
        }
        
        # å¦‚æœæœ‰PRDå˜ä½“ï¼Œæ·»åŠ å˜ä½“ç»Ÿè®¡
        if has_prd_variants:
            stats_dict['prd_variants'] = {}
            for variant in prd_variants_data.keys():
                truthful_variant = prd_variants_data[variant]['truthful']
                hallucinated_variant = prd_variants_data[variant]['hallucinated']
                
                if truthful_variant and hallucinated_variant:
                    stats_dict['prd_variants'][variant] = {
                        'truthful': {
                            'mean': np.mean(truthful_variant),
                            'std': np.std(truthful_variant),
                            'median': np.median(truthful_variant)
                        },
                        'hallucinated': {
                            'mean': np.mean(hallucinated_variant),
                            'std': np.std(hallucinated_variant),
                            'median': np.median(hallucinated_variant)
            }
        }
        
        # æ‰§è¡ŒSemantic Driftåˆ†æ
        print("\nğŸŒŠ æ‰§è¡Œè¯­ä¹‰æ¼‚ç§» (Semantic Drift) åˆ†æ...")
        
        # åˆ†æçœŸå®å›ç­”çš„è¯­ä¹‰æ¼‚ç§»
        truthful_drift_data = []
        for sample in truthful_samples[:200]:  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥æé«˜æ€§èƒ½
            drift_result = analyze_semantic_drift(sample)
            truthful_drift_data.append(drift_result)
        
        # åˆ†æå¹»è§‰å›ç­”çš„è¯­ä¹‰æ¼‚ç§»
        hallucinated_drift_data = []
        for sample in hallucinated_samples[:200]:  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥æé«˜æ€§èƒ½
            drift_result = analyze_semantic_drift(sample)
            hallucinated_drift_data.append(drift_result)
        
        # è®¡ç®—è¯­ä¹‰æ¼‚ç§»ç»Ÿè®¡æŒ‡æ ‡
        if truthful_drift_data and hallucinated_drift_data:
            # æå–æ¼‚ç§»æŒ‡æ ‡
            truthful_slopes = [d['drift_slope'] for d in truthful_drift_data]
            halluc_slopes = [d['drift_slope'] for d in hallucinated_drift_data]
            truthful_gaps = [d['drift_gap'] for d in truthful_drift_data]
            halluc_gaps = [d['drift_gap'] for d in hallucinated_drift_data]
            truthful_ratios = [d['early_late_ratio'] for d in truthful_drift_data]
            halluc_ratios = [d['early_late_ratio'] for d in hallucinated_drift_data]
            
            # æ·»åŠ è¯­ä¹‰æ¼‚ç§»ç»Ÿè®¡åˆ°ä¸»ç»Ÿè®¡å­—å…¸
            stats_dict['semantic_drift'] = {
                'truthful': {
                    'drift_slope': {
                        'mean': np.mean(truthful_slopes),
                        'std': np.std(truthful_slopes),
                        'median': np.median(truthful_slopes)
                    },
                    'drift_gap': {
                        'mean': np.mean(truthful_gaps),
                        'std': np.std(truthful_gaps),
                        'median': np.median(truthful_gaps)
                    },
                    'early_late_ratio': {
                        'mean': np.mean(truthful_ratios),
                        'std': np.std(truthful_ratios),
                        'median': np.median(truthful_ratios)
                    }
                },
                'hallucinated': {
                    'drift_slope': {
                        'mean': np.mean(halluc_slopes),
                        'std': np.std(halluc_slopes),
                        'median': np.median(halluc_slopes)
                    },
                    'drift_gap': {
                        'mean': np.mean(halluc_gaps),
                        'std': np.std(halluc_gaps),
                        'median': np.median(halluc_gaps)
                    },
                    'early_late_ratio': {
                        'mean': np.mean(halluc_ratios),
                        'std': np.std(halluc_ratios),
                        'median': np.median(halluc_ratios)
                    }
                },
                'truthful_raw': truthful_drift_data,
                'hallucinated_raw': hallucinated_drift_data
            }
        
        # è¿›è¡Œtæ£€éªŒ
        prd_t_stat, prd_p_value = stats.ttest_ind(truthful_prd, hallucinated_prd)
        gass_t_stat, gass_p_value = stats.ttest_ind(truthful_gass, hallucinated_gass)
        
        stats_dict['statistical_tests'] = {
            'prd_t_test': {
                't_statistic': prd_t_stat,
                'p_value': prd_p_value
            },
            'gass_t_test': {
                't_statistic': gass_t_stat,
                'p_value': gass_p_value
            }
        }
        
        # è®¡ç®—æ•ˆåº”å¤§å° (Cohen's d)
        def cohens_d(group1, group2):
            n1, n2 = len(group1), len(group2)
            pooled_std = np.sqrt(((n1 - 1) * np.var(group1, ddof=1) + (n2 - 1) * np.var(group2, ddof=1)) / (n1 + n2 - 2))
            return (np.mean(group1) - np.mean(group2)) / pooled_std
        
        prd_d = cohens_d(truthful_prd, hallucinated_prd)
        gass_d = cohens_d(truthful_gass, hallucinated_gass)
        
        stats_dict['effect_sizes'] = {
            'prd_cohens_d': prd_d,
            'gass_cohens_d': gass_d
        }
        
        # å¦‚æœæœ‰PRDå˜ä½“ï¼Œè®¡ç®—å˜ä½“çš„ç»Ÿè®¡æ£€éªŒå’Œæ•ˆåº”å¤§å°
        if has_prd_variants:
            stats_dict['statistical_tests']['prd_variants'] = {}
            stats_dict['effect_sizes']['prd_variants'] = {}
            
            for variant in prd_variants_data.keys():
                truthful_variant = prd_variants_data[variant]['truthful']
                hallucinated_variant = prd_variants_data[variant]['hallucinated']
                
                if truthful_variant and hallucinated_variant and len(truthful_variant) > 1 and len(hallucinated_variant) > 1:
                    # tæ£€éªŒ
                    variant_t_stat, variant_p_value = stats.ttest_ind(truthful_variant, hallucinated_variant)
                    stats_dict['statistical_tests']['prd_variants'][variant] = {
                        't_statistic': variant_t_stat,
                        'p_value': variant_p_value
                    }
                    
                    # Cohen's d
                    variant_d = cohens_d(truthful_variant, hallucinated_variant)
                    stats_dict['effect_sizes']['prd_variants'][variant] = variant_d
        
        # æ·»åŠ Semantic Driftç»Ÿè®¡æ£€éªŒå’Œæ•ˆåº”å¤§å°
        if 'semantic_drift' in stats_dict and truthful_drift_data and hallucinated_drift_data:
            # å¯¹æ¯ä¸ªæ¼‚ç§»æŒ‡æ ‡è¿›è¡Œç»Ÿè®¡æ£€éªŒ
            drift_metrics = ['drift_slope', 'drift_gap', 'early_late_ratio']
            stats_dict['statistical_tests']['semantic_drift_tests'] = {}
            stats_dict['effect_sizes']['semantic_drift_effects'] = {}
            
            for metric in drift_metrics:
                truthful_values = [d[metric] for d in truthful_drift_data]
                halluc_values = [d[metric] for d in hallucinated_drift_data]
                
                if len(truthful_values) > 1 and len(halluc_values) > 1:
                    # tæ£€éªŒ
                    t_stat, p_value = stats.ttest_ind(truthful_values, halluc_values)
                    stats_dict['statistical_tests']['semantic_drift_tests'][metric] = {
                        't_statistic': t_stat,
                        'p_value': p_value
                    }
                    
                    # Cohen's d
                    effect_size = cohens_d(truthful_values, halluc_values)
                    stats_dict['effect_sizes']['semantic_drift_effects'][metric] = effect_size
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        analysis_file = os.path.join(output_dir, 'rq1_analysis.json')
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(stats_dict, f, indent=2, ensure_ascii=False, default=float)
        
        # ä¿å­˜è¯­ä¹‰æ¼‚ç§»è¯¦ç»†æ•°æ®
        if 'semantic_drift' in stats_dict and truthful_drift_data and hallucinated_drift_data:
            drift_detailed_file = os.path.join(output_dir, 'rq1_semantic_drift_detailed.json')
            drift_detailed_data = {
                'metadata': {
                    'analysis_type': 'semantic_drift',
                    'timestamp': datetime.now().isoformat(),
                    'segment_size': 5,  # tokens per segment
                    'sample_counts': {
                        'truthful_samples': len(truthful_drift_data),
                        'hallucinated_samples': len(hallucinated_drift_data)
                    }
                },
                'truthful_samples': truthful_drift_data,
                'hallucinated_samples': hallucinated_drift_data,
                'summary_statistics': stats_dict['semantic_drift']
            }
            
            with open(drift_detailed_file, 'w', encoding='utf-8') as f:
                json.dump(drift_detailed_data, f, indent=2, ensure_ascii=False, default=float)
            
            # ç”Ÿæˆè¯­ä¹‰æ¼‚ç§»ä¸“é—¨æŠ¥å‘Š
            drift_report_file = os.path.join(output_dir, 'rq1_semantic_drift_report.txt')
            drift_report = generate_semantic_drift_report(stats_dict['semantic_drift'], truthful_drift_data, hallucinated_drift_data)
            
            with open(drift_report_file, 'w', encoding='utf-8') as f:
                f.write(drift_report)
            
            print(f"ğŸ’¾ è¯­ä¹‰æ¼‚ç§»è¯¦ç»†æ•°æ®ä¿å­˜è‡³: {drift_detailed_file}")
            print(f"ğŸ“„ è¯­ä¹‰æ¼‚ç§»åˆ†ææŠ¥å‘Šä¿å­˜è‡³: {drift_report_file}")
        
        # ç»˜åˆ¶åˆ†ç¦»çš„å›¾è¡¨
        figure_paths = plot_rq1_figures_separate(
            truthful_prd, hallucinated_prd,
            truthful_gass, hallucinated_gass,
            output_dir
        )
        
        # ç»˜åˆ¶Semantic Driftåˆ†æå›¾è¡¨
        drift_figure_paths = None
        if 'semantic_drift' in stats_dict and truthful_drift_data and hallucinated_drift_data:
            print("\nğŸŒŠ ç”Ÿæˆè¯­ä¹‰æ¼‚ç§»åˆ†æå›¾è¡¨...")
            drift_figure_paths = plot_semantic_drift_analysis(
                truthful_drift_data, 
                hallucinated_drift_data, 
                output_dir
            )
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        detailed_report = generate_detailed_report(stats_dict)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = os.path.join(output_dir, 'rq1_detailed_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(detailed_report)
        
        # æ‰“å°ç»“æœ
        print("\n" + "="*80)
        print("RQ1å®éªŒç»“æœ")
        print("="*80)
        print(f"è¯„ä¼°æ–¹æ³•: {evaluation_method}")
        print(f"æ­£ç¡®å›ç­”æ ·æœ¬æ•°: {len(truthful_prd)}")
        print(f"å¹»è§‰å›ç­”æ ·æœ¬æ•°: {len(hallucinated_prd)}")
        print(f"æ€»æ ·æœ¬æ•°: {len(truthful_prd) + len(hallucinated_prd)}")
        print(f"å¹»è§‰ç‡: {len(hallucinated_prd) / (len(truthful_prd) + len(hallucinated_prd)):.2%}")
        print()
        
        print("PRDåˆ†æ•°åˆ†æ:")
        print(f"  æ­£ç¡®å›ç­” - å‡å€¼: {np.mean(truthful_prd):.4f}, æ ‡å‡†å·®: {np.std(truthful_prd):.4f}")
        print(f"  å¹»è§‰å›ç­” - å‡å€¼: {np.mean(hallucinated_prd):.4f}, æ ‡å‡†å·®: {np.std(hallucinated_prd):.4f}")
        print(f"  tæ£€éªŒ på€¼: {prd_p_value:.6f}")
        print(f"  Cohen's d: {prd_d:.4f}")
        print()
        
        print("SASåˆ†æ•°åˆ†æ:")
        print(f"  æ­£ç¡®å›ç­” - å‡å€¼: {np.mean(truthful_gass):.4f}, æ ‡å‡†å·®: {np.std(truthful_gass):.4f}")
        print(f"  å¹»è§‰å›ç­” - å‡å€¼: {np.mean(hallucinated_gass):.4f}, æ ‡å‡†å·®: {np.std(hallucinated_gass):.4f}")
        print(f"  tæ£€éªŒ på€¼: {gass_p_value:.6f}")
        print(f"  Cohen's d: {gass_d:.4f}")
        print()
        
        print("æ–‡ä»¶ä¿å­˜ä½ç½®:")
        print(f"  ç»Ÿè®¡åˆ†æ: {analysis_file}")
        print(f"  è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print(f"  åˆ†å¸ƒå›¾: {figure_paths['distributions']['png']} (PNG, 600 DPI)")
        print(f"          {figure_paths['distributions']['pdf']} (PDF)")
        print(f"  ç®±çº¿å›¾: {figure_paths['boxplots']['png']} (PNG, 600 DPI)")
        print(f"          {figure_paths['boxplots']['pdf']} (PDF)")
        print(f"  ç‰¹å¾åˆ†æå›¾: {figure_paths['analysis']['png']} (PNG, 600 DPI)")
        print(f"              {figure_paths['analysis']['pdf']} (PDF)")
        
        # æ˜¾ç¤ºè¯­ä¹‰æ¼‚ç§»æ–‡ä»¶ä¿¡æ¯
        if 'semantic_drift' in stats_dict:
            drift_detailed_file = os.path.join(output_dir, 'rq1_semantic_drift_detailed.json')
            drift_report_file = os.path.join(output_dir, 'rq1_semantic_drift_report.txt')
            print(f"  è¯­ä¹‰æ¼‚ç§»è¯¦ç»†æ•°æ®: {drift_detailed_file}")
            print(f"  è¯­ä¹‰æ¼‚ç§»åˆ†ææŠ¥å‘Š: {drift_report_file}")
        
        # æ˜¾ç¤ºSemantic Driftåˆ†æç»“æœ
        if 'semantic_drift' in stats_dict:
            print()
            print("ğŸŒŠ è¯­ä¹‰æ¼‚ç§» (Semantic Drift) åˆ†æ:")
            drift_data = stats_dict['semantic_drift']
            print(f"  æ¼‚ç§»æ–œç‡ - æ­£ç¡®å›ç­”å‡å€¼: {drift_data['truthful']['drift_slope']['mean']:.6f}")
            print(f"             å¹»è§‰å›ç­”å‡å€¼: {drift_data['hallucinated']['drift_slope']['mean']:.6f}")
            print(f"  å‰åæ®µå·®å¼‚ - æ­£ç¡®å›ç­”å‡å€¼: {drift_data['truthful']['drift_gap']['mean']:.4f}")
            print(f"               å¹»è§‰å›ç­”å‡å€¼: {drift_data['hallucinated']['drift_gap']['mean']:.4f}")
            
            if drift_figure_paths:
                print(f"  è¯­ä¹‰æ¼‚ç§»å›¾: {drift_figure_paths['png']} (PNG, 600 DPI)")
                print(f"              {drift_figure_paths['pdf']} (PDF)")
        
        logger.info("RQ1 experiment completed successfully")
        
    except Exception as e:
        logger.error(f"Error in RQ1 experiment: {e}")
        raise

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    class Args:
        def __init__(self, results_file):
            self.results_file = results_file
    
    # ç¤ºä¾‹ç”¨æ³•
    import sys
    if len(sys.argv) > 1:
        args = Args(sys.argv[1])
        run_rq1_experiment(args)
    else:
        print("ç”¨æ³•: python run_rq1.py <results_file>")
        print("ç¤ºä¾‹: python run_rq1.py experiment_records/inference_results/llama2-7b/colab_dev_simple.jsonl")