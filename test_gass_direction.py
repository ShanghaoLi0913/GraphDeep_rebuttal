#!/usr/bin/env python3
"""
æµ‹è¯•Llama3ä¸åŒå±‚çš„GASSæ–¹å‘æ€§
å…³é”®ï¼šæ‰¾åˆ°å“ªä¸ªå±‚èƒ½å¤Ÿè®© truthful_gass > hallucinated_gass
"""

import torch
import json
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from gass_metrics import calculate_gass
from dataset_processor import load_entities_and_relations
import numpy as np
from tqdm import tqdm
import warnings
import random
from datetime import datetime
import argparse

warnings.filterwarnings("ignore")

def setup_model():
    """åŠ è½½Llama3-8Bæ¨¡å‹"""
    print("ğŸ¤– Loading Llama3-8B model...")
    
    model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        use_cache=False,
        token=True
    )
    
    return model, tokenizer

def load_test_data(num_samples=50):
    """åŠ è½½æµ‹è¯•æ•°æ®æ ·æœ¬"""
    print(f"ğŸ“ Loading {num_samples} test samples...")
    
    test_file = 'experiment_records/trimming_results/metaqa-1hop/test_simple_trimming_results_with_ges.jsonl'
    
    samples = []
    with open(test_file, 'r', encoding='utf-8') as f:
        # è·³è¿‡é…ç½®è¡Œ
        config_line = f.readline()
        
        for i, line in enumerate(f):
            if i >= num_samples:
                break
            try:
                sample = json.loads(line.strip())
                samples.append(sample)
            except:
                continue
    
    print(f"âœ… Loaded {len(samples)} samples")
    return samples

def generate_answer(question, trimmed_triples, model, tokenizer):
    """æ­£å¸¸ç”Ÿæˆç­”æ¡ˆï¼Œä½¿ç”¨ä¸main_colab.pyç›¸åŒçš„promptæ ¼å¼"""
    # æ„å»ºKnowledgeéƒ¨åˆ†
    triples_text = "Knowledge:\n"
    for h, r, t in trimmed_triples:
        triples_text += f"{h} {r} {t}\n"
    
    # æ„å»ºç”¨æˆ·è¾“å…¥ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    user_content = f"{triples_text}\nQuestion: {question}\nAnswer:"
    
    # ç³»ç»Ÿæç¤º
    system_prompt = """Based on the triples retrieved from a knowledge graph, please answer the question. Please return ONLY the answer entities as a list, each prefixed with "ans:". Do not include explanations or reasoning."""
    
    # æ„å»ºå®Œæ•´çš„æç¤ºï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]
    
    # åº”ç”¨èŠå¤©æ¨¡æ¿
    formatted_input = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer(formatted_input, return_tensors="pt", add_special_tokens=False)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ä¸main_colab.pyç›¸åŒçš„å‚æ•°ï¼‰
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,  # ä¸main_colab.pyä¸€è‡´
            do_sample=False,    # è´ªå©ªè§£ç ï¼Œç¡®ä¿ç»“æœä¸€è‡´
            pad_token_id=tokenizer.eos_token_id
        )
    
    # æå–ç”Ÿæˆçš„ç­”æ¡ˆéƒ¨åˆ†
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå–ç”Ÿæˆçš„ç­”æ¡ˆéƒ¨åˆ†ï¼ˆç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
    input_length = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))
    answer_part = generated_text[input_length:].strip()
    
    # æ¸…ç†ç­”æ¡ˆæ ¼å¼
    if answer_part.startswith("assistant"):
        answer_part = answer_part[len("assistant"):].strip()
    
    # ç§»é™¤å¯èƒ½çš„æ¢è¡Œç¬¦
    answer_part = answer_part.replace('\n', ' ').strip()
    
    return answer_part if len(answer_part) > 0 else None

def generate_once_for_all_layers(question, trimmed_triples, model, tokenizer):
    """è¿›è¡Œä¸€æ¬¡generationï¼Œè¿”å›æ‰€æœ‰layerè®¡ç®—éœ€è¦çš„ä¿¡æ¯"""
    
    # ç³»ç»Ÿæç¤ºï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    system_prompt = """Based on the triples retrieved from a knowledge graph, please answer the question. Please return ONLY the answer entities as a list, each prefixed with "ans:". Do not include explanations or reasoning."""
    
    # æ„å»ºKnowledgeéƒ¨åˆ†ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    triples_text = "Knowledge:\n"
    for h, r, t in trimmed_triples:
        triples_text += f"{h} {r} {t}\n"
    
    # æ„å»ºç”¨æˆ·è¾“å…¥ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    user_content = f"{triples_text}\nQuestion: {question}\nAnswer:"
    
    # æ„å»ºå®Œæ•´çš„æç¤ºï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]
    
    # å‡†å¤‡è¾“å…¥ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # å®é™…ç”Ÿæˆï¼ˆä¸main_colab.pyå®Œå…¨ä¸€è‡´ï¼‰
    with torch.no_grad(), torch.autocast(device_type="cuda", dtype=torch.float16):
        try:
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                num_return_sequences=1,
                do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç 
                temperature=1.0,
                use_cache=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                output_attentions=False  # ç”Ÿæˆæ—¶ä¸éœ€è¦attention
            )
        except Exception as gen_error:
            print(f"âš ï¸ Generation failed: {gen_error}")
            return None, None, None, None
    
    # æ‰¾åˆ°ç”Ÿæˆæ–‡æœ¬çš„å¼€å§‹ä½ç½®ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    input_length = len(inputs['input_ids'][0])
    if len(outputs.shape) > 1:
        generated_ids = outputs[0][input_length:]
        full_output_ids = outputs[0]  # å®Œæ•´çš„è¾“å‡ºåŒ…æ‹¬è¾“å…¥éƒ¨åˆ†
    else:
        generated_ids = outputs[input_length:]
        full_output_ids = outputs
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    # è®¡ç®—ç­”æ¡ˆä½ç½®ï¼ˆä¸main_colab.pyå®Œå…¨ä¸€è‡´çš„é€»è¾‘ï¼‰
    raw_answer_start = input_length
    raw_answer_end = len(full_output_ids)
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬æ¥æ‰¾çœŸæ­£çš„ç­”æ¡ˆä½ç½®
    if raw_answer_end > raw_answer_start:
        generated_text = tokenizer.decode(full_output_ids[raw_answer_start:raw_answer_end], skip_special_tokens=True)
        
        # å°è¯•æ‰¾åˆ°'ans:'åé¢çš„å†…å®¹
        if 'ans:' in generated_text.lower():
            # åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­æ‰¾åˆ°ans:çš„ä½ç½®
            ans_pos = generated_text.lower().find('ans:')
            if ans_pos >= 0:
                # è®¡ç®—ans:åé¢çš„tokenä½ç½®
                ans_part = generated_text[ans_pos + 4:].strip()  # ans:åé¢çš„å†…å®¹
                if ans_part:
                    # é‡æ–°tokenizeæ¥æ‰¾å‡†ç¡®ä½ç½®
                    ans_prefix = generated_text[:ans_pos + 4]  # åŒ…å«'ans:'
                    ans_prefix_tokens = tokenizer(ans_prefix, add_special_tokens=False)['input_ids']
                    answer_start_idx = raw_answer_start + len(ans_prefix_tokens)
                else:
                    answer_start_idx = raw_answer_start
                answer_end_idx = raw_answer_end
            else:
                answer_start_idx = raw_answer_start
                answer_end_idx = raw_answer_end
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ans:ï¼Œä½¿ç”¨æ•´ä¸ªç”Ÿæˆçš„éƒ¨åˆ†
            answer_start_idx = raw_answer_start
            answer_end_idx = raw_answer_end
    else:
        answer_start_idx = raw_answer_start
        answer_end_idx = raw_answer_end
    
    # ç¡®ä¿ç­”æ¡ˆèŒƒå›´æœ‰æ•ˆï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    if answer_end_idx > len(full_output_ids):
        answer_end_idx = len(full_output_ids)
    if answer_start_idx >= answer_end_idx:
        answer_start_idx = raw_answer_start
        answer_end_idx = raw_answer_end
    
    if answer_start_idx >= answer_end_idx:
        return None, None, None, None
    
    return full_output_ids, output_text, answer_start_idx, answer_end_idx

def calculate_gass_for_layer(full_output_ids, trimmed_triples, gold_knowledge, 
                           answer_start_idx, answer_end_idx, model, tokenizer, layer):
    """ä½¿ç”¨å·²æœ‰çš„generationç»“æœè®¡ç®—ç‰¹å®šlayerçš„GASS"""
    
    try:
        # ä¸ºäº†å‡†ç¡®å¤ç°ä¹‹å‰çš„è¡Œä¸ºï¼Œæ ¹æ®layeré€‰æ‹©è°ƒç”¨æ–¹å¼
        if layer == 32:  
            # Layer 32: å¤ç°ä¹‹å‰main_colab.pyåœ¨Llama3ä¸Šçš„è¡Œä¸ºï¼ˆæ˜ç¡®ä½¿ç”¨layer_indices=[-1]ï¼‰
            gass_score = calculate_gass(
                model=model,
                tokenizer=tokenizer,
                input_ids=full_output_ids,
                retrieved_subgraph=trimmed_triples,
                gold_subgraph=gold_knowledge,
                answer_start_idx=answer_start_idx,
                answer_end_idx=answer_end_idx,
                debug=False,
                layer_indices=[-1],  # æ˜ç¡®æŒ‡å®šæœ€åä¸€å±‚
                model_type="llama3"
            )
        else:  
            # å…¶ä»–å±‚: æ˜ç¡®æŒ‡å®šlayerå’Œmodel_type
            gass_score = calculate_gass(
                model=model,
                tokenizer=tokenizer,
                input_ids=full_output_ids,
                retrieved_subgraph=trimmed_triples,
                gold_subgraph=gold_knowledge,
                answer_start_idx=answer_start_idx,
                answer_end_idx=answer_end_idx,
                debug=False,
                layer_indices=[layer],
                model_type="llama3"
            )
        return gass_score
    except Exception as e:
        print(f"âŒ Error calculating GASS for layer {layer}: {e}")
        return None

def calculate_gass_for_generated_answer(question, trimmed_triples, gold_knowledge, model, tokenizer, layer):
    """è®¡ç®—å®é™…generationçš„GASSåˆ†æ•°ï¼Œå®Œå…¨å¤ç°main_colab.pyçš„æ–¹å¼"""
    
    # ç³»ç»Ÿæç¤ºï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    system_prompt = """Based on the triples retrieved from a knowledge graph, please answer the question. Please return ONLY the answer entities as a list, each prefixed with "ans:". Do not include explanations or reasoning."""
    
    # æ„å»ºKnowledgeéƒ¨åˆ†ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    triples_text = "Knowledge:\n"
    for h, r, t in trimmed_triples:
        triples_text += f"{h} {r} {t}\n"
    
    # æ„å»ºç”¨æˆ·è¾“å…¥ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    user_content = f"{triples_text}\nQuestion: {question}\nAnswer:"
    
    # æ„å»ºå®Œæ•´çš„æç¤ºï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]
    
    # å‡†å¤‡è¾“å…¥ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # å®é™…ç”Ÿæˆï¼ˆä¸main_colab.pyå®Œå…¨ä¸€è‡´ï¼‰
    with torch.no_grad(), torch.autocast(device_type="cuda", dtype=torch.float16):
        try:
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                num_return_sequences=1,
                do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç 
                temperature=1.0,
                use_cache=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                output_attentions=False  # ç”Ÿæˆæ—¶ä¸éœ€è¦attention
            )
        except Exception as gen_error:
            print(f"âš ï¸ Generation failed: {gen_error}")
            return None, None
    
    # æ‰¾åˆ°ç”Ÿæˆæ–‡æœ¬çš„å¼€å§‹ä½ç½®ï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    input_length = len(inputs['input_ids'][0])
    if len(outputs.shape) > 1:
        generated_ids = outputs[0][input_length:]
        full_output_ids = outputs[0]  # å®Œæ•´çš„è¾“å‡ºåŒ…æ‹¬è¾“å…¥éƒ¨åˆ†
    else:
        generated_ids = outputs[input_length:]
        full_output_ids = outputs
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    output_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    # è®¡ç®—ç­”æ¡ˆä½ç½®ï¼ˆä¸main_colab.pyå®Œå…¨ä¸€è‡´çš„é€»è¾‘ï¼‰
    raw_answer_start = input_length
    raw_answer_end = len(full_output_ids)
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬æ¥æ‰¾çœŸæ­£çš„ç­”æ¡ˆä½ç½®
    if raw_answer_end > raw_answer_start:
        generated_text = tokenizer.decode(full_output_ids[raw_answer_start:raw_answer_end], skip_special_tokens=True)
        
        # å°è¯•æ‰¾åˆ°'ans:'åé¢çš„å†…å®¹
        if 'ans:' in generated_text.lower():
            # åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­æ‰¾åˆ°ans:çš„ä½ç½®
            ans_pos = generated_text.lower().find('ans:')
            if ans_pos >= 0:
                # è®¡ç®—ans:åé¢çš„tokenä½ç½®
                ans_part = generated_text[ans_pos + 4:].strip()  # ans:åé¢çš„å†…å®¹
                if ans_part:
                    # é‡æ–°tokenizeæ¥æ‰¾å‡†ç¡®ä½ç½®
                    ans_prefix = generated_text[:ans_pos + 4]  # åŒ…å«'ans:'
                    ans_prefix_tokens = tokenizer(ans_prefix, add_special_tokens=False)['input_ids']
                    answer_start_idx = raw_answer_start + len(ans_prefix_tokens)
                else:
                    answer_start_idx = raw_answer_start
                answer_end_idx = raw_answer_end
            else:
                answer_start_idx = raw_answer_start
                answer_end_idx = raw_answer_end
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ans:ï¼Œä½¿ç”¨æ•´ä¸ªç”Ÿæˆçš„éƒ¨åˆ†
            answer_start_idx = raw_answer_start
            answer_end_idx = raw_answer_end
    else:
        answer_start_idx = raw_answer_start
        answer_end_idx = raw_answer_end
    
    # ç¡®ä¿ç­”æ¡ˆèŒƒå›´æœ‰æ•ˆï¼ˆä¸main_colab.pyä¸€è‡´ï¼‰
    if answer_end_idx > len(full_output_ids):
        answer_end_idx = len(full_output_ids)
    if answer_start_idx >= answer_end_idx:
        answer_start_idx = raw_answer_start
        answer_end_idx = raw_answer_end
    
    if answer_start_idx >= answer_end_idx:
        return None, None
    
    try:
        # ä¸ºäº†å‡†ç¡®å¤ç°ä¹‹å‰çš„è¡Œä¸ºï¼Œæ‰€æœ‰å±‚éƒ½æ˜ç¡®æŒ‡å®šlayer_indices
        if layer == 32:  
            # Layer 32: å¤ç°ä¹‹å‰main_colab.pyåœ¨Llama3ä¸Šçš„è¡Œä¸ºï¼ˆæ˜ç¡®ä½¿ç”¨layer_indices=[-1]ï¼‰
            gass_score = calculate_gass(
                model=model,
                tokenizer=tokenizer,
                input_ids=full_output_ids,
                retrieved_subgraph=trimmed_triples,
                gold_subgraph=gold_knowledge,
                answer_start_idx=answer_start_idx,
                answer_end_idx=answer_end_idx,
                debug=False,
                layer_indices=[-1],  # æ˜ç¡®æŒ‡å®šæœ€åä¸€å±‚
                model_type="llama3"
            )
        else:  
            # å…¶ä»–å±‚: æ˜ç¡®æŒ‡å®šlayerå’Œmodel_type
            gass_score = calculate_gass(
                model=model,
                tokenizer=tokenizer,
                input_ids=full_output_ids,
                retrieved_subgraph=trimmed_triples,
                gold_subgraph=gold_knowledge,
                answer_start_idx=answer_start_idx,
                answer_end_idx=answer_end_idx,
                debug=False,
                layer_indices=[layer],
                model_type="llama3"
            )
        return gass_score, output_text
    except Exception as e:
        print(f"âŒ Error calculating GASS: {e}")
        return None, output_text

def test_gass_layers(model, tokenizer, samples, candidate_layers=[32, 31, 30, 28, 25, 20], output_file=None):
    """æµ‹è¯•ä¸åŒå±‚çš„GASSåˆ†æ•°ï¼Œè¾¹è·‘è¾¹ä¿å­˜"""
    print(f"ğŸ§ª Testing GASS for layers: {candidate_layers}")
    
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"llama3_layer_gass_results_{timestamp}.jsonl"
    
    processed_count = 0
    
    with open(output_file, 'w', encoding='utf-8') as f:
        # å†™å…¥é…ç½®ä¿¡æ¯ï¼ˆæ¨¡ä»¿main_colab.pyï¼‰
        config = {
            'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
            'script': 'test_gass_direction.py',
            'model': 'meta-llama/Meta-Llama-3-8B-Instruct',
            'total_samples': len(samples),
            'candidate_layers': candidate_layers,
            'pytorch_version': torch.__version__,
            'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A'
        }
        f.write(json.dumps({'config': config}, ensure_ascii=False) + '\n')
        f.flush()
        
        for i, sample in enumerate(tqdm(samples, desc="Processing samples")):
            try:
                question = sample.get('question', '')
                gold_answers = sample.get('golden_texts', [])
                gold_expansion_set = sample.get('gold_expansion_set', [])
                trimmed_triples = sample.get('trimmed_triples', [])
                
                if not gold_answers or not gold_expansion_set or not trimmed_triples:
                    continue
                
                # å¤„ç†é‡‘æ ‡å‡†çŸ¥è¯†ï¼ˆä½¿ç”¨gold_expansion_setï¼‰
                gold_knowledge = []
                for triple in gold_expansion_set:
                    if isinstance(triple, list) and len(triple) == 3:
                        gold_knowledge.append(triple)
                
                if not gold_knowledge:
                    continue
                
                print(f"\næ ·æœ¬ {i}:")
                print(f"  é—®é¢˜: {question}")
                print(f"  é‡‘æ ‡å‡†: {gold_answers}")
                
                # è¿›è¡Œä¸€æ¬¡å®é™…generationï¼Œè·å¾—å®Œæ•´çš„generationç»“æœ
                full_output_ids, generated_answer, answer_start_idx, answer_end_idx = generate_once_for_all_layers(
                    question, trimmed_triples, model, tokenizer
                )
                
                if generated_answer is None:
                    print(f"  âŒ Generation failed, skipping sample")
                    continue
                
                print(f"  ç”Ÿæˆç­”æ¡ˆ: {generated_answer}")
                
                # åˆå§‹åŒ–ç»“æœ
                sample_result = {
                    'sample_id': i,
                    'question': question,
                    'generated_answer': generated_answer,
                    'golden_answers': gold_answers,
                    'gold_expansion_set': gold_expansion_set,
                    'layer_gass': {}
                }
                
                # ä½¿ç”¨ç›¸åŒçš„generationç»“æœè®¡ç®—æ‰€æœ‰å±‚çš„GASSåˆ†æ•°
                for layer in candidate_layers:
                    gass_score = calculate_gass_for_layer(
                        full_output_ids, trimmed_triples, gold_knowledge, 
                        answer_start_idx, answer_end_idx, model, tokenizer, layer
                    )
                    
                    if gass_score is not None:
                        sample_result['layer_gass'][layer] = gass_score
                        print(f"  Layer {layer}: GASS={gass_score:.4f}")
                    else:
                        sample_result['layer_gass'][layer] = None
                        print(f"  Layer {layer}: GASS=ERROR")
                
                # ç«‹å³å†™å…¥æ–‡ä»¶ï¼ˆæ¨¡ä»¿main_colab.pyï¼‰
                f.write(json.dumps(sample_result, ensure_ascii=False) + '\n')
                f.flush()  # ç¡®ä¿åŠæ—¶å†™å…¥
                processed_count += 1
                
                # æ¯10ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if processed_count % 10 == 0:
                    print(f"âœ… Processed {processed_count} samples, saved to {output_file}")
            
            except Exception as e:
                print(f"âŒ Error processing sample {i}: {e}")
                continue
    
    print(f"\nğŸ‰ Total processed: {processed_count} samples")
    print(f"ğŸ’¾ Results saved to: {output_file}")
    return output_file, processed_count

def analyze_layer_results(results):
    """åˆ†æä¸åŒå±‚çš„GASSç»“æœ"""
    print("\nğŸ“Š Layer GASS Analysis:")
    print("=" * 60)
    
    # æ”¶é›†å„å±‚æ•°æ®
    layer_data = {}
    layers = set()
    
    for sample in results:
        for layer, gass in sample['layer_gass'].items():
            if layer not in layer_data:
                layer_data[layer] = []
            layer_data[layer].append(gass)
            layers.add(layer)
    
    # è¾“å‡ºç»Ÿè®¡
    print(f"{'Layer':<6} {'Samples':<8} {'Mean':<8} {'Std':<8} {'Min':<8} {'Max':<8}")
    print("=" * 60)
    
    for layer in sorted(layers, reverse=True):
        if layer in layer_data and layer_data[layer]:
            scores = layer_data[layer]
            mean_score = np.mean(scores)
            std_score = np.std(scores)
            min_score = np.min(scores)
            max_score = np.max(scores)
            
            print(f"{layer:<6} {len(scores):<8} {mean_score:<8.4f} {std_score:<8.4f} "
                  f"{min_score:<8.4f} {max_score:<8.4f}")
    
    return layer_data

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='Llama3 Layer GASS Test')
    parser.add_argument('-n', '--num_samples', type=int, default=50, 
                        help='Number of samples to process (default: 50)')
    parser.add_argument('--layers', nargs='+', type=int, default=[32, 28, 24, 20, 16, 12, 8, 4],
                        help='Layers to test (default: coarse grid 32 28 24 20 16 12 8 4)')
    parser.add_argument('-o', '--output', type=str, default=None,
                        help='Output file name (default: auto-generated)')
    
    args = parser.parse_args()
    
    print("ğŸ”¬ Llama3 Layer GASS Test - ç²—ç²’åº¦æ …æ ¼æ‰«æ")
    print("ç›®æ ‡ï¼šå¯»æ‰¾GASSæ–¹å‘æ‹ç‚¹ (truthful > hallucinated)")
    print("=" * 60)
    print(f"ğŸ“Š Processing {args.num_samples} samples")
    print(f"ğŸ”¢ Coarse grid layers: {args.layers}")
    print("ç­–ç•¥ï¼šæ¯éš”4-5å±‚é‡‡æ ·ï¼Œå¿«é€Ÿé”å®šåè½¬ç‚¹ä½ç½®")
    
    # å€™é€‰å±‚
    candidate_layers = args.layers
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = setup_model()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    samples = load_test_data(num_samples=args.num_samples)
    
    # æµ‹è¯•ä¸åŒå±‚ï¼ˆè¾¹è·‘è¾¹ä¿å­˜ï¼‰
    output_file, processed_count = test_gass_layers(model, tokenizer, samples, candidate_layers, args.output)
    
    print(f"\nâœ… Layer analysis complete!")
    print(f"ğŸ“ Output file: {output_file}")
    print(f"ğŸ“Š Processed samples: {processed_count}")
    print("\nğŸ’¡ Tip: å¯ä»¥éšæ—¶ä¸­æ–­å¹¶æŸ¥çœ‹å·²ä¿å­˜çš„ç»“æœï¼")

if __name__ == "__main__":
    main()