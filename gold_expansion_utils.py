#!/usr/bin/env python3
"""
Gold Expansion Set å·¥å…·æ¨¡å—

ä¸ºFGASè®¡ç®—æ„å»ºè¯­ä¹‰ä¸°å¯Œçš„ä¸‰å…ƒç»„æ‰©å±•é›†åˆï¼Œè§£å†³åŸå§‹gold triples
è¯­ä¹‰ä¿¡å·ç¨€ç–çš„é—®é¢˜ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
- TUSä½¿ç”¨åŸå§‹gold triplesï¼ˆæ³¨æ„åŠ›èšç„¦ï¼‰
- FGASä½¿ç”¨gold_expansion_setï¼ˆè¯­ä¹‰è¡¨è¾¾å¯¹é½ï¼‰

ä½œè€…: GraphDeEP Team
åˆ›å»ºæ—¥æœŸ: 2024-12-27
"""

from typing import List, Dict, Set, Tuple, Any
import json

# å®šä¹‰è¯­ä¹‰å¼ºçš„å…³ç³»ç±»å‹
SEMANTIC_RICH_RELATIONS = {
    'directed_by', 'acted_in', 'genre', 'release_year', 'language',
    'produced_by', 'written_by', 'cinematography', 'music_by',
    'starring', 'has_genre', 'release_date', 'runtime', 'budget',
    'award', 'nomination', 'country', 'company', 'sequel_to',
    'prequel_to', 'based_on', 'remake_of', 'character', 'role'
}

def extract_core_entities(gold_triples: List[List[str]]) -> Set[str]:
    """
    ä»gold triplesä¸­æå–æ ¸å¿ƒå®ä½“
    
    Args:
        gold_triples: åŸå§‹goldä¸‰å…ƒç»„åˆ—è¡¨ [[h, r, t], ...]
        
    Returns:
        æ ¸å¿ƒå®ä½“é›†åˆ
    """
    core_entities = set()
    for h, r, t in gold_triples:
        core_entities.add(h)
        core_entities.add(t)
    return core_entities

def is_semantic_relation(relation: str) -> bool:
    """
    åˆ¤æ–­å…³ç³»æ˜¯å¦å…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰è¡¨è¾¾
    
    Args:
        relation: å…³ç³»åç§°
        
    Returns:
        æ˜¯å¦ä¸ºè¯­ä¹‰ä¸°å¯Œçš„å…³ç³»
    """
    relation_lower = relation.lower()
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯­ä¹‰ä¸°å¯Œçš„å…³é”®è¯
    for semantic_rel in SEMANTIC_RICH_RELATIONS:
        if semantic_rel in relation_lower:
            return True
    return False

def find_neighbor_triples(core_entities: Set[str], 
                         all_triples: List[List[str]], 
                         gold_triples: List[List[str]],
                         max_per_entity: int = 3) -> List[List[str]]:
    """
    æŸ¥æ‰¾ä¸æ ¸å¿ƒå®ä½“ç›¸å…³çš„é‚»å±…ä¸‰å…ƒç»„
    
    Args:
        core_entities: æ ¸å¿ƒå®ä½“é›†åˆ
        all_triples: å®Œæ•´çš„ä¸‰å…ƒç»„åˆ—è¡¨ï¼ˆæ¥è‡ªtrimmed subgraphï¼‰
        gold_triples: åŸå§‹goldä¸‰å…ƒç»„ï¼ˆéœ€è¦æ’é™¤ï¼‰
        max_per_entity: æ¯ä¸ªå®ä½“æœ€å¤šæ‰©å±•çš„ä¸‰å…ƒç»„æ•°
        
    Returns:
        é‚»å±…ä¸‰å…ƒç»„åˆ—è¡¨
    """
    # å°†gold triplesè½¬æ¢ä¸ºé›†åˆä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾
    gold_set = set(tuple(triple) for triple in gold_triples)
    
    # ä¸ºæ¯ä¸ªæ ¸å¿ƒå®ä½“æ”¶é›†ç›¸å…³ä¸‰å…ƒç»„
    entity_triples = {entity: [] for entity in core_entities}
    
    for triple in all_triples:
        h, r, t = triple
        triple_tuple = tuple(triple)
        
        # è·³è¿‡å·²åœ¨gold triplesä¸­çš„
        if triple_tuple in gold_set:
            continue
            
        # åªä¿ç•™è¯­ä¹‰ä¸°å¯Œçš„å…³ç³»
        if not is_semantic_relation(r):
            continue
            
        # å¦‚æœå¤´å®ä½“æˆ–å°¾å®ä½“åœ¨æ ¸å¿ƒé›†åˆä¸­ï¼Œæ·»åŠ åˆ°å¯¹åº”å®ä½“çš„åˆ—è¡¨
        if h in core_entities:
            entity_triples[h].append(triple)
        if t in core_entities:
            entity_triples[t].append(triple)
    
    # ä¸ºæ¯ä¸ªå®ä½“é™åˆ¶æ‰©å±•æ•°é‡
    neighbor_triples = []
    for entity, triples in entity_triples.items():
        # å¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ’åºé€»è¾‘ï¼Œæ¯”å¦‚æ ¹æ®å…³ç³»é‡è¦æ€§
        selected = triples[:max_per_entity]
        neighbor_triples.extend(selected)
    
    # å»é‡
    unique_neighbors = []
    seen = set()
    for triple in neighbor_triples:
        triple_tuple = tuple(triple)
        if triple_tuple not in seen:
            seen.add(triple_tuple)
            unique_neighbors.append(triple)
    
    return unique_neighbors

def create_gold_expansion_set(gold_triples: List[List[str]], 
                            trimmed_triples: List[List[str]],
                            max_expansion_size: int = 20,
                            max_per_entity: int = 3) -> List[List[str]]:
    """
    åˆ›å»ºgold_expansion_set
    
    Args:
        gold_triples: åŸå§‹goldä¸‰å…ƒç»„
        trimmed_triples: å®Œæ•´çš„trimmedå­å›¾ä¸‰å…ƒç»„
        max_expansion_size: gold_expansion_setçš„æœ€å¤§å¤§å°
        max_per_entity: æ¯ä¸ªæ ¸å¿ƒå®ä½“æœ€å¤šæ‰©å±•çš„ä¸‰å…ƒç»„æ•°
        
    Returns:
        gold_expansion_setä¸‰å…ƒç»„åˆ—è¡¨
    """
    # Step 1: æå–æ ¸å¿ƒå®ä½“
    core_entities = extract_core_entities(gold_triples)
    
    # Step 2: æŸ¥æ‰¾é‚»å±…ä¸‰å…ƒç»„
    neighbor_triples = find_neighbor_triples(
        core_entities, trimmed_triples, gold_triples, max_per_entity
    )
    
    # Step 3: æ„å»ºgold_expansion_setï¼ˆåŒ…å«åŸå§‹gold triples + æ‰©å±•ä¸‰å…ƒç»„ï¼‰
    gold_expansion_set = gold_triples.copy()
    gold_expansion_set.extend(neighbor_triples)
    
    # Step 4: é™åˆ¶æ€»å¤§å°
    if len(gold_expansion_set) > max_expansion_size:
        # ä¿ç•™æ‰€æœ‰gold triplesï¼Œç„¶åæˆªå–æ‰©å±•éƒ¨åˆ†
        remaining_slots = max_expansion_size - len(gold_triples)
        if remaining_slots > 0:
            gold_expansion_set = gold_triples + neighbor_triples[:remaining_slots]
        else:
            gold_expansion_set = gold_triples[:max_expansion_size]
    
    return gold_expansion_set

def analyze_expansion_quality(gold_triples: List[List[str]], 
                            gold_expansion_set: List[List[str]]) -> Dict[str, Any]:
    """
    åˆ†ægold_expansion_setçš„è´¨é‡æŒ‡æ ‡ï¼ˆç²¾ç®€ç‰ˆï¼‰
    
    Args:
        gold_triples: åŸå§‹goldä¸‰å…ƒç»„
        gold_expansion_set: gold_expansion_setä¸‰å…ƒç»„åˆ—è¡¨
        
    Returns:
        è´¨é‡åˆ†æç»“æœï¼ˆ4ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼‰
    """
    # ç»Ÿè®¡å…³ç³»ç±»å‹
    gold_relations = [triple[1] for triple in gold_triples]
    ges_relations = [triple[1] for triple in gold_expansion_set]
    
    # ç»Ÿè®¡è¯­ä¹‰ä¸°å¯Œå…³ç³»çš„æ¯”ä¾‹
    semantic_in_gold = sum(1 for r in gold_relations if is_semantic_relation(r))
    semantic_in_ges = sum(1 for r in ges_relations if is_semantic_relation(r))
    
    # è¿”å›ç²¾ç®€ç‰ˆæŒ‡æ ‡
    return {
        'original_size': len(gold_triples),
        'ges_size': len(gold_expansion_set),
        'expansion_ratio': len(gold_expansion_set) / len(gold_triples) if gold_triples else 0,
        'semantic_improvement': (semantic_in_ges / len(gold_expansion_set)) - (semantic_in_gold / len(gold_triples)) if gold_triples and gold_expansion_set else 0
    }

# åˆ«åå‡½æ•°
def analyze_ges_quality(gold_triples: List[List[str]], 
                       gold_expansion_set: List[List[str]]) -> Dict[str, Any]:
    """
    analyze_expansion_quality çš„åˆ«åï¼Œä¿æŒå‘åå…¼å®¹
    """
    return analyze_expansion_quality(gold_triples, gold_expansion_set)

def process_sample_with_ges(sample: Dict[str, Any], 
                           max_expansion_size: int = 20,
                           max_per_entity: int = 3) -> Dict[str, Any]:
    """
    ä¸ºå•ä¸ªæ ·æœ¬åˆ›å»ºGESå¹¶æ›´æ–°æ ·æœ¬æ•°æ®
    
    Args:
        sample: trimmingç»“æœä¸­çš„æ ·æœ¬
        max_expansion_size: GESæœ€å¤§å¤§å°
        max_per_entity: æ¯ä¸ªå®ä½“æœ€å¤§æ‰©å±•æ•°
        
    Returns:
        æ›´æ–°åçš„æ ·æœ¬ï¼ŒåŒ…å«GESå­—æ®µ
    """
    gold_triples = sample.get('gold_triples', [])
    trimmed_triples = sample.get('trimmed_triples', [])
    
    # åˆ›å»ºGES
    ges = create_gold_expansion_set(
        gold_triples, trimmed_triples, 
        max_expansion_size, max_per_entity
    )
    
    # åˆ†æè´¨é‡
    quality_metrics = analyze_ges_quality(gold_triples, ges)
    
    # æ›´æ–°æ ·æœ¬
    updated_sample = sample.copy()
    updated_sample['gold_expansion_set'] = ges
    updated_sample['ges_quality_metrics'] = quality_metrics
    
    return updated_sample

def batch_create_ges(input_file: str, 
                    output_file: str,
                    max_expansion_size: int = 20,
                    max_per_entity: int = 3) -> str:
    """
    æ‰¹é‡ä¸ºtrimmingç»“æœåˆ›å»ºGES
    
    Args:
        input_file: è¾“å…¥çš„trimmingç»“æœæ–‡ä»¶
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        max_expansion_size: GESæœ€å¤§å¤§å°
        max_per_entity: æ¯ä¸ªå®ä½“æœ€å¤§æ‰©å±•æ•°
        
    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print(f"æ­£åœ¨ä¸º {input_file} åˆ›å»ºGold Expansion Set...")
    
    processed_count = 0
    total_expansion_ratio = 0
    total_semantic_improvement = 0
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line_num, line in enumerate(f_in, 1):
            if not line.strip():
                continue
                
            try:
                data = json.loads(line)
                
                # è·³è¿‡é…ç½®å’Œç»Ÿè®¡è¡Œ
                if 'config' in data or 'final_stats' in data or 'batch_stats' in data:
                    f_out.write(line)
                    continue
                
                # å¤„ç†æ ·æœ¬
                if 'sample_id' in data:
                    updated_sample = process_sample_with_ges(
                        data, max_expansion_size, max_per_entity
                    )
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    quality_metrics = updated_sample['ges_quality_metrics']
                    total_expansion_ratio += quality_metrics['expansion_ratio']
                    total_semantic_improvement += quality_metrics['semantic_improvement']
                    processed_count += 1
                    
                    f_out.write(json.dumps(updated_sample, ensure_ascii=False) + '\n')
                else:
                    f_out.write(line)
                    
            except json.JSONDecodeError as e:
                print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                f_out.write(line)
                continue
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if processed_count > 0:
        avg_expansion_ratio = total_expansion_ratio / processed_count
        avg_semantic_improvement = total_semantic_improvement / processed_count
        
        print(f"\n=== GESåˆ›å»ºå®Œæˆ ===")
        print(f"å¤„ç†æ ·æœ¬æ•°: {processed_count}")
        print(f"å¹³å‡æ‰©å±•æ¯”ä¾‹: {avg_expansion_ratio:.2f}x")
        print(f"å¹³å‡è¯­ä¹‰å…³ç³»æ”¹å–„: {avg_semantic_improvement:.3f}")
        print(f"ç»“æœä¿å­˜è‡³: {output_file}")
    
    return output_file

def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    import argparse
    import os
    from datetime import datetime
    
    parser = argparse.ArgumentParser(
        description='ä¸ºJSONLæ–‡ä»¶æ·»åŠ Gold Expansion Set',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python gold_expansion_utils.py input.jsonl
  python gold_expansion_utils.py input.jsonl --output custom_output.jsonl
  python gold_expansion_utils.py input.jsonl --max-size 15 --max-per-entity 2
        """
    )
    
    parser.add_argument('input_file', 
                       help='è¾“å…¥çš„trimmingç»“æœJSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', 
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: åŸæ–‡ä»¶å_with_ges.jsonl)')
    parser.add_argument('--max-size', type=int, default=20,
                       help='GESæœ€å¤§å¤§å° (é»˜è®¤: 20)')
    parser.add_argument('--max-per-entity', type=int, default=3,
                       help='æ¯ä¸ªå®ä½“æœ€å¤§æ‰©å±•æ•° (é»˜è®¤: 3)')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_file):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return 1
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if args.output:
        output_file = args.output
    else:
        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        input_base = os.path.splitext(args.input_file)[0]
        output_file = f"{input_base}_with_ges.jsonl"
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(output_file) and not args.force:
        print(f"âŒ é”™è¯¯: è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨: {output_file}")
        print("ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶è¦†ç›–ï¼Œæˆ–ä½¿ç”¨ --output æŒ‡å®šä¸åŒçš„è¾“å‡ºæ–‡ä»¶å")
        return 1
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("="*60)
    print("ğŸš€ Gold Expansion Set (GES) ç”Ÿæˆå·¥å…·")
    print("="*60)
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {args.input_file}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"âš™ï¸  æœ€å¤§GESå¤§å°: {args.max_size}")
    print(f"âš™ï¸  æ¯å®ä½“æœ€å¤§æ‰©å±•æ•°: {args.max_per_entity}")
    print(f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-"*60)
    
    try:
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        start_time = datetime.now()
        result_file = batch_create_ges(
            args.input_file, 
            output_file,
            args.max_size,
            args.max_per_entity
        )
        end_time = datetime.now()
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (end_time - start_time).total_seconds()
        
        print("-"*60)
        print(f"âœ… å¤„ç†å®Œæˆ!")
        print(f"ğŸ• æ€»å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
        print("="*60)
        
        return 0
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main()) 