"""
æ•°æ®é›†å¤„ç†å™¨æ¨¡å— (Dataset Processor)

æœ¬æ¨¡å—è´Ÿè´£GraphDeEPé¡¹ç›®çš„æ•°æ®é›†å®Œæ•´å¤„ç†æµç¨‹ï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. åŠ è½½åŸå§‹MetaQAæ•°æ®é›†å’Œå®ä½“å…³ç³»
2. æ•°æ®é¢„å¤„ç†å’Œæ ¼å¼è½¬æ¢
3. ğŸ¯ åŒé‡å­å›¾è£å‰ªç­–ç•¥ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
4. æ„å»ºæ¨¡å‹è¾“å…¥prompt
5. ä¿å­˜å¤„ç†åçš„æ•°æ®é›†

ğŸ¯ åŒé‡å­å›¾è£å‰ªç­–ç•¥ (Dual Subgraph Trimming Strategy):
æœ¬æ¨¡å—çš„æ ¸å¿ƒåˆ›æ–°ï¼Œè§£å†³TUSå’ŒFGASæŒ‡æ ‡äº’æ–¥é—®é¢˜ï¼š

TUSç­–ç•¥: ç²¾ç¡®shortest path golden triples
- ç›®æ ‡: æ³¨æ„åŠ›ç²¾åº¦ (attention precision)
- æ–¹æ³•: é—®é¢˜å®ä½“ â†’ ç­”æ¡ˆå®ä½“çš„æœ€çŸ­è·¯å¾„
- è¾“å‡º: gold_tripleså­—æ®µ (å¹³å‡2.4ä¸ª)

FGASç­–ç•¥: æ‰©å±•è¯­ä¹‰golden expansion set  
- ç›®æ ‡: è¯­ä¹‰ä¸°å¯Œåº¦ (semantic richness)
- æ–¹æ³•: golden triples + 1-hopé‚»æ¥ä¸‰å…ƒç»„
- è¾“å‡º: golden_expansion_setå­—æ®µ (å¹³å‡13.5ä¸ª)

ä¸»è¦å‡½æ•°:
- load_data: åŠ è½½åŸå§‹æ•°æ®é›†
- load_entities_and_relations: åŠ è½½å®ä½“å’Œå…³ç³»
- dual_subgraph_trimming: ğŸ¯ åŒé‡å­å›¾è£å‰ªç­–ç•¥ (æ¨è)
- prepare_dataset_dual_strategy: ğŸ¯ ä½¿ç”¨åŒé‡ç­–ç•¥å‡†å¤‡æ•°æ®é›† (é»˜è®¤)
- prepare_dataset_gold_priority: é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆè£å‰ª (å¤‡é€‰)
- prepare_dataset: åŸå§‹è£å‰ªæ–¹æ³• (åŸºçº¿)
- build_prompt: æ„å»ºæ¨¡å‹è¾“å…¥prompt

ä½œè€…: GraphDeEP Team
åˆ›å»ºæ—¥æœŸ: 2024-03-19
åŒé‡ç­–ç•¥: 2024-12-25
"""

import json
import os
import time
from datetime import datetime
from typing import List, Dict, Tuple, Set, Any
from tqdm import tqdm
from collections import defaultdict
import random

def load_data(data_path: str) -> List[Dict]:
    """
    åŠ è½½MetaQAæ•°æ®é›†
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„æˆ–æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
    
    Returns:
        æ•°æ®é›†åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«é—®é¢˜ã€å­å›¾å’Œç­”æ¡ˆ
    """
    data = []
    try:
        # å¦‚æœdata_pathæ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œè·å–å…¶ç›®å½•
        if data_path.endswith('.json'):
            data_dir = os.path.dirname(data_path)
            dev_file = data_path
        else:
            data_dir = data_path
            dev_file = os.path.join(data_path, "dev_simple.json")
            
        # åŠ è½½æ•°æ®
        with open(dev_file) as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
                    
        print(f"Loaded {len(data)} samples from {dev_file}")
        
    except Exception as e:
        print(f"Error loading data: {e}")
        raise
    return data

def load_entities_and_relations(data_path: str) -> Tuple[List[str], List[str]]:
    """
    åŠ è½½å®ä½“å’Œå…³ç³»åˆ—è¡¨
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„æˆ–æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
    
    Returns:
        å®ä½“åˆ—è¡¨å’Œå…³ç³»åˆ—è¡¨çš„å…ƒç»„
    """
    try:
        # å¦‚æœdata_pathæ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œè·å–å…¶ç›®å½•
        if data_path.endswith('.json'):
            data_dir = os.path.dirname(data_path)
        else:
            data_dir = data_path
            
        entities_file = os.path.join(data_dir, 'entities.txt')
        relations_file = os.path.join(data_dir, 'relations.txt')
        
        # åŠ è½½å®ä½“
        with open(entities_file, 'r', encoding='utf-8') as f:
            entities = [line.strip() for line in f if line.strip()]
            
        # åŠ è½½å…³ç³»
        with open(relations_file, 'r', encoding='utf-8') as f:
            relations = [line.strip() for line in f if line.strip()]
            
        print(f"Loaded {len(entities)} entities from {entities_file}")
        print(f"Loaded {len(relations)} relations from {relations_file}")
            
        return entities, relations
    except Exception as e:
        print(f"Error loading entities and relations: {e}")
        raise

def convert_indices_to_freebase(triple: List[int], 
                              entity_list: List[str], 
                              relation_list: List[str]) -> List[str]:
    """
    å°†ä¸‰å…ƒç»„çš„æ•°å­—ç´¢å¼•è½¬æ¢ä¸ºå®é™…çš„freebase IDå’Œå…³ç³»è·¯å¾„
    
    Args:
        triple: [head_idx, relation_idx, tail_idx]å½¢å¼çš„ä¸‰å…ƒç»„
        entity_list: å®ä½“åˆ—è¡¨ï¼ˆfreebase IDï¼‰
        relation_list: å…³ç³»åˆ—è¡¨ï¼ˆå…³ç³»è·¯å¾„ï¼‰
    
    Returns:
        [head_freebase_id, relation_path, tail_freebase_id]å½¢å¼çš„ä¸‰å…ƒç»„
    """
    head_idx, rel_idx, tail_idx = triple
    head_id = entity_list[head_idx] if head_idx < len(entity_list) else str(head_idx)
    relation = relation_list[rel_idx] if rel_idx < len(relation_list) else str(rel_idx)
    tail_id = entity_list[tail_idx] if tail_idx < len(entity_list) else str(tail_idx)
    return [head_id, relation, tail_id]

def build_prompt(sample: Dict, entity_list: List[str], relation_list: List[str], topk: int = 20) -> str:
    """
    æ„å»ºæ¨¡å‹è¾“å…¥çš„prompt
    
    Args:
        sample: æ ·æœ¬æ•°æ®å­—å…¸
        entity_list: å®ä½“åˆ—è¡¨
        relation_list: å…³ç³»åˆ—è¡¨
        topk: é€‰æ‹©çš„ä¸‰å…ƒç»„æ•°é‡
    
    Returns:
        æ„å»ºå¥½çš„promptå­—ç¬¦ä¸²
    """
    # è·å–è£å‰ªåçš„å­å›¾
    trimmed_triples = process_sample(sample, entity_list, topk)
    
    # æ„å»ºprompt
    prompt = ('Based on the triples retrieved from a knowledge graph, '
             'please answer the question. Please return formatted answers '
             'as a list, each prefixed with "ans:".\n Triplets:\n')
    
    for h, r, t in trimmed_triples:
        h_label = entity_list[h]
        r_label = relation_list[r]
        t_label = entity_list[t]
        prompt += f"({h_label}, {r_label}, {t_label})\n"
    
    prompt += f"\nQuestion:\n{sample['question']}\nAnswer:"
    return prompt

def get_gold_triples(triples: List[List[int]], 
                    question_entities: Set[int], 
                    answer_entities: Set[int]) -> List[List[int]]:
    """
    è·å–goldä¸‰å…ƒç»„ï¼ˆæœ€çŸ­è·¯å¾„ä¸Šçš„ä¸‰å…ƒç»„ï¼‰
    
    Args:
        triples: åŸå§‹ä¸‰å…ƒç»„åˆ—è¡¨
        question_entities: é—®é¢˜å®ä½“é›†åˆ
        answer_entities: ç­”æ¡ˆå®ä½“é›†åˆ
    
    Returns:
        goldä¸‰å…ƒç»„åˆ—è¡¨
    """
    # 1. æ„å»ºå›¾ç»“æ„
    graph, triple_dict = build_graph(triples)
    
    # 2. æ‰¾å‡ºæ‰€æœ‰é—®é¢˜å®ä½“åˆ°ç­”æ¡ˆå®ä½“çš„æœ€çŸ­è·¯å¾„
    gold_triples = set()  # ä½¿ç”¨é›†åˆå»é‡
    
    for q_entity in question_entities:
        for a_entity in answer_entities:
            path = find_shortest_path(graph, q_entity, a_entity)
            if path:
                path_triples = get_path_triples(path, triple_dict)
                gold_triples.update(path_triples)
    
    return [list(triple) for triple in gold_triples]  # è½¬æ¢å›åˆ—è¡¨å½¢å¼

def prepare_dataset(data_dir: str, output_dir: str, topk: int = 30) -> str:
    """
    å‡†å¤‡æ•°æ®é›†ï¼šåŠ è½½ã€è£å‰ªå’Œä¿å­˜
    
    Args:
        data_dir: åŸå§‹æ•°æ®é›†ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        topk: å­å›¾å¤§å°
    
    Returns:
        ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„
    """
    # 1. åŠ è½½æ•°æ®
    print("\nLoading data...")
    data = load_data(os.path.join(data_dir, "dev_simple.json"))
    entity_list, relation_list = load_entities_and_relations(data_dir)
    
    # ç»Ÿè®¡åŸå§‹ä¸‰å…ƒç»„æ•°é‡
    triple_counts = [len(sample['subgraph']['tuples']) for sample in data]
    print(f"æ ·æœ¬æ€»æ•°: {len(triple_counts)}")
    print(f"ä¸‰å…ƒç»„æ•°é‡ - æœ€å¤§: {max(triple_counts)}, æœ€å°: {min(triple_counts)}, å¹³å‡: {sum(triple_counts)/len(triple_counts):.2f}")
    
    # 2. è®¾ç½®å®éªŒå‚æ•°
    batch_size = 8
    num_samples = len(data)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # è®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„
    trimming_result_path = os.path.join(output_dir, f"trimming_results_{timestamp}.jsonl")
    
    # è®°å½•é…ç½®ä¿¡æ¯
    config = {
        "timestamp": timestamp,
        "topk": topk,
        "total_samples": num_samples,
        "data_dir": data_dir
    }
    
    # 3. å¼€å§‹æ•°æ®é›†è£å‰ª
    print("\n=== å¼€å§‹å­å›¾è£å‰ª ===")
    trimming_start_time = time.time()
    answer_covered_count = 0  # å…¨å±€è®¡æ•°å™¨
    batch_recalls = []
    
    with open(trimming_result_path, "w", encoding="utf-8") as f:
        # å†™å…¥é…ç½®ä¿¡æ¯
        f.write(json.dumps({"config": config}, ensure_ascii=False) + "\n")
        
        for i in tqdm(range(0, num_samples, batch_size), desc="Trimming Subgraphs"):
            batch_samples = data[i:i+batch_size]
            batch_covered = 0
            
            for j, sample in enumerate(batch_samples):
                question = sample['question']
                golden_texts = [ans['text'].lower() for ans in sample.get('answers', []) if ans.get('text')]
                
                # è·å–é—®é¢˜å®ä½“å’Œç­”æ¡ˆå®ä½“ - ä½¿ç”¨ç²¾ç¡®åŒ¹é…
                question_entities = set()
                question_lower = question.lower()
                for idx, entity in enumerate(entity_list):
                    entity_lower = entity.lower()
                    # ç²¾ç¡®åŒ¹é…ï¼šå®ä½“å¿…é¡»ä½œä¸ºå®Œæ•´è¯å‡ºç°
                    if entity_lower in question_lower:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´å•è¯ï¼ˆå‰åæ˜¯ç©ºæ ¼æˆ–æ ‡ç‚¹ï¼‰
                        start_pos = question_lower.find(entity_lower)
                        while start_pos != -1:
                            end_pos = start_pos + len(entity_lower)
                            
                            # æ£€æŸ¥å‰åè¾¹ç•Œ
                            before_ok = (start_pos == 0 or not question_lower[start_pos-1].isalnum())
                            after_ok = (end_pos == len(question_lower) or not question_lower[end_pos].isalnum())
                            
                            if before_ok and after_ok:
                                question_entities.add(idx)
                                break
                            
                            start_pos = question_lower.find(entity_lower, start_pos + 1)
                
                answer_entities = set()
                for ans in sample.get('answers', []):
                    if ans.get('text'):
                        ans_text = ans['text'].lower().strip()
                        for idx, entity in enumerate(entity_list):
                            entity_lower = entity.lower()
                            # ç²¾ç¡®åŒ¹é…ï¼šç­”æ¡ˆæ–‡æœ¬å¿…é¡»ä¸å®ä½“å®Œå…¨åŒ¹é…æˆ–å®ä½“åŒ…å«ç­”æ¡ˆæ–‡æœ¬
                            if ans_text == entity_lower or (ans_text in entity_lower and len(ans_text) >= 3):
                                # å¯¹äºåŒ…å«å…³ç³»ï¼Œç¡®ä¿æ˜¯å®Œæ•´å•è¯åŒ¹é…
                                if ans_text == entity_lower:
                                    answer_entities.add(idx)
                                else:
                                    # æ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´å•è¯
                                    start_pos = entity_lower.find(ans_text)
                                    if start_pos != -1:
                                        end_pos = start_pos + len(ans_text)
                                        before_ok = (start_pos == 0 or not entity_lower[start_pos-1].isalnum())
                                        after_ok = (end_pos == len(entity_lower) or not entity_lower[end_pos].isalnum())
                                        if before_ok and after_ok:
                                            answer_entities.add(idx)
                
                # è·å–goldä¸‰å…ƒç»„
                gold_triples = get_gold_triples(
                    sample['subgraph']['tuples'],
                    question_entities,
                    answer_entities
                )
                
                # ä½¿ç”¨æ–°çš„å­å›¾æ„å»ºæ–¹æ³•
                try:
                    trimmed_triples = process_sample(sample, entity_list, topk)
                    answer_covered = True  # ç”±äºç®—æ³•ä¿è¯ç­”æ¡ˆå¯è¾¾ï¼Œæ‰€ä»¥ä¸€å®šä¸ºTrue
                    batch_covered += 1
                    answer_covered_count += 1
                except AssertionError as e:
                    print(f"Warning: Failed to find path to answer for question: {question}")
                    # å¦‚æœæ‰¾ä¸åˆ°è·¯å¾„ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•ä½œä¸ºåå¤‡
                    trimmed_triples = get_fixed_length_subgraph(
                        sample['subgraph']['tuples'],
                        extract_question_entities(question, entity_list),
                        golden_texts,
                        entity_list,
                        topk
                    )
                    answer_covered = is_answer_covered(trimmed_triples, entity_list, golden_texts)
                    if answer_covered:
                        batch_covered += 1
                        answer_covered_count += 1
                
                # å°†ä¸‰å…ƒç»„çš„ç´¢å¼•è½¬æ¢ä¸ºå®é™…æ–‡æœ¬
                text_triples = []
                gold_text_triples = []
                for h, r, t in trimmed_triples:
                    head_text = entity_list[h]
                    rel_text = relation_list[r]
                    tail_text = entity_list[t]
                    text_triples.append([head_text, rel_text, tail_text])
                
                for h, r, t in gold_triples:
                    head_text = entity_list[h]
                    rel_text = relation_list[r]
                    tail_text = entity_list[t]
                    gold_text_triples.append([head_text, rel_text, tail_text])
                
                # å†™å…¥è£å‰ªç»“æœ
                result = {
                    "sample_id": i + j,
                    "question": question,
                    "golden_texts": golden_texts,
                    "trimmed_subgraph_length": len(trimmed_triples),
                    "original_subgraph_length": len(sample['subgraph']['tuples']),
                    "gold_triples_length": len(gold_triples),  # æ·»åŠ goldä¸‰å…ƒç»„é•¿åº¦
                    "answer_covered": answer_covered,
                    "trimmed_triples": text_triples,  # ä¿å­˜æ–‡æœ¬å½¢å¼çš„ä¸‰å…ƒç»„
                    "gold_triples": gold_text_triples,  # ä¿å­˜goldä¸‰å…ƒç»„
                    "processing_time": time.time() - trimming_start_time
                }
                f.write(json.dumps(result, ensure_ascii=False) + "\n")
            
            # è®¡ç®—batchç»Ÿè®¡ä¿¡æ¯
            batch_recall = batch_covered / len(batch_samples) * 100
            batch_recalls.append(batch_recall)
            current_recall = answer_covered_count / (i + len(batch_samples)) * 100
            
            # ä¿å­˜batchç»Ÿè®¡ä¿¡æ¯
            batch_stats = {
                "batch_stats": {
                    "batch_id": i // batch_size,
                    "batch_size": len(batch_samples),
                    "batch_covered": batch_covered,
                    "batch_recall": batch_recall,
                    "cumulative_recall": current_recall,
                    "processing_time": time.time() - trimming_start_time
                }
            }
            f.write(json.dumps(batch_stats, ensure_ascii=False) + "\n")
        
        # å†™å…¥æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        final_stats = {
            "final_stats": {
                "total_samples": num_samples,
                "answer_covered_count": answer_covered_count,
                "answer_recall": answer_covered_count / num_samples * 100,
                "avg_batch_recall": sum(batch_recalls) / len(batch_recalls),
                "min_batch_recall": min(batch_recalls),
                "max_batch_recall": max(batch_recalls),
                "total_time": time.time() - trimming_start_time
            }
        }
        f.write(json.dumps(final_stats, ensure_ascii=False) + "\n")
    
    print(f"\n=== è£å‰ªå®Œæˆ ===")
    print(f"Answer Recall: {answer_covered_count/num_samples*100:.2f}% ({answer_covered_count}/{num_samples})")
    print(f"å¹³å‡Batch Recall: {sum(batch_recalls)/len(batch_recalls):.2f}%")
    print(f"æœ€å°Batch Recall: {min(batch_recalls):.2f}%")
    print(f"æœ€å¤§Batch Recall: {max(batch_recalls):.2f}%")
    print(f"è£å‰ªè€—æ—¶: {time.time() - trimming_start_time:.2f}ç§’")
    print(f"ç»“æœå·²ä¿å­˜è‡³: {trimming_result_path}")
    
    return trimming_result_path

def extract_question_entities(question: str, entity_list: List[str]) -> List[int]:
    """
    ä»é—®é¢˜ä¸­æå–å®ä½“ç´¢å¼• - ä½¿ç”¨ç²¾ç¡®åŒ¹é…
    
    Args:
        question: è¾“å…¥é—®é¢˜
        entity_list: å®ä½“åˆ—è¡¨
    
    Returns:
        é—®é¢˜ä¸­å‡ºç°çš„å®ä½“ç´¢å¼•åˆ—è¡¨
    """
    entities = []
    question_lower = question.lower()
    
    for i, entity in enumerate(entity_list):
        entity_lower = entity.lower()
        # ç²¾ç¡®åŒ¹é…ï¼šå®ä½“å¿…é¡»ä½œä¸ºå®Œæ•´è¯å‡ºç°
        if entity_lower in question_lower:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´å•è¯ï¼ˆå‰åæ˜¯ç©ºæ ¼æˆ–æ ‡ç‚¹ï¼‰
            start_pos = question_lower.find(entity_lower)
            while start_pos != -1:
                end_pos = start_pos + len(entity_lower)
                
                # æ£€æŸ¥å‰åè¾¹ç•Œ
                before_ok = (start_pos == 0 or not question_lower[start_pos-1].isalnum())
                after_ok = (end_pos == len(question_lower) or not question_lower[end_pos].isalnum())
                
                if before_ok and after_ok:
                    entities.append(i)
                    break
                
                start_pos = question_lower.find(entity_lower, start_pos + 1)
    
    return entities

def is_answer_covered(trimmed_triples: List[List[int]], entity_list: List[str], golden_texts: List[str]) -> bool:
    """
    æ£€æŸ¥è£å‰ªåçš„å­å›¾æ˜¯å¦åŒ…å«æ­£ç¡®ç­”æ¡ˆ
    
    Args:
        trimmed_triples: è£å‰ªåçš„ä¸‰å…ƒç»„åˆ—è¡¨
        entity_list: å®ä½“åˆ—è¡¨
        golden_texts: æ­£ç¡®ç­”æ¡ˆæ–‡æœ¬åˆ—è¡¨
    
    Returns:
        æ˜¯å¦åŒ…å«æ­£ç¡®ç­”æ¡ˆ
    """
    # è·å–å­å›¾ä¸­çš„æ‰€æœ‰å®ä½“
    subgraph_entities = set()
    for h, r, t in trimmed_triples:
        subgraph_entities.add(entity_list[h].lower())
        subgraph_entities.add(entity_list[t].lower())
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆ
    for answer in golden_texts:
        if answer.lower() in subgraph_entities:
            return True
    
    return False

def build_external_context(sample: Dict[str, Any], 
                         tokenizer,
                         entity_list: List[str], 
                         relation_list: List[str]) -> Dict[str, Any]:
    """
    ä»trimming_resultsä¸­çš„æ ·æœ¬æ„å»ºexternal_contextå­—å…¸
    
    å‚æ•°:
        sample: trimming_resultsä¸­çš„ä¸€ä¸ªæ ·æœ¬
        tokenizer: åˆ†è¯å™¨ï¼Œç”¨äºè·å–tokenä½ç½®
        entity_list: å®ä½“åˆ—è¡¨
        relation_list: å…³ç³»åˆ—è¡¨
    
    è¿”å›:
        external_contextå­—å…¸ï¼ŒåŒ…å«ï¼š
        - entities: å­å›¾ä¸­çš„å®ä½“IDåˆ—è¡¨
        - relations: å­å›¾ä¸­çš„å…³ç³»IDåˆ—è¡¨
        - triples: ä¸‰å…ƒç»„åˆ—è¡¨
        - entity_positions: å®ä½“åœ¨è¾“å…¥åºåˆ—ä¸­çš„ä½ç½®
        - relation_positions: å…³ç³»åœ¨è¾“å…¥åºåˆ—ä¸­çš„ä½ç½®
        - gold_triples_positions: goldä¸‰å…ƒç»„åœ¨è¾“å…¥åºåˆ—ä¸­çš„ä½ç½®
    """
    # 1. å°†æ–‡æœ¬ä¸‰å…ƒç»„è½¬æ¢ä¸ºIDå½¢å¼
    triples = []
    entities = set()
    relations = set()
    
    # è·å–goldä¸‰å…ƒç»„åˆ—è¡¨
    gold_triples = sample['gold_triples']
    
    for h_text, r_text, t_text in sample['trimmed_triples']:
        # æŸ¥æ‰¾å®ä½“å’Œå…³ç³»çš„ID
        try:
            h_id = entity_list.index(h_text)
        except ValueError:
            h_id = len(entity_list) + len(entities)
            entities.add(h_id)
            
        try:
            r_id = relation_list.index(r_text)
        except ValueError:
            r_id = len(relation_list) + len(relations)
            relations.add(r_id)
            
        try:
            t_id = entity_list.index(t_text)
        except ValueError:
            t_id = len(entity_list) + len(entities)
            entities.add(t_id)
        
        triples.append([h_id, r_id, t_id])
        entities.add(h_id)
        entities.add(t_id)
        relations.add(r_id)
    
    # 2. æ„å»ºè¾“å…¥åºåˆ—
    question = sample['question']
    context = ""
    gold_positions = set()  # ç”¨äºè®°å½•goldä¸‰å…ƒç»„çš„ä½ç½®
    current_pos = len(tokenizer.encode(question + " [SEP] ", add_special_tokens=True)) - 1
    
    for h, r, t in triples:
        h_text = entity_list[h] if h < len(entity_list) else f"entity_{h}"
        r_text = relation_list[r] if r < len(relation_list) else f"relation_{r}"
        t_text = entity_list[t] if t < len(entity_list) else f"entity_{t}"
        
        # å¦‚æœå½“å‰ä¸‰å…ƒç»„æ˜¯goldä¸‰å…ƒç»„
        triple_text = f"{h_text} {r_text} {t_text}"
        if [h_text, r_text, t_text] in gold_triples:
            # è®¡ç®—è¿™ä¸ªä¸‰å…ƒç»„ä¸­æ‰€æœ‰tokençš„ä½ç½®
            triple_tokens = tokenizer.encode(triple_text, add_special_tokens=False)
            gold_positions.update(range(current_pos, current_pos + len(triple_tokens)))
        
        context += triple_text + " . "
        current_pos += len(tokenizer.encode(triple_text + " . ", add_special_tokens=False))
    
    # 3. å¯¹å®Œæ•´è¾“å…¥è¿›è¡Œç¼–ç 
    full_input = f"{question} [SEP] {context}"
    tokens = tokenizer.encode(full_input, add_special_tokens=True)
    
    # 4. æ‰¾åˆ°å®ä½“å’Œå…³ç³»åœ¨åºåˆ—ä¸­çš„ä½ç½®
    entity_positions = []
    relation_positions = []
    
    # å¯¹é—®é¢˜ä¸­çš„å®ä½“è¿›è¡Œå®šä½
    for entity_id in entities:
        if entity_id >= len(entity_list):
            continue
        entity_text = entity_list[entity_id]
        # åœ¨é—®é¢˜ä¸­æŸ¥æ‰¾
        entity_tokens = tokenizer.encode(entity_text, add_special_tokens=False)
        for i in range(len(tokens) - len(entity_tokens) + 1):
            if tokens[i:i+len(entity_tokens)] == entity_tokens:
                entity_positions.extend(range(i, i + len(entity_tokens)))
    
    # å¯¹å­å›¾ä¸­çš„å…³ç³»è¿›è¡Œå®šä½
    for relation_id in relations:
        if relation_id >= len(relation_list):
            continue
        relation_text = relation_list[relation_id]
        # åœ¨å­å›¾éƒ¨åˆ†æŸ¥æ‰¾
        relation_tokens = tokenizer.encode(relation_text, add_special_tokens=False)
        for i in range(len(tokens) - len(relation_tokens) + 1):
            if tokens[i:i+len(relation_tokens)] == relation_tokens:
                relation_positions.extend(range(i, i + len(relation_tokens)))
    
    return {
        'entities': list(entities),
        'relations': list(relations),
        'triples': triples,
        'entity_positions': sorted(list(set(entity_positions))),
        'relation_positions': sorted(list(set(relation_positions))),
        'gold_triples_positions': sorted(list(gold_positions))  # æ·»åŠ goldä¸‰å…ƒç»„ä½ç½®
    }

# ============= å­å›¾å¤„ç†å·¥å…·å‡½æ•° =============

def build_graph(triples: List[List[int]]) -> Tuple[Dict[int, List[int]], Dict[Tuple[int, int], Tuple[int, int, int]]]:
    """æ„å»ºæ— å‘å›¾ï¼Œç”¨äºå¯»æ‰¾æœ€çŸ­è·¯å¾„"""
    graph = defaultdict(list)
    triple_dict = {}  # ç”¨äºåæŸ¥ä¸‰å…ƒç»„
    for i, (h, r, t) in enumerate(triples):
        graph[h].append(t)
        graph[t].append(h)
        triple_dict[(h, t)] = (h, r, t)  # ç¡®ä¿å­˜å‚¨ä¸ºå…ƒç»„
        triple_dict[(t, h)] = (h, r, t)  # ç¡®ä¿å­˜å‚¨ä¸ºå…ƒç»„
    return graph, triple_dict

def find_shortest_path(graph: Dict[int, List[int]], start: int, end: int, max_depth: int = 3) -> List[int]:
    """ä½¿ç”¨BFSæ‰¾åˆ°ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„æœ€çŸ­è·¯å¾„"""
    if start == end:
        return [start]
    
    visited = {start}
    queue = [(start, [start])]
    
    while queue:
        vertex, path = queue.pop(0)
        if len(path) > max_depth:  # é™åˆ¶æœ€å¤§è·³æ•°
            continue
            
        for neighbor in graph[vertex]:
            if neighbor == end:
                return path + [neighbor]
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, path + [neighbor]))
    return None

def get_path_triples(path: List[int], triple_dict: Dict[Tuple[int, int], Tuple[int, int, int]]) -> List[Tuple[int, int, int]]:
    """å°†è·¯å¾„è½¬æ¢ä¸ºä¸‰å…ƒç»„"""
    path_triples = []
    for i in range(len(path) - 1):
        h, t = path[i], path[i + 1]
        if (h, t) in triple_dict:
            path_triples.append(triple_dict[(h, t)])
        elif (t, h) in triple_dict:
            path_triples.append(triple_dict[(t, h)])
    return path_triples

def get_neighbor_triples(entities: Set[int], triples: List[List[int]], exclude_triples: Set[Tuple[int, int, int]]) -> List[Tuple[int, int, int]]:
    """è·å–å®ä½“çš„1-hopé‚»å±…å½¢æˆçš„ä¸‰å…ƒç»„"""
    neighbors = []
    for h, r, t in triples:
        triple = (h, r, t)
        if triple not in exclude_triples:
            if h in entities or t in entities:
                neighbors.append(triple)
    return neighbors

def score_triple(triple: Tuple[int, int, int], path_entities: Set[int], q_entities: Set[int], a_entities: Set[int]) -> int:
    """å¯¹ä¸‰å…ƒç»„è¿›è¡Œè¯„åˆ†ï¼Œè€ƒè™‘å¤šä¸ªå› ç´ """
    h, r, t = triple
    score = 0
    
    # 1. ä¸è·¯å¾„å®ä½“çš„è¿æ¥
    if h in path_entities or t in path_entities:
        score += 3
        
    # 2. ä¸é—®é¢˜å®ä½“çš„è¿æ¥
    if h in q_entities or t in q_entities:
        score += 2
        
    # 3. ä¸ç­”æ¡ˆå®ä½“çš„è¿æ¥
    if h in a_entities or t in a_entities:
        score += 2
        
    return score

def get_subgraph_gold_priority(triples: List[List[int]], 
                              question_entities: Set[int], 
                              answer_entities: Set[int], 
                              max_size: int = 30,
                              gold_priority_ratio: float = 0.7) -> Tuple[List[List[int]], List[List[int]]]:
    """
    åŸºäºé‡‘ä¸‰å…ƒç»„ä¼˜å…ˆçº§çš„å­å›¾æ„å»ºæ–¹æ³•ï¼Œå…¼é¡¾TUSå’ŒFGASéœ€æ±‚
    
    ç­–ç•¥ï¼š
    1. å…ˆä»åŸå§‹å›¾ç”Ÿæˆå®Œæ•´çš„é‡‘ä¸‰å…ƒç»„é›†åˆï¼ˆä¿è¯TUSå®Œæ•´æ€§ï¼‰
    2. ç¡®ä¿é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆè¢«ä¿ç•™åœ¨è£å‰ªåçš„å­å›¾ä¸­ï¼ˆä¿è¯FGASä¸€è‡´æ€§ï¼‰
    3. ç”¨å‰©ä½™ç©ºé—´å¡«å……é‡è¦çš„é‚»å±…ä¸‰å…ƒç»„ï¼ˆä¿è¯å­å›¾è´¨é‡ï¼‰
    4. è¿”å›è£å‰ªåçš„å­å›¾å’Œå¯¹åº”çš„é‡‘ä¸‰å…ƒç»„
    
    Args:
        triples: åŸå§‹ä¸‰å…ƒç»„åˆ—è¡¨
        question_entities: é—®é¢˜å®ä½“é›†åˆ
        answer_entities: ç­”æ¡ˆå®ä½“é›†åˆ
        max_size: å­å›¾æœ€å¤§å¤§å°
        gold_priority_ratio: é‡‘ä¸‰å…ƒç»„åœ¨å­å›¾ä¸­çš„æœ€å¤§å æ¯”ï¼ˆ0.7è¡¨ç¤º70%ç©ºé—´ç•™ç»™é‡‘ä¸‰å…ƒç»„ï¼‰
        
    Returns:
        (trimmed_triples, effective_gold_triples) å…ƒç»„
        - trimmed_triples: è£å‰ªåçš„å­å›¾
        - effective_gold_triples: ç¡®å®ä¿ç•™åœ¨å­å›¾ä¸­çš„é‡‘ä¸‰å…ƒç»„
    """
    # 1. æ„å»ºå›¾ç»“æ„
    graph, triple_dict = build_graph(triples)
    
    # 2. ç”Ÿæˆå®Œæ•´çš„é‡‘ä¸‰å…ƒç»„é›†åˆï¼ˆç”¨äºTUSçš„å®Œæ•´åˆ†æï¼‰
    original_gold_triples = set()
    path_entities = set()
    
    # æ£€æŸ¥é—®é¢˜å®ä½“å’Œç­”æ¡ˆå®ä½“æ˜¯å¦åœ¨å­å›¾ä¸­
    subgraph_entities = set()
    for h, r, t in triples:
        subgraph_entities.add(h)
        subgraph_entities.add(t)
    
    valid_question_entities = question_entities & subgraph_entities
    valid_answer_entities = answer_entities & subgraph_entities
    
    print(f"è°ƒè¯•ä¿¡æ¯: é—®é¢˜å®ä½“ {len(question_entities)} ä¸ªï¼Œå…¶ä¸­ {len(valid_question_entities)} ä¸ªåœ¨å­å›¾ä¸­")
    print(f"è°ƒè¯•ä¿¡æ¯: ç­”æ¡ˆå®ä½“ {len(answer_entities)} ä¸ªï¼Œå…¶ä¸­ {len(valid_answer_entities)} ä¸ªåœ¨å­å›¾ä¸­")
    
    # å¦‚æœé—®é¢˜å®ä½“æˆ–ç­”æ¡ˆå®ä½“ä¸åœ¨å­å›¾ä¸­ï¼Œä½¿ç”¨åå¤‡ç­–ç•¥
    if not valid_question_entities or not valid_answer_entities:
        print("è­¦å‘Š: é—®é¢˜å®ä½“æˆ–ç­”æ¡ˆå®ä½“ä¸åœ¨å­å›¾ä¸­ï¼Œä½¿ç”¨åå¤‡ç­–ç•¥")
        # åå¤‡ç­–ç•¥ï¼šç›´æ¥æŸ¥æ‰¾åŒ…å«ç­”æ¡ˆå®ä½“çš„ä¸‰å…ƒç»„ä½œä¸ºé‡‘ä¸‰å…ƒç»„
        for h, r, t in triples:
            if h in answer_entities or t in answer_entities:
                original_gold_triples.add((h, r, t))
                path_entities.update([h, t])
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰ä¸‰å…ƒç»„çš„å‰å‡ ä¸ªä½œä¸ºé‡‘ä¸‰å…ƒç»„
        if not original_gold_triples:
            print("è­¦å‘Š: æ‰¾ä¸åˆ°åŒ…å«ç­”æ¡ˆå®ä½“çš„ä¸‰å…ƒç»„ï¼Œä½¿ç”¨å‰å‡ ä¸ªä¸‰å…ƒç»„ä½œä¸ºé‡‘ä¸‰å…ƒç»„")
            for i, (h, r, t) in enumerate(triples[:5]):  # å–å‰5ä¸ª
                original_gold_triples.add((h, r, t))
                path_entities.update([h, t])
    else:
        # æ­£å¸¸ç­–ç•¥ï¼šæŸ¥æ‰¾æœ€çŸ­è·¯å¾„
        for q_entity in valid_question_entities:
            for a_entity in valid_answer_entities:
                path = find_shortest_path(graph, q_entity, a_entity)
                if path:
                    path_triples = get_path_triples(path, triple_dict)
                    original_gold_triples.update(path_triples)
                    path_entities.update(path)
    
    # 3. è®¡ç®—é‡‘ä¸‰å…ƒç»„çš„ä¼˜å…ˆçº§é¢„ç®—
    max_gold_count = min(len(original_gold_triples), int(max_size * gold_priority_ratio))
    
    # 4. å¯¹é‡‘ä¸‰å…ƒç»„æŒ‰é‡è¦æ€§æ’åº
    gold_triples_scored = []
    for triple in original_gold_triples:
        score = score_triple(triple, path_entities, question_entities, answer_entities)
        gold_triples_scored.append((triple, score))
    
    # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€é‡è¦çš„é‡‘ä¸‰å…ƒç»„
    gold_triples_scored.sort(key=lambda x: x[1], reverse=True)
    priority_gold_triples = [triple for triple, _ in gold_triples_scored[:max_gold_count]]
    
    # 5. è·å–é‚»å±…ä¸‰å…ƒç»„
    neighbor_triples = get_neighbor_triples(
        path_entities, 
        triples, 
        set(priority_gold_triples)
    )
    
    # 6. å¯¹é‚»å±…ä¸‰å…ƒç»„è¯„åˆ†
    scored_neighbors = [
        (triple, score_triple(triple, path_entities, question_entities, answer_entities))
        for triple in neighbor_triples
    ]
    scored_neighbors.sort(key=lambda x: x[1], reverse=True)
    
    # 7. ç»„åˆæœ€ç»ˆå­å›¾ï¼šä¼˜å…ˆé‡‘ä¸‰å…ƒç»„ + é‡è¦é‚»å±…
    selected_triples = list(priority_gold_triples)
    remaining_size = max_size - len(selected_triples)
    
    # æ·»åŠ é«˜åˆ†é‚»å±…ä¸‰å…ƒç»„
    for triple, _ in scored_neighbors[:remaining_size]:
        selected_triples.append(triple)
    
    # 8. å¦‚æœè¿˜ä¸å¤Ÿï¼Œé‡å¤ä½¿ç”¨å·²æœ‰ä¸‰å…ƒç»„
    while len(selected_triples) < max_size:
        selected_triples.extend(selected_triples[:max_size - len(selected_triples)])
    
    # 9. ç¡®ä¿è¿”å›æ°å¥½max_sizeä¸ªä¸‰å…ƒç»„
    final_triples = [list(t) for t in selected_triples[:max_size]]
    effective_gold_triples = [list(t) for t in priority_gold_triples]
    
    # 10. éªŒè¯ç­”æ¡ˆå¯è¾¾æ€§ï¼ˆå¯¹äºæœ‰æ•ˆçš„ç­”æ¡ˆå®ä½“ï¼‰
    final_entities = set()
    for h, r, t in final_triples:
        final_entities.add(h)
        final_entities.add(t)
    
    # åªéªŒè¯åŸæœ¬åœ¨å­å›¾ä¸­çš„ç­”æ¡ˆå®ä½“
    original_valid_answers = answer_entities & subgraph_entities
    if original_valid_answers:
        final_valid_answers = original_valid_answers & final_entities
        if not final_valid_answers:
            print(f"è­¦å‘Š: åŸæœ¬æœ‰æ•ˆçš„ç­”æ¡ˆå®ä½“ {original_valid_answers} åœ¨æœ€ç»ˆå­å›¾ä¸­ä¸¢å¤±")
        else:
            print(f"éªŒè¯é€šè¿‡: {len(final_valid_answers)}/{len(original_valid_answers)} ä¸ªæœ‰æ•ˆç­”æ¡ˆå®ä½“ä¿ç•™åœ¨æœ€ç»ˆå­å›¾ä¸­")
    
    return final_triples, effective_gold_triples

def get_subgraph_shortest_path_plus(triples: List[List[int]], 
                                  question_entities: Set[int], 
                                  answer_entities: Set[int], 
                                  entity_list: List[str], 
                                  max_size: int = 30) -> List[List[int]]:
    """
    åŸºäºæœ€çŸ­è·¯å¾„çš„å­å›¾æ„å»ºæ–¹æ³•ï¼ŒåŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼š
    1. æ‰¾å‡ºé—®é¢˜å®ä½“åˆ°ç­”æ¡ˆå®ä½“çš„æœ€çŸ­è·¯å¾„
    2. æ‰©å±•è·¯å¾„ä¸Šå®ä½“çš„1-hopé‚»å±…
    3. æ ¹æ®é‡è¦æ€§è¯„åˆ†é€‰æ‹©æœ€ç»ˆå­å›¾
    
    ç¡®ä¿è¿”å›æ°å¥½max_sizeä¸ªä¸‰å…ƒç»„ï¼Œå¦‚æœä¸è¶³åˆ™é‡å¤ä½¿ç”¨å·²æœ‰ä¸‰å…ƒç»„ã€‚
    """
    # 1. æ„å»ºå›¾ç»“æ„
    graph, triple_dict = build_graph(triples)
    
    # 2. æ‰¾å‡ºæ‰€æœ‰é—®é¢˜å®ä½“åˆ°ç­”æ¡ˆå®ä½“çš„æœ€çŸ­è·¯å¾„
    all_path_triples = set()  # ä½¿ç”¨å…ƒç»„å­˜å‚¨ä¸‰å…ƒç»„
    path_entities = set()
    
    for q_entity in question_entities:
        for a_entity in answer_entities:
            path = find_shortest_path(graph, q_entity, a_entity)
            if path:
                path_triples = get_path_triples(path, triple_dict)
                all_path_triples.update(path_triples)
                path_entities.update(path)
    
    # 3. è·å–è·¯å¾„å®ä½“çš„é‚»å±…ä¸‰å…ƒç»„
    neighbor_triples = get_neighbor_triples(
        path_entities, 
        triples, 
        all_path_triples
    )
    
    # 4. å¯¹é‚»å±…ä¸‰å…ƒç»„è¯„åˆ†å¹¶é€‰æ‹©æœ€é‡è¦çš„
    scored_neighbors = [
        (triple, score_triple(triple, path_entities, question_entities, answer_entities))
        for triple in neighbor_triples
    ]
    scored_neighbors.sort(key=lambda x: x[1], reverse=True)
    
    # 5. ç»„åˆæœ€ç»ˆå­å›¾
    selected_triples = list(all_path_triples)
    selected_triples.extend(triple for triple, _ in scored_neighbors)
    
    # 6. å¦‚æœä¸‰å…ƒç»„æ€»æ•°ä¸è¶³max_sizeï¼Œåˆ™å¾ªç¯ä½¿ç”¨å·²æœ‰çš„ä¸‰å…ƒç»„
    while len(selected_triples) < max_size:
        selected_triples.extend(selected_triples[:max_size - len(selected_triples)])
    
    # 7. ç¡®ä¿è¿”å›æ°å¥½max_sizeä¸ªä¸‰å…ƒç»„
    result = [list(t) for t in selected_triples[:max_size]]
    
    # éªŒè¯ç­”æ¡ˆå¯è¾¾æ€§
    final_entities = set()
    for h, r, t in result:
        final_entities.add(h)
        final_entities.add(t)
    assert answer_entities & final_entities, "Answer entities must be in the subgraph"
    
    return result

def process_sample(sample: Dict, entity_list: List[str], topk: int = 30) -> List[List[int]]:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œè¿”å›è£å‰ªåçš„å­å›¾
    """
    question = sample['question']
    triples = sample['subgraph']['tuples']
    
    # è·å–é—®é¢˜å®ä½“ - ä½¿ç”¨ç²¾ç¡®åŒ¹é…
    question_entities = set()
    question_lower = question.lower()
    for i, entity in enumerate(entity_list):
        entity_lower = entity.lower()
        if entity_lower in question_lower:
            start_pos = question_lower.find(entity_lower)
            while start_pos != -1:
                end_pos = start_pos + len(entity_lower)
                before_ok = (start_pos == 0 or not question_lower[start_pos-1].isalnum())
                after_ok = (end_pos == len(question_lower) or not question_lower[end_pos].isalnum())
                if before_ok and after_ok:
                    question_entities.add(i)
                    break
                start_pos = question_lower.find(entity_lower, start_pos + 1)
    
    # è·å–ç­”æ¡ˆå®ä½“ - ä½¿ç”¨ç²¾ç¡®åŒ¹é…
    answer_entities = set()
    for ans in sample.get('answers', []):
        if ans.get('text'):
            ans_text = ans['text'].lower().strip()
            for i, entity in enumerate(entity_list):
                entity_lower = entity.lower()
                if ans_text == entity_lower or (ans_text in entity_lower and len(ans_text) >= 3):
                    if ans_text == entity_lower:
                        answer_entities.add(i)
                    else:
                        start_pos = entity_lower.find(ans_text)
                        if start_pos != -1:
                            end_pos = start_pos + len(ans_text)
                            before_ok = (start_pos == 0 or not entity_lower[start_pos-1].isalnum())
                            after_ok = (end_pos == len(entity_lower) or not entity_lower[end_pos].isalnum())
                            if before_ok and after_ok:
                                answer_entities.add(i)
    
    # ä½¿ç”¨æ”¹è¿›çš„å­å›¾æ„å»ºæ–¹æ³•
    trimmed_triples = get_subgraph_shortest_path_plus(
        triples,
        question_entities,
        answer_entities,
        entity_list,
        topk
    )
    
    return trimmed_triples

def process_sample_gold_aware(sample: Dict, entity_list: List[str], topk: int = 30) -> Tuple[List[List[int]], List[List[int]]]:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œä½¿ç”¨é‡‘ä¸‰å…ƒç»„æ„ŸçŸ¥çš„è£å‰ªæ–¹æ³•
    
    Args:
        sample: æ ·æœ¬æ•°æ®å­—å…¸
        entity_list: å®ä½“åˆ—è¡¨
        topk: å­å›¾å¤§å°
    
    Returns:
        (trimmed_triples, gold_triples) å…ƒç»„
    """
    question = sample['question']
    triples = sample['subgraph']['tuples']
    
    # è·å–é—®é¢˜å®ä½“
    question_entities = set()
    question_lower = question.lower()
    for i, entity in enumerate(entity_list):
        entity_lower = entity.lower()
        if entity_lower in question_lower:
            start_pos = question_lower.find(entity_lower)
            while start_pos != -1:
                end_pos = start_pos + len(entity_lower)
                before_ok = (start_pos == 0 or not question_lower[start_pos-1].isalnum())
                after_ok = (end_pos == len(question_lower) or not question_lower[end_pos].isalnum())
                if before_ok and after_ok:
                    question_entities.add(i)
                    break
                start_pos = question_lower.find(entity_lower, start_pos + 1)
    
    # è·å–ç­”æ¡ˆå®ä½“
    answer_entities = set()
    for ans in sample.get('answers', []):
        if ans.get('text'):
            ans_text = ans['text'].lower().strip()
            for i, entity in enumerate(entity_list):
                entity_lower = entity.lower()
                if ans_text == entity_lower or (ans_text in entity_lower and len(ans_text) >= 3):
                    if ans_text == entity_lower:
                        answer_entities.add(i)
                    else:
                        start_pos = entity_lower.find(ans_text)
                        if start_pos != -1:
                            end_pos = start_pos + len(ans_text)
                            before_ok = (start_pos == 0 or not entity_lower[start_pos-1].isalnum())
                            after_ok = (end_pos == len(entity_lower) or not entity_lower[end_pos].isalnum())
                            if before_ok and after_ok:
                                answer_entities.add(i)
    
    # ä½¿ç”¨é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆçš„è£å‰ªæ–¹æ³•
    trimmed_triples, gold_triples = get_subgraph_gold_priority(
        triples,
        question_entities,
        answer_entities,
        topk
    )
    
    return trimmed_triples, gold_triples

def get_fixed_length_subgraph(triples: List[List[int]], 
                            entity_indices: List[int], 
                            golden_texts: List[str], 
                            entity_list: List[str], 
                            topk: int = 30) -> List[List[int]]:
    """
    è£å‰ªå‡ºä¸entity_indicesç›¸å…³çš„å›ºå®štopkæ¡ä¸‰å…ƒç»„çš„å­å›¾ã€‚
    ç¡®ä¿åŒ…å«ç­”æ¡ˆçš„ä¸‰å…ƒç»„ä¸€å®šåœ¨ç»“æœä¸­ï¼Œå¹¶ä¸”æ€»æ˜¯è¿”å›æ°å¥½topkä¸ªä¸‰å…ƒç»„ã€‚
    å¦‚æœåŸå§‹ä¸‰å…ƒç»„æ•°é‡ä¸è¶³topkï¼Œåˆ™é‡å¤ä½¿ç”¨å·²æœ‰ä¸‰å…ƒç»„ç›´åˆ°è¾¾åˆ°topkã€‚
    """
    # 1. æ‰¾å‡ºåŒ…å«ç­”æ¡ˆçš„ä¸‰å…ƒç»„
    answer_triples = []
    for triple in triples:
        h, r, t = triple
        h_text = entity_list[h].lower()
        t_text = entity_list[t].lower()
        if any(gold in h_text or gold in t_text for gold in golden_texts):
            answer_triples.append(triple)
    
    # 2. è·å–1-hopä¸‰å…ƒç»„
    one_hop = [triple for triple in triples if triple[0] in entity_indices or triple[2] in entity_indices]
    
    # 3. ç¡®ä¿answer_triplesåœ¨ç»“æœä¸­
    result = list(set(answer_triples))  # å»é‡
    
    # 4. æ·»åŠ 1-hopä¸‰å…ƒç»„ï¼ˆæ’é™¤å·²æœ‰çš„answer_triplesï¼‰
    remaining_one_hop = [t for t in one_hop if t not in result]
    result.extend(remaining_one_hop)
    
    # 5. å¦‚æœè¿˜ä¸å¤Ÿï¼Œæ·»åŠ 2-hopä¸‰å…ƒç»„
    if len(result) < topk:
        one_hop_entities = set([h for h, _, t in result] + [t for h, _, t in result])
        two_hop = [triple for triple in triples 
                  if (triple[0] in one_hop_entities or triple[2] in one_hop_entities) 
                  and triple not in result]
        result.extend(two_hop)
    
    # 6. å¦‚æœè¿˜ä¸å¤Ÿï¼Œå¾ªç¯ä½¿ç”¨å·²æœ‰ä¸‰å…ƒç»„
    while len(result) < topk:
        result.extend(result[:topk - len(result)])
    
    # 7. ç¡®ä¿è¿”å›æ°å¥½topkä¸ªä¸‰å…ƒç»„
    return result[:topk]

def prepare_dataset_gold_priority(data_dir: str, output_dir: str, topk: int = 30, num_samples: int = None) -> str:
    """
    ä½¿ç”¨é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆæ–¹æ³•å‡†å¤‡æ•°æ®é›†ï¼šåŠ è½½ã€è£å‰ªå’Œä¿å­˜
    
    Args:
        data_dir: åŸå§‹æ•°æ®é›†ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        topk: å­å›¾å¤§å°
        num_samples: å¤„ç†çš„æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ ·æœ¬
    
    Returns:
        ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„
    """
    # 1. åŠ è½½æ•°æ®
    print("\nğŸš€ å¼€å§‹ä½¿ç”¨é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆæ–¹æ³•å‡†å¤‡æ•°æ®é›†")
    print(f"å‚æ•°: topk={topk}, num_samples={num_samples or 'å…¨éƒ¨'}")
    
    print("\nğŸ“š åŠ è½½æ•°æ®...")
    data = load_data(os.path.join(data_dir, "dev_simple.json"))
    entity_list, relation_list = load_entities_and_relations(data_dir)
    
    # å¦‚æœæŒ‡å®šäº†æ ·æœ¬æ•°é‡ï¼Œåˆ™åªå¤„ç†æŒ‡å®šæ•°é‡çš„æ ·æœ¬
    if num_samples is not None:
        data = data[:num_samples]
    
    print(f"å°†å¤„ç† {len(data)} ä¸ªæ ·æœ¬")
    print(f"å®ä½“æ•°é‡: {len(entity_list)}, å…³ç³»æ•°é‡: {len(relation_list)}")
    
    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # è®¾ç½®ä¿å­˜æ–‡ä»¶è·¯å¾„
    result_path = os.path.join(output_dir, f"trimming_results_gold_priority_{timestamp}.jsonl")
    
    # 3. å¤„ç†æ ·æœ¬
    print(f"\nâš™ï¸ å¼€å§‹ä½¿ç”¨é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆæ–¹æ³•å¤„ç†æ ·æœ¬...")
    
    processed_samples = []
    stats = {
        'total_samples': len(data),
        'successful_samples': 0,
        'failed_samples': 0,
        'total_original_gold': 0,
        'total_effective_gold': 0,
        'gold_retention_rates': []
    }
    
    # è®°å½•é…ç½®ä¿¡æ¯
    config = {
        "timestamp": timestamp,
        "method": "gold_priority_trimming",
        "topk": topk,
        "total_samples": len(data),
        "data_dir": data_dir
    }
    
    start_time = time.time()
    
    with open(result_path, "w", encoding="utf-8") as f:
        # å†™å…¥é…ç½®ä¿¡æ¯
        f.write(json.dumps({"config": config}, ensure_ascii=False) + "\n")
        
        for i, sample in enumerate(tqdm(data, desc="å¤„ç†æ ·æœ¬")):
            try:
                # ä½¿ç”¨æ–°çš„é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆè£å‰ªæ–¹æ³•
                trimmed_triples, effective_gold_triples = process_sample_gold_aware(
                    sample, entity_list, topk
                )
                
                # è·å–åŸå§‹é‡‘ä¸‰å…ƒç»„ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
                question = sample['question']
                golden_texts = [ans['text'].lower() for ans in sample.get('answers', []) if ans.get('text')]
                
                # è·å–é—®é¢˜å®ä½“å’Œç­”æ¡ˆå®ä½“
                question_entities = set()
                question_lower = question.lower()
                for idx, entity in enumerate(entity_list):
                    entity_lower = entity.lower()
                    if entity_lower in question_lower:
                        start_pos = question_lower.find(entity_lower)
                        while start_pos != -1:
                            end_pos = start_pos + len(entity_lower)
                            before_ok = (start_pos == 0 or not question_lower[start_pos-1].isalnum())
                            after_ok = (end_pos == len(question_lower) or not question_lower[end_pos].isalnum())
                            if before_ok and after_ok:
                                question_entities.add(idx)
                                break
                            start_pos = question_lower.find(entity_lower, start_pos + 1)
                
                answer_entities = set()
                for ans in sample.get('answers', []):
                    if ans.get('text'):
                        ans_text = ans['text'].lower().strip()
                        for idx, entity in enumerate(entity_list):
                            entity_lower = entity.lower()
                            if ans_text == entity_lower or (ans_text in entity_lower and len(ans_text) >= 3):
                                if ans_text == entity_lower:
                                    answer_entities.add(idx)
                                else:
                                    start_pos = entity_lower.find(ans_text)
                                    if start_pos != -1:
                                        end_pos = start_pos + len(ans_text)
                                        before_ok = (start_pos == 0 or not entity_lower[start_pos-1].isalnum())
                                        after_ok = (end_pos == len(entity_lower) or not entity_lower[end_pos].isalnum())
                                        if before_ok and after_ok:
                                            answer_entities.add(idx)
                
                # è·å–åŸå§‹å®Œæ•´çš„é‡‘ä¸‰å…ƒç»„
                original_gold_triples = get_gold_triples(
                    sample['subgraph']['tuples'],
                    question_entities,
                    answer_entities
                )
                
                # å°†ä¸‰å…ƒç»„çš„ç´¢å¼•è½¬æ¢ä¸ºå®é™…æ–‡æœ¬ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
                text_triples = []
                effective_gold_text = []
                original_gold_text = []
                
                # è½¬æ¢è£å‰ªåçš„å­å›¾ä¸‰å…ƒç»„
                for h, r, t in trimmed_triples:
                    head_text = entity_list[h]
                    rel_text = relation_list[r]
                    tail_text = entity_list[t]
                    text_triples.append([head_text, rel_text, tail_text])
                
                # è½¬æ¢æœ‰æ•ˆçš„é‡‘ä¸‰å…ƒç»„
                for h, r, t in effective_gold_triples:
                    head_text = entity_list[h]
                    rel_text = relation_list[r]
                    tail_text = entity_list[t]
                    effective_gold_text.append([head_text, rel_text, tail_text])
                
                # è½¬æ¢åŸå§‹å®Œæ•´çš„é‡‘ä¸‰å…ƒç»„
                for h, r, t in original_gold_triples:
                    head_text = entity_list[h]
                    rel_text = relation_list[r]
                    tail_text = entity_list[t]
                    original_gold_text.append([head_text, rel_text, tail_text])
                
                # è®¡ç®—é‡‘ä¸‰å…ƒç»„ä¿ç•™ç‡
                retention_rate = 0
                if original_gold_triples:
                    retention_rate = len(effective_gold_triples) / len(original_gold_triples)
                    stats['gold_retention_rates'].append(retention_rate)
                
                # å°†åŸå§‹å­å›¾çš„tuplesä¹Ÿè½¬æ¢ä¸ºæ–‡æœ¬å½¢å¼
                original_subgraph_text = []
                for h, r, t in sample['subgraph']['tuples']:
                    head_text = entity_list[h]
                    rel_text = relation_list[r]
                    tail_text = entity_list[t]
                    original_subgraph_text.append([head_text, rel_text, tail_text])
                
                # æ„å»ºç»“æœ - å®Œå…¨æŒ‰ç…§åŸå§‹æ ¼å¼
                processing_time = time.time() - start_time
                result = {
                    "sample_id": i,
                    "question": question,
                    "golden_texts": golden_texts,
                    "trimmed_subgraph_length": len(trimmed_triples),
                    "original_subgraph_length": len(sample['subgraph']['tuples']),
                    "gold_triples_length": len(effective_gold_triples),
                    "answer_covered": True,  # å‡è®¾éƒ½è¦†ç›–äº†ï¼Œä¸åŸæ ¼å¼ä¸€è‡´
                    "trimmed_triples": text_triples,
                    "gold_triples": effective_gold_text,
                    "processing_time": processing_time
                }
                
                f.write(json.dumps(result, ensure_ascii=False) + "\n")
                
                stats['successful_samples'] += 1
                stats['total_original_gold'] += len(original_gold_triples)
                stats['total_effective_gold'] += len(effective_gold_triples)
                
            except Exception as e:
                print(f"å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {str(e)}")
                stats['failed_samples'] += 1
                continue
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if stats['gold_retention_rates']:
            stats['avg_retention_rate'] = sum(stats['gold_retention_rates']) / len(stats['gold_retention_rates'])
            stats['min_retention_rate'] = min(stats['gold_retention_rates'])
            stats['max_retention_rate'] = max(stats['gold_retention_rates'])
        
        if stats['total_original_gold'] > 0:
            stats['overall_retention_rate'] = stats['total_effective_gold'] / stats['total_original_gold']
        
        # å†™å…¥æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        final_stats = {
            "final_statistics": stats,
            "timestamp": timestamp,
            "total_time": time.time() - start_time
        }
        f.write(json.dumps(final_stats, ensure_ascii=False) + "\n")
    
    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"æˆåŠŸ: {stats['successful_samples']}, å¤±è´¥: {stats['failed_samples']}")
    print(f"å¹³å‡é‡‘ä¸‰å…ƒç»„ä¿ç•™ç‡: {stats.get('avg_retention_rate', 0):.1%}")
    print(f"æ•´ä½“é‡‘ä¸‰å…ƒç»„ä¿ç•™ç‡: {stats.get('overall_retention_rate', 0):.1%}")
    print(f"æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")
    print(f"ç»“æœå·²ä¿å­˜è‡³: {result_path}")
    
    return result_path

def get_golden_expansion_set(triples: List[List[int]], 
                           golden_triples: List[List[int]]) -> List[List[int]]:
    """
    ä¸ºFGASç”ŸæˆGolden Expansion Set (GES)ï¼šGolden Triples + é‚»æ¥ä¸‰å…ƒç»„
    
    Args:
        triples: å®Œæ•´ä¸‰å…ƒç»„åˆ—è¡¨
        golden_triples: åŸºç¡€golden triplesï¼ˆshortest pathï¼‰
    
    Returns:
        æ‰©å±•çš„golden triplesé›†åˆï¼ˆåŒ…å«1-hopé‚»æ¥ï¼‰
    """
    if not golden_triples:
        return []
    
    # è½¬æ¢ä¸ºé›†åˆä¾¿äºæŸ¥æ‰¾
    golden_set = set(tuple(t) for t in golden_triples)
    expansion_set = set(golden_set)  # å…ˆåŒ…å«åŸå§‹golden triples
    
    # æå–golden triplesä¸­çš„æ‰€æœ‰å®ä½“
    golden_entities = set()
    for h, r, t in golden_triples:
        golden_entities.add(h)
        golden_entities.add(t)
    
    # æ‰¾åˆ°ä¸golden entitiesç›¸è¿çš„æ‰€æœ‰ä¸‰å…ƒç»„ï¼ˆ1-hopé‚»æ¥ï¼‰
    for h, r, t in triples:
        triple_tuple = (h, r, t)
        # å¦‚æœè¿™ä¸ªä¸‰å…ƒç»„ä¸åœ¨golden setä¸­ï¼Œä½†åŒ…å«golden entityï¼Œåˆ™æ·»åŠ 
        if triple_tuple not in golden_set and (h in golden_entities or t in golden_entities):
            expansion_set.add(triple_tuple)
    
    return [list(t) for t in expansion_set]

def get_subgraph_simple(triples: List[List[int]], 
                       question_entities: Set[int], 
                       answer_entities: Set[int], 
                       topk: int = 20) -> List[List[int]]:
    """
    ç®€åŒ–çš„å­å›¾æ„å»ºå‡½æ•°ï¼Œä¸“æ³¨äºæ ¸å¿ƒé€»è¾‘
    """
    if not question_entities or not answer_entities:
        return triples[:topk] if len(triples) >= topk else triples
    
    # ä¼˜å…ˆé€‰æ‹©åŒ…å«é—®é¢˜æˆ–ç­”æ¡ˆå®ä½“çš„ä¸‰å…ƒç»„
    relevant_triples = []
    other_triples = []
    
    for triple in triples:
        h, r, t = triple
        if (h in question_entities or t in question_entities or 
            h in answer_entities or t in answer_entities):
            relevant_triples.append(triple)
        else:
            other_triples.append(triple)
    
    # ç»„åˆç»“æœ
    result = relevant_triples[:topk]
    if len(result) < topk:
        result.extend(other_triples[:topk - len(result)])
    
    return result

def prepare_dataset_tus_consistent(data_dir: str, 
                                  output_dir: str, 
                                  topk: int = 20,
                                  num_samples: int = None) -> str:
    """
    ä½¿ç”¨TUSä¸€è‡´æ€§åŒé‡ç­–ç•¥å‡†å¤‡æ•°æ®é›†
    
    ğŸ¯ æ ¸å¿ƒç›®æ ‡ï¼š
    - TUSç­–ç•¥ï¼šä¸æˆåŠŸç‰ˆæœ¬100%ä¸€è‡´ï¼Œç¡®ä¿TUSæ˜¾è‘—æ€§
    - FGASç­–ç•¥ï¼šæä¾›é¢å¤–çš„è¯­ä¹‰æ‰©å±•ï¼Œä¸ºFGASè®¡ç®—æœåŠ¡
    - æ•°æ®ä¸€è‡´æ€§ï¼šæ‰€æœ‰golden tripleséƒ½åœ¨æœ€ç»ˆå­å›¾ä¸­
    
    Args:
        data_dir: åŸå§‹æ•°æ®é›†ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        topk: å­å›¾å¤§å°ï¼ˆé»˜è®¤20ï¼Œä¸æˆåŠŸç‰ˆæœ¬ä¸€è‡´ï¼‰
        num_samples: å¤„ç†æ ·æœ¬æ•°é‡é™åˆ¶
    
    Returns:
        ä¿å­˜çš„ç»“æœæ–‡ä»¶è·¯å¾„
    """
    # 1. åŠ è½½æ•°æ®
    print("\nğŸš€ ä½¿ç”¨TUSä¸€è‡´æ€§åŒé‡ç­–ç•¥å‡†å¤‡æ•°æ®é›†")
    print("="*60)
    print("ç­–ç•¥è¯´æ˜:")
    print("  ğŸ¯ TUS: 100%å¤ç”¨æˆåŠŸç‰ˆæœ¬é€»è¾‘ï¼Œç¡®ä¿æ˜¾è‘—æ€§")
    print("  ğŸŒŸ FGAS: æä¾›è¯­ä¹‰æ‰©å±•é›†åˆï¼Œå¢å¼ºè¯­ä¹‰è¯„ä¼°")
    print("  âœ… ä¸€è‡´æ€§: æ‰€æœ‰golden tripleséƒ½åœ¨æœ€ç»ˆå­å›¾ä¸­")
    print("="*60)
    
    data = load_data(os.path.join(data_dir, "dev_simple.json"))
    entity_list, relation_list = load_entities_and_relations(data_dir)
    
    if num_samples:
        data = data[:num_samples]
        print(f"ğŸ”„ é™åˆ¶å¤„ç† {num_samples} ä¸ªæ ·æœ¬")
    
    print(f"ğŸ“š åŠ è½½å®Œæˆ: {len(data)} ä¸ªæ ·æœ¬")
    
    # 2. å¤„ç†æ•°æ®
    results = []
    tus_gold_stats = []
    fgas_expansion_stats = []
    success_count = 0
    
    for i, sample in enumerate(tqdm(data, desc="TUSä¸€è‡´æ€§ç­–ç•¥å¤„ç†")):
        try:
            # ä½¿ç”¨TUSä¸€è‡´æ€§åŒé‡ç­–ç•¥
            trimmed_subgraph, tus_golden_triples, fgas_golden_expansion_set = dual_subgraph_trimming_tus_consistent(
                sample, entity_list, topk
            )
            
            # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ä¿å­˜
            trimmed_subgraph_text = [convert_indices_to_freebase(t, entity_list, relation_list) 
                                   for t in trimmed_subgraph]
            tus_golden_text = [convert_indices_to_freebase(t, entity_list, relation_list) 
                             for t in tus_golden_triples]
            fgas_expansion_text = [convert_indices_to_freebase(t, entity_list, relation_list) 
                                 for t in fgas_golden_expansion_set]
            
            # æ„å»ºç»“æœï¼ˆä¿æŒä¸ç°æœ‰æ ¼å¼å…¼å®¹ï¼‰
            golden_texts = [ans['text'].lower() for ans in sample.get('answers', []) if ans.get('text')]
            answer_covered = len(set(golden_texts) & 
                               set([t[0].lower() if len(t) > 0 else '' for t in trimmed_subgraph_text] + 
                                   [t[2].lower() if len(t) > 2 else '' for t in trimmed_subgraph_text])) > 0
            
            result = {
                'sample_id': i,
                'question': sample['question'],
                'golden_texts': golden_texts,
                'trimmed_subgraph_length': len(trimmed_subgraph),
                'original_subgraph_length': len(sample['subgraph']['tuples']),
                'gold_triples_length': len(tus_golden_triples),
                'golden_expansion_length': len(fgas_golden_expansion_set),
                'answer_covered': answer_covered,
                'trimmed_triples': trimmed_subgraph_text,
                'gold_triples': tus_golden_text,          # ğŸ¯ TUSä½¿ç”¨ï¼šä¸æˆåŠŸç‰ˆæœ¬ä¸€è‡´
                'golden_expansion_set': fgas_expansion_text,  # ğŸŒŸ FGASä½¿ç”¨ï¼šè¯­ä¹‰æ‰©å±•é›†åˆ
                'processing_time': 0.0,
                'strategy': 'tus_consistent_dual'
            }
            results.append(result)
            
            # ç»Ÿè®¡ä¿¡æ¯
            tus_gold_stats.append(len(tus_golden_triples))
            fgas_expansion_stats.append(len(fgas_golden_expansion_set))
            success_count += 1
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
            # åˆ›å»ºé”™è¯¯è®°å½•
            result = {
                'sample_id': i,
                'question': sample.get('question', ''),
                'golden_texts': [],
                'trimmed_subgraph_length': 0,
                'original_subgraph_length': len(sample.get('subgraph', {}).get('tuples', [])),
                'gold_triples_length': 0,
                'golden_expansion_length': 0,
                'answer_covered': False,
                'trimmed_triples': [],
                'gold_triples': [],
                'golden_expansion_set': [],
                'processing_time': 0.0,
                'strategy': 'tus_consistent_dual',
                'error': str(e)
            }
            results.append(result)
    
    # 3. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(output_dir, f"trimming_results_tus_consistent_{timestamp}.jsonl")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # é…ç½®ä¿¡æ¯
    config = {
        "config": {
            "timestamp": timestamp,
            "method": "tus_consistent_dual_strategy",
            "topk": topk,
            "total_samples": len(data),
            "data_dir": data_dir,
            "strategy_description": "TUSä¸æˆåŠŸç‰ˆæœ¬100%ä¸€è‡´ + FGASè¯­ä¹‰æ‰©å±•é›†åˆ"
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(json.dumps(config, ensure_ascii=False) + '\n')
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    # 4. æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print(f"\nğŸ¯ TUSä¸€è‡´æ€§åŒé‡ç­–ç•¥å¤„ç†å®Œæˆï¼")
    print(f"="*60)
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"  æˆåŠŸå¤„ç†: {success_count}")
    print(f"  å¤±è´¥æ ·æœ¬: {len(results) - success_count}")
    print(f"  æˆåŠŸç‡: {success_count/len(results)*100:.1f}%")
    
    if tus_gold_stats:
        print(f"\nğŸ¯ TUS Golden Triplesç»Ÿè®¡ (ä¸æˆåŠŸç‰ˆæœ¬ä¸€è‡´):")
        print(f"  å¹³å‡æ•°é‡: {sum(tus_gold_stats)/len(tus_gold_stats):.2f}")
        print(f"  æœ€å¤§æ•°é‡: {max(tus_gold_stats)}")
        print(f"  æœ€å°æ•°é‡: {min(tus_gold_stats)}")
        
        # TUSåˆ†å¸ƒåˆ†æ
        tus_1_3 = sum(1 for x in tus_gold_stats if 1 <= x <= 3)
        tus_4_10 = sum(1 for x in tus_gold_stats if 4 <= x <= 10)
        tus_over_10 = sum(1 for x in tus_gold_stats if x > 10)
        
        print(f"  åˆ†å¸ƒåˆ†æ:")
        print(f"    1-3ä¸ª (æœ€ä½³): {tus_1_3} æ ·æœ¬ ({tus_1_3/len(tus_gold_stats)*100:.1f}%)")
        print(f"    4-10ä¸ª (åˆç†): {tus_4_10} æ ·æœ¬ ({tus_4_10/len(tus_gold_stats)*100:.1f}%)")
        print(f"    >10ä¸ª (è¾ƒå¤š): {tus_over_10} æ ·æœ¬ ({tus_over_10/len(tus_gold_stats)*100:.1f}%)")
    
    if fgas_expansion_stats:
        print(f"\nğŸŒŸ FGAS Golden Expansion Setç»Ÿè®¡:")
        print(f"  å¹³å‡æ•°é‡: {sum(fgas_expansion_stats)/len(fgas_expansion_stats):.2f}")
        print(f"  æœ€å¤§æ•°é‡: {max(fgas_expansion_stats)}")
        print(f"  æœ€å°æ•°é‡: {min(fgas_expansion_stats)}")
        
        # FGASåˆ†å¸ƒåˆ†æ
        fgas_small = sum(1 for x in fgas_expansion_stats if x < 5)
        fgas_medium = sum(1 for x in fgas_expansion_stats if 5 <= x <= 15)
        fgas_large = sum(1 for x in fgas_expansion_stats if x > 15)
        
        print(f"  åˆ†å¸ƒåˆ†æ:")
        print(f"    <5ä¸ª (è¾ƒå°‘): {fgas_small} æ ·æœ¬ ({fgas_small/len(fgas_expansion_stats)*100:.1f}%)")
        print(f"    5-15ä¸ª (åˆç†): {fgas_medium} æ ·æœ¬ ({fgas_medium/len(fgas_expansion_stats)*100:.1f}%)")
        print(f"    >15ä¸ª (ä¸°å¯Œ): {fgas_large} æ ·æœ¬ ({fgas_large/len(fgas_expansion_stats)*100:.1f}%)")
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"  1. ä½¿ç”¨ 'gold_triples' å­—æ®µè®¡ç®—TUSæŒ‡æ ‡")
    print(f"  2. ä½¿ç”¨ 'golden_expansion_set' å­—æ®µè®¡ç®—FGASæŒ‡æ ‡")
    print(f"  3. éªŒè¯TUSæ˜¯å¦è¾¾åˆ°æ˜¾è‘—æ€§ï¼ˆæœŸæœ›å€¼ï¼šp < 0.05ï¼‰")
    print(f"  4. æ£€æŸ¥FGASçš„è¯­ä¹‰ä¸°å¯Œåº¦æ˜¯å¦æœ‰æ‰€æ”¹å–„")
    
    return output_file

def generate_fgas_expansion_from_tus(trimmed_subgraph: List[List[int]], 
                                    tus_golden_triples: List[List[int]], 
                                    question_entities: Set[int],
                                    answer_entities: Set[int]) -> List[List[int]]:
    """
    ğŸ¯ ç²¾ç¡®çš„FGASæ‰©å±•ç­–ç•¥ï¼šç¡®ä¿æœ‰åŒºåˆ†èƒ½åŠ›
    
    ç­–ç•¥ï¼š
    1. èµ·å§‹ç‚¹ï¼šTUS golden triplesï¼ˆæ ¸å¿ƒï¼‰
    2. åªæ‰©å±•ä¸golden triplesç›´æ¥ç›¸è¿çš„ä¸‰å…ƒç»„ï¼ˆ1-hopä¸¥æ ¼é™åˆ¶ï¼‰
    3. æ§åˆ¶æ‰©å±•è§„æ¨¡ï¼Œé¿å…åŒ…å«æ•´ä¸ªå­å›¾
    
    Args:
        trimmed_subgraph: æœ€ç»ˆçš„è£å‰ªå­å›¾
        tus_golden_triples: TUS golden triples
        question_entities: é—®é¢˜å®ä½“é›†åˆ
        answer_entities: ç­”æ¡ˆå®ä½“é›†åˆ
    
    Returns:
        FGAS golden expansion setï¼ˆå°è€Œç²¾ç¡®çš„æ‰©å±•é›†åˆï¼‰
    """
    if not tus_golden_triples:
        return []
    
    # å°†å­å›¾å’ŒTUS goldenè½¬ä¸ºé›†åˆä¾¿äºæŸ¥æ‰¾
    subgraph_set = set(tuple(t) for t in trimmed_subgraph)
    tus_golden_set = set(tuple(t) for t in tus_golden_triples)
    
    # ğŸ”¥ èµ·å§‹ç‚¹ï¼šæ‰€æœ‰TUS golden triplesï¼ˆå¿…é¡»åŒ…å«ï¼‰
    expansion_set = set(tus_golden_set)
    
    # è·å–TUS goldenå®ä½“
    tus_entities = set()
    for h, r, t in tus_golden_triples:
        tus_entities.add(h)
        tus_entities.add(t)
    
    # ğŸ¯ ç²¾ç¡®æ‰©å±•ï¼šåªæ·»åŠ ä¸golden entitiesç›´æ¥ç›¸è¿çš„å…³é”®ä¸‰å…ƒç»„
    candidates = []
    
    for triple in trimmed_subgraph:
        triple_tuple = tuple(triple)
        
        # è·³è¿‡å·²ç»åœ¨TUS goldenä¸­çš„ä¸‰å…ƒç»„
        if triple_tuple in tus_golden_set:
            continue
            
        h, r, t = triple
        score = 0
        
        # ğŸ”¥ è¯„åˆ†æ ‡å‡†ï¼šæ›´ä¸¥æ ¼çš„æ¡ä»¶
        
        # æœ€é«˜ä¼˜å…ˆçº§ï¼šä¸goldenå®ä½“ç›´æ¥ç›¸è¿ ä¸” æ¶‰åŠç­”æ¡ˆå®ä½“
        if (h in tus_entities or t in tus_entities) and (h in answer_entities or t in answer_entities):
            score = 10
        
        # ä¸­ç­‰ä¼˜å…ˆçº§ï¼šä¸goldenå®ä½“ç›´æ¥ç›¸è¿ ä¸” æ¶‰åŠé—®é¢˜å®ä½“  
        elif (h in tus_entities or t in tus_entities) and (h in question_entities or t in question_entities):
            score = 5
        
        # ä½ä¼˜å…ˆçº§ï¼šä»…ä¸goldenå®ä½“ç›´æ¥ç›¸è¿
        elif h in tus_entities or t in tus_entities:
            score = 2
            
        # å…¶ä»–ä¸‰å…ƒç»„ï¼šä¸åŒ…å«
        else:
            score = 0
        
        if score > 0:
            candidates.append((score, triple_tuple))
    
    # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©å‰å‡ ä¸ª
    candidates.sort(key=lambda x: x[0], reverse=True)
    
    # ğŸ¯ æ§åˆ¶æ‰©å±•è§„æ¨¡ï¼šæœ€å¤šæ‰©å±•åˆ°goldençš„2-3å€
    max_expansion = max(len(tus_golden_triples) * 3, 10)  # è‡³å°‘10ä¸ªï¼Œæœ€å¤šgoldençš„3å€
    max_additional = max_expansion - len(tus_golden_triples)
    
    added_count = 0
    for score, triple_tuple in candidates:
        if added_count >= max_additional:
            break
        expansion_set.add(triple_tuple)
        added_count += 1
    
    result = [list(t) for t in expansion_set]
    
    print(f"ğŸ¯ FGASæ‰©å±•ç­–ç•¥:")
    print(f"  - TUS golden: {len(tus_golden_triples)} ä¸ª")
    print(f"  - å€™é€‰æ‰©å±•: {len(candidates)} ä¸ª")
    print(f"  - å®é™…æ‰©å±•: {added_count} ä¸ª")
    print(f"  - æœ€ç»ˆFGAS: {len(result)} ä¸ª (æ‰©å±•æ¯”ä¾‹: {len(result)/len(tus_golden_triples):.1f}x)")
    
    return result

def dual_subgraph_trimming_tus_consistent(sample: Dict, 
                                         entity_list: List[str], 
                                         topk: int = 20) -> Tuple[List[List[int]], List[List[int]], List[List[int]]]:
    """
    ğŸ¯ TUSä¸€è‡´æ€§åŒé‡ç­–ç•¥ï¼šç¡®ä¿TUSä¸æˆåŠŸç‰ˆæœ¬å®Œå…¨ä¸€è‡´ + ä¿ç•™FGASæ‰©å±•é›†åˆ
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. TUSç­–ç•¥ï¼šå…ˆç”Ÿæˆgolden triplesï¼Œå†å¼ºåˆ¶ä¿ç•™ï¼ˆä¿è¯å®Œæ•´æ€§ï¼‰
    2. FGASç­–ç•¥ï¼šåŸºäºå®Œæ•´çš„golden triplesè¿›è¡Œæ‰©å±•
    3. æ•°æ®ä¸€è‡´æ€§ï¼š100%ç¡®ä¿golden triplesåœ¨æœ€ç»ˆå­å›¾ä¸­
    
    Args:
        sample: æ ·æœ¬æ•°æ®
        entity_list: å®ä½“åˆ—è¡¨  
        topk: å­å›¾å¤§å°ï¼ˆé»˜è®¤20ï¼Œä¸æˆåŠŸç‰ˆæœ¬ä¸€è‡´ï¼‰
    
    Returns:
        tuple: (trimmed_subgraph, tus_golden_triples, fgas_golden_expansion_set)
        - trimmed_subgraph: åŒ…å«æ‰€æœ‰golden triplesçš„å­å›¾
        - tus_golden_triples: ä»åŸå›¾ç”Ÿæˆçš„å®Œæ•´golden triples
        - fgas_golden_expansion_set: åŸºäºå®Œæ•´goldençš„FGASæ‰©å±•é›†åˆ
    """
    
    print(f"ğŸ¯ ä½¿ç”¨TUSä¸€è‡´æ€§åŒé‡ç­–ç•¥å¤„ç†æ ·æœ¬...")
    
    # ==================== ç¬¬1æ­¥ï¼šå®ä½“åŒ¹é…ï¼ˆä¸æˆåŠŸç‰ˆæœ¬ä¸€è‡´ï¼‰====================
    
    question = sample['question']
    triples = sample['subgraph']['tuples']
    
                        # ğŸ”¥ ä¸TUSæ˜¾è‘—ç‰ˆæœ¬100%ç›¸åŒçš„ç®€å•å®ä½“åŒ¹é…é€»è¾‘
    question_entities = set()
    for idx, entity in enumerate(entity_list):
        if entity in question:  # ç®€å•åŒ…å«åŒ¹é…ï¼Œä¸æˆåŠŸç‰ˆæœ¬ä¸€è‡´
            question_entities.add(idx)
    
    answer_entities = set()
    for ans in sample.get('answers', []):
        if ans.get('text'):
            ans_text = ans['text'].lower()
            for idx, entity in enumerate(entity_list):
                if ans_text in entity.lower():  # ç®€å•åŒ…å«åŒ¹é…ï¼Œä¸æˆåŠŸç‰ˆæœ¬ä¸€è‡´
                    answer_entities.add(idx)
    
    # ==================== ç¬¬2æ­¥ï¼šæ–¹æ¡ˆâ‘¡å®ç° - å…ˆç”Ÿæˆgoldenï¼Œå†å¼ºåˆ¶ä¿ç•™ ====================
    
    # ğŸ”¥ æ­¥éª¤1ï¼šä»åŸå›¾ç”Ÿæˆå®Œæ•´çš„TUS golden triplesï¼ˆç¡®ä¿å®Œæ•´æ€§ï¼‰
    tus_golden_triples = get_gold_triples(
        sample['subgraph']['tuples'],  # ä»åŸå§‹å¤§å›¾ç”Ÿæˆï¼Œç¡®ä¿å®Œæ•´è·¯å¾„
        question_entities,
        answer_entities
    )
    
    print(f"âœ… TUS Golden Triplesç”Ÿæˆå®Œæˆï¼Œæ•°é‡: {len(tus_golden_triples)}")
    
    if not tus_golden_triples:
        print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°TUS golden triplesï¼Œä½¿ç”¨fallbackç­–ç•¥")
        # ä½¿ç”¨ç®€å•çš„å­å›¾æ„å»º
        trimmed_subgraph = get_subgraph_simple(triples, question_entities, answer_entities, topk)
        return trimmed_subgraph, [], []
    
    # ğŸ”¥ æ­¥éª¤2ï¼šæ„å»ºåŒ…å«æ‰€æœ‰golden triplesçš„å­å›¾ï¼ˆå¼ºåˆ¶ä¿ç•™ï¼‰
    trimmed_subgraph = ensure_golden_in_subgraph(
        triples, 
        tus_golden_triples, 
        question_entities, 
        answer_entities, 
        topk
    )
    
    # ğŸš¨ å…³é”®ä¿®å¤ï¼šå¦‚æœgoldenæ•°é‡è¶…è¿‡topkï¼Œéœ€è¦æ›´æ–°å®é™…çš„golden triplesåˆ—è¡¨
    # ç¡®ä¿åç»­éªŒè¯ä½¿ç”¨çš„æ˜¯å®é™…åŒ…å«åœ¨å­å›¾ä¸­çš„golden triples
    if len(tus_golden_triples) > topk:
        subgraph_set = set(tuple(t) for t in trimmed_subgraph)
        original_golden_set = set(tuple(t) for t in tus_golden_triples)
        actual_golden_in_subgraph = [list(t) for t in subgraph_set & original_golden_set]
        
        print(f"ğŸ”„ æ›´æ–°golden triplesï¼šåŸå§‹{len(tus_golden_triples)}ä¸ª â†’ å®é™…åŒ…å«{len(actual_golden_in_subgraph)}ä¸ª")
        tus_golden_triples = actual_golden_in_subgraph
    
    print(f"âœ… å­å›¾æ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(trimmed_subgraph)}ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰{len(tus_golden_triples)}ä¸ªgolden triples")
    
    # ==================== ç¬¬3æ­¥ï¼šä¸ºFGASç”Ÿæˆæ‰©å±•é›†åˆ ====================
    
    # ğŸŒŸ åŸºäºå®Œæ•´çš„golden tripleså’Œç¡®ä¿ä¸€è‡´æ€§çš„å­å›¾ç”ŸæˆFGASæ‰©å±•
    fgas_golden_expansion_set = generate_fgas_expansion_from_tus(
        trimmed_subgraph,  # å·²ç¡®ä¿åŒ…å«æ‰€æœ‰golden triplesçš„å­å›¾
        tus_golden_triples,  # å®Œæ•´çš„golden triples
        question_entities,
        answer_entities
    )
    
    print(f"âœ… FGAS Golden Expansion Setç”Ÿæˆå®Œæˆï¼Œæ•°é‡: {len(fgas_golden_expansion_set)}")
    
    # ==================== ç¬¬4æ­¥ï¼šæœ€ç»ˆéªŒè¯ï¼ˆç¡®ä¿100%ä¸€è‡´æ€§ï¼‰====================
    
    # éªŒè¯TUS golden triples 100% åœ¨å­å›¾ä¸­
    subgraph_set = set(tuple(t) for t in trimmed_subgraph)
    tus_golden_set = set(tuple(t) for t in tus_golden_triples)
    
    missing_golden = tus_golden_set - subgraph_set
    if missing_golden:
        print(f"âŒ ä¸¥é‡é”™è¯¯ï¼š{len(missing_golden)} ä¸ªTUS golden triplesä¸åœ¨å­å›¾ä¸­ï¼")
        # è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä½œä¸ºæœ€åçš„å®‰å…¨æªæ–½
        for missing_triple in missing_golden:
            if len(trimmed_subgraph) < topk:
                trimmed_subgraph.append(list(missing_triple))
            else:
                # æ›¿æ¢æœ€ä¸é‡è¦çš„ä¸‰å…ƒç»„
                trimmed_subgraph[-1] = list(missing_triple)
    else:
        print(f"âœ… éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰{len(tus_golden_triples)}ä¸ªTUS golden tripleséƒ½åœ¨å­å›¾ä¸­")
    
    # éªŒè¯FGAS expansion setä¹Ÿåœ¨å­å›¾ä¸­
    fgas_set = set(tuple(t) for t in fgas_golden_expansion_set)
    missing_fgas = fgas_set - subgraph_set
    if missing_fgas:
        print(f"âš ï¸ è­¦å‘Šï¼š{len(missing_fgas)} ä¸ªFGAS expansion triplesä¸åœ¨å­å›¾ä¸­")
    else:
        print(f"âœ… éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰{len(fgas_golden_expansion_set)}ä¸ªFGAS expansion tripleséƒ½åœ¨å­å›¾ä¸­")
    
    # éªŒè¯ç­”æ¡ˆå®ä½“è¦†ç›–
    subgraph_entities = set()
    for h, r, t in trimmed_subgraph:
        subgraph_entities.add(h)
        subgraph_entities.add(t)
    
    if not (answer_entities & subgraph_entities):
        print(f"âš ï¸ è­¦å‘Šï¼šç­”æ¡ˆå®ä½“ä¸åœ¨æœ€ç»ˆå­å›¾ä¸­")
    else:
        print(f"âœ… éªŒè¯é€šè¿‡ï¼šç­”æ¡ˆå®ä½“åœ¨æœ€ç»ˆå­å›¾ä¸­")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - é—®é¢˜å®ä½“: {len(question_entities)} ä¸ª")
    print(f"  - ç­”æ¡ˆå®ä½“: {len(answer_entities)} ä¸ª") 
    print(f"  - å­å›¾å¤§å°: {len(trimmed_subgraph)} ä¸ªä¸‰å…ƒç»„")
    print(f"  - TUS Golden: {len(tus_golden_triples)} ä¸ªä¸‰å…ƒç»„")
    print(f"  - FGAS Expansion: {len(fgas_golden_expansion_set)} ä¸ªä¸‰å…ƒç»„")
    print(f"  - TUSä¸€è‡´æ€§: {'âœ… 100%' if not missing_golden else 'âŒ ä¸å®Œæ•´'}")
    print(f"  - FGASä¸€è‡´æ€§: {'âœ… 100%' if not missing_fgas else 'âŒ ä¸å®Œæ•´'}")
    
    return trimmed_subgraph, tus_golden_triples, fgas_golden_expansion_set


def ensure_golden_in_subgraph(triples: List[List[int]], 
                             golden_triples: List[List[int]], 
                             question_entities: Set[int], 
                             answer_entities: Set[int], 
                             max_size: int = 20) -> List[List[int]]:
    """
    æ„å»ºå­å›¾ï¼Œç¡®ä¿æ‰€æœ‰golden tripleséƒ½è¢«åŒ…å«ï¼ˆæ–¹æ¡ˆâ‘¡çš„æ ¸å¿ƒå®ç°ï¼‰
    
    ç­–ç•¥ï¼š
    1. å¼ºåˆ¶åŒ…å«æ‰€æœ‰golden triplesï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    2. æ·»åŠ ä¸é—®é¢˜/ç­”æ¡ˆå®ä½“ç›¸å…³çš„é‡è¦ä¸‰å…ƒç»„
    3. å¡«å……å‰©ä½™ç©ºé—´
    
    Args:
        triples: åŸå§‹å®Œæ•´ä¸‰å…ƒç»„åˆ—è¡¨
        golden_triples: å¿…é¡»åŒ…å«çš„golden triples
        question_entities: é—®é¢˜å®ä½“é›†åˆ
        answer_entities: ç­”æ¡ˆå®ä½“é›†åˆ
        max_size: å­å›¾æœ€å¤§å¤§å°
    
    Returns:
        ç¡®ä¿åŒ…å«æ‰€æœ‰golden triplesçš„å­å›¾
    """
    if not golden_triples:
        return get_subgraph_simple(triples, question_entities, answer_entities, max_size)
    
    # ğŸ”¥ ç¬¬1ä¼˜å…ˆçº§ï¼šå¼ºåˆ¶åŒ…å«æ‰€æœ‰golden triples
    subgraph_set = set(tuple(t) for t in golden_triples)
    print(f"ğŸ”¥ å¼ºåˆ¶åŒ…å«{len(golden_triples)}ä¸ªgolden triples")
    
    # ğŸš¨ ä¿®å¤ï¼šå¦‚æœgolden triplesæ•°é‡è¶…è¿‡max_sizeï¼Œéœ€è¦ä¼˜å…ˆé€‰æ‹©æœ€é‡è¦çš„
    if len(subgraph_set) >= max_size:
        print(f"âš ï¸ Golden triplesæ•°é‡({len(subgraph_set)})è¶…è¿‡topké™åˆ¶({max_size})ï¼Œè¿›è¡Œä¼˜å…ˆçº§ç­›é€‰")
        
        # æŒ‰ä¼˜å…ˆçº§è¯„åˆ†é€‰æ‹©æœ€é‡è¦çš„golden triples
        golden_scored = []
        for triple in golden_triples:
            h, r, t = triple
            score = 0
            
            # ä¸é—®é¢˜å®ä½“ç›¸å…³ - æœ€é«˜ä¼˜å…ˆçº§
            if h in question_entities or t in question_entities:
                score += 10
            
            # ä¸ç­”æ¡ˆå®ä½“ç›¸å…³ - æ¬¡é«˜ä¼˜å…ˆçº§  
            if h in answer_entities or t in answer_entities:
                score += 8
            
            # è¿æ¥é—®é¢˜å’Œç­”æ¡ˆå®ä½“çš„è·¯å¾„ - é«˜ä¼˜å…ˆçº§
            if ((h in question_entities and t in answer_entities) or 
                (h in answer_entities and t in question_entities)):
                score += 15
            
            golden_scored.append((score, triple))
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©å‰max_sizeä¸ª
        golden_scored.sort(key=lambda x: x[0], reverse=True)
        selected_golden = [triple for _, triple in golden_scored[:max_size]]
        
        result = selected_golden
        print(f"âœ… ä¼˜å…ˆçº§ç­›é€‰ï¼šä»{len(golden_triples)}ä¸ªgoldenä¸­é€‰æ‹©äº†{len(result)}ä¸ªæœ€é‡è¦çš„")
        return result
    
    # ğŸ”¥ ç¬¬2ä¼˜å…ˆçº§ï¼šæ·»åŠ ä¸é—®é¢˜/ç­”æ¡ˆå®ä½“ç›´æ¥ç›¸å…³çš„ä¸‰å…ƒç»„
    important_triples = []
    for triple in triples:
        triple_tuple = tuple(triple)
        if triple_tuple not in subgraph_set:  # é¿å…é‡å¤
            h, r, t = triple
            score = 0
            
            # ä¸é—®é¢˜å®ä½“ç›¸å…³
            if h in question_entities or t in question_entities:
                score += 5
            
            # ä¸ç­”æ¡ˆå®ä½“ç›¸å…³  
            if h in answer_entities or t in answer_entities:
                score += 5
            
            # ä¸golden triplesä¸­çš„å®ä½“ç›¸å…³
            golden_entities = set()
            for gh, gr, gt in golden_triples:
                golden_entities.add(gh)
                golden_entities.add(gt)
            
            if h in golden_entities or t in golden_entities:
                score += 3
            
            if score > 0:
                important_triples.append((score, triple))
    
    # æŒ‰é‡è¦æ€§æ’åº
    important_triples.sort(key=lambda x: x[0], reverse=True)
    
    # æ·»åŠ é‡è¦ä¸‰å…ƒç»„ç›´åˆ°è¾¾åˆ°max_size
    remaining_slots = max_size - len(subgraph_set)
    added_important = 0
    
    for score, triple in important_triples:
        if len(subgraph_set) >= max_size:
            break
        subgraph_set.add(tuple(triple))
        added_important += 1
    
    print(f"âœ… æ·»åŠ äº†{added_important}ä¸ªé‡è¦ä¸‰å…ƒç»„")
    
    # ğŸ”¥ ç¬¬3ä¼˜å…ˆçº§ï¼šå¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œéšæœºå¡«å……å…¶ä»–ä¸‰å…ƒç»„
    if len(subgraph_set) < max_size:
        remaining_slots = max_size - len(subgraph_set)
        other_triples = []
        
        for triple in triples:
            triple_tuple = tuple(triple)
            if triple_tuple not in subgraph_set:
                other_triples.append(triple)
                if len(other_triples) >= remaining_slots:
                    break
        
        for triple in other_triples:
            subgraph_set.add(tuple(triple))
        
        print(f"âœ… å¡«å……äº†{len(other_triples)}ä¸ªå…¶ä»–ä¸‰å…ƒç»„")
    
    result = [list(t) for t in subgraph_set]
    print(f"ğŸ“Š æœ€ç»ˆå­å›¾: {len(result)}ä¸ªä¸‰å…ƒç»„ (golden: {len(golden_triples)}, å…¶ä»–: {len(result) - len(golden_triples)})")
    
    return result

if __name__ == "__main__":
    """
    æ•°æ®é›†å¤„ç†ä¸»ç¨‹åº
    
    é»˜è®¤ä½¿ç”¨åŒé‡å­å›¾è£å‰ªç­–ç•¥ï¼ŒåŒæ—¶ç”ŸæˆTUSå’ŒFGASæ‰€éœ€çš„ä¸åŒgolden triplesï¼š
    - TUS: ä½¿ç”¨ç²¾ç¡®shortest path golden triples (æ³¨æ„åŠ›ç²¾åº¦)
    - FGAS: ä½¿ç”¨æ‰©å±•è¯­ä¹‰golden expansion set (è¯­ä¹‰ä¸°å¯Œåº¦)
    """
    # è®¾ç½®å‚æ•°
    data_dir = "/mnt/d/datasets/GraphTruth/metaqa-1hop/metaqa-1hop"
    output_dir = "experiment_records"
    
    print("ğŸš€ è¿è¡ŒåŒé‡å­å›¾è£å‰ªç­–ç•¥ (é»˜è®¤æ–¹æ³•)")
    print("="*60)
    print("ç­–ç•¥è¯´æ˜:")
    print("  - TUS: ä½¿ç”¨ç²¾ç¡®shortest path golden triples")
    print("  - FGAS: ä½¿ç”¨æ‰©å±•è¯­ä¹‰golden expansion set")
    print("  - ç›®æ ‡: è®©TUSå’ŒFGASæŒ‡æ ‡åŒæ—¶æ˜¾è‘—")
    print("="*60)
    
    result_file = prepare_dataset_tus_consistent(
        data_dir=data_dir,
        output_dir=output_dir,
        topk=20,                    # å­å›¾å¤§å°
        num_samples=None           # å¤„ç†å…¨éƒ¨æ ·æœ¬
    )
    
    print(f"\nâœ… åŒé‡ç­–ç•¥å¤„ç†å®Œæˆ!")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
    print(f"   1. ä½¿ç”¨æ­¤æ–‡ä»¶è¿è¡Œinferenceå®éªŒ")
    print(f"   2. åˆ†åˆ«è®¡ç®—TUS (gold_triples) å’Œ FGAS (golden_expansion_set) æŒ‡æ ‡")
    print(f"   3. éªŒè¯ä¸¤ä¸ªæŒ‡æ ‡æ˜¯å¦åŒæ—¶æ˜¾è‘—")
    
    # å…¶ä»–æ–¹æ³•å¯é€‰ä½¿ç”¨ï¼ˆå–æ¶ˆæ³¨é‡Šå³å¯ï¼‰
    # print("\n--- å¯é€‰: é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆæ–¹æ³• ---")
    # result_file_gold = prepare_dataset_gold_priority(
    #     data_dir=data_dir,
    #     output_dir=output_dir,
    #     topk=20,
    #     num_samples=None
    # )
    # print(f"é‡‘ä¸‰å…ƒç»„ä¼˜å…ˆç»“æœ: {result_file_gold}")
    
    # print("\n--- å¯é€‰: åŸæœ‰æ–¹æ³• ---")
    # result_file_original = prepare_dataset(
    #     data_dir=data_dir,
    #     output_dir=output_dir,
    #     topk=30
    # )
    # print(f"åŸæœ‰æ–¹æ³•ç»“æœ: {result_file_original}") 