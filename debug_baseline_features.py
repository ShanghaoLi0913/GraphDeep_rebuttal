#!/usr/bin/env python3
"""
è¯Šæ–­baselineç‰¹å¾é—®é¢˜
"""
import json
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, classification_report
from sentence_transformers import SentenceTransformer

def load_and_analyze_features():
    """åŠ è½½æ•°æ®å¹¶åˆ†æç‰¹å¾"""
    print("ğŸ” è¯Šæ–­baselineç‰¹å¾é—®é¢˜...")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = []
    test_file = "/mnt/d/experiments/GraphDeEP/experiment_records/inference_results/llama2-7b/colab_test_simple.jsonl"
    
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip() and not line.startswith('{"config"'):
                test_data.append(json.loads(line))
                if len(test_data) >= 100:  # åªåˆ†æå‰100ä¸ªæ ·æœ¬
                    break
    
    print(f"âœ… åŠ è½½äº† {len(test_data)} ä¸ªæ ·æœ¬")
    
    # æå–æ ‡ç­¾
    labels = []
    for item in test_data:
        if 'squad_evaluation' in item:
            is_hallucination = item['squad_evaluation'].get('squad_is_hallucination', False)
        else:
            is_hallucination = not item.get('metrics', {}).get('hit@1', False)
        labels.append(int(is_hallucination))
    
    labels = np.array(labels)
    print(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)} (å¹»è§‰ç‡: {np.mean(labels):.3f})")
    
    # åŠ è½½sentence transformerï¼ˆè½»é‡çº§ç‰ˆæœ¬ï¼‰
    print("ğŸ“¥ åŠ è½½SentenceTransformer...")
    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # æå–ä¸€ä¸ªç®€å•ç‰¹å¾ï¼šBERTScore vs Question
    print("ğŸ”§ æå–BERTScoreç‰¹å¾...")
    bertscore_features = []
    
    for item in test_data:
        question = item.get('question', '')
        output = item.get('model_output', '')
        
        if question and output:
            # è®¡ç®—é—®é¢˜å’Œå›ç­”çš„ç›¸ä¼¼åº¦
            question_emb = sentence_model.encode([question])
            answer_emb = sentence_model.encode([output])
            similarity = float(np.dot(question_emb[0], answer_emb[0]) / 
                             (np.linalg.norm(question_emb[0]) * np.linalg.norm(answer_emb[0])))
        else:
            similarity = 0.0
        
        bertscore_features.append(similarity)
    
    bertscore_features = np.array(bertscore_features).reshape(-1, 1)
    
    # åˆ†æç‰¹å¾åˆ†å¸ƒ
    print("\nğŸ“ˆ BERTScoreç‰¹å¾åˆ†æ:")
    print(f"  èŒƒå›´: [{bertscore_features.min():.3f}, {bertscore_features.max():.3f}]")
    print(f"  å‡å€¼Â±æ ‡å‡†å·®: {bertscore_features.mean():.3f}Â±{bertscore_features.std():.3f}")
    
    # åˆ†ç»„åˆ†æ
    hallucination_mask = labels == 1
    correct_mask = labels == 0
    
    hall_scores = bertscore_features[hallucination_mask].flatten()
    correct_scores = bertscore_features[correct_mask].flatten()
    
    print(f"  å¹»è§‰æ ·æœ¬: {hall_scores.mean():.3f}Â±{hall_scores.std():.3f}")
    print(f"  æ­£ç¡®æ ·æœ¬: {correct_scores.mean():.3f}Â±{correct_scores.std():.3f}")
    print(f"  å·®å¼‚: {correct_scores.mean() - hall_scores.mean():.3f}")
    
    # å¿«é€Ÿè®­ç»ƒæµ‹è¯•
    print("\nğŸš€ å¿«é€Ÿåˆ†ç±»å™¨æµ‹è¯•:")
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(bertscore_features)
    
    # è®­ç»ƒé€»è¾‘å›å½’
    clf = LogisticRegression(random_state=42, class_weight='balanced')
    clf.fit(X_scaled, labels)
    
    # é¢„æµ‹
    y_proba = clf.predict_proba(X_scaled)[:, 1]
    y_pred = clf.predict(X_scaled)
    
    # è¯„ä¼°
    auc = roc_auc_score(labels, y_proba)
    report = classification_report(labels, y_pred, output_dict=True, zero_division=0)
    
    print(f"  AUC: {auc:.4f}")
    print(f"  F1: {report['1']['f1-score']:.4f}")
    print(f"  Precision: {report['1']['precision']:.4f}")
    print(f"  Recall: {report['1']['recall']:.4f}")
    
    # æ£€æŸ¥é¢„æµ‹åˆ†å¸ƒ
    print(f"  é¢„æµ‹åˆ†å¸ƒ: {np.bincount(y_pred)}")
    print(f"  æ¦‚ç‡èŒƒå›´: [{y_proba.min():.3f}, {y_proba.max():.3f}]")
    
    # é˜ˆå€¼åˆ†æ
    from sklearn.metrics import precision_recall_curve
    precisions, recalls, thresholds = precision_recall_curve(labels, y_proba)
    f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx]
    best_f1 = f1_scores[best_idx]
    
    print(f"\nğŸ¯ é˜ˆå€¼ä¼˜åŒ–:")
    print(f"  æœ€ä½³é˜ˆå€¼: {best_threshold:.3f}")
    print(f"  æœ€ä½³F1: {best_f1:.4f}")
    
    # ç”¨æœ€ä½³é˜ˆå€¼é‡æ–°é¢„æµ‹
    y_pred_opt = (y_proba >= best_threshold).astype(int)
    print(f"  ä¼˜åŒ–åé¢„æµ‹åˆ†å¸ƒ: {np.bincount(y_pred_opt)}")
    
    return auc, best_f1

if __name__ == "__main__":
    load_and_analyze_features()